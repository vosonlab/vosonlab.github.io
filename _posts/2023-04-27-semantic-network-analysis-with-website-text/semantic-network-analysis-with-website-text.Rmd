---
title: "Semantic network analysis with website text"
description: |
  A short description of the post.
author:
  - name: Robert Ackland
    url: https://orcid.org/0000-0002-0008-1766
  - name: Francisca Borquez  
  - name: Sidiq Madya
    url: https://orcid.org/0000-0002-8444-3145
date: 2023-04-27
output:
  distill::distill_article:
    self_contained: false
draft: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 1. Introduction

In this blogpost we conduct a type of semantic network analysis, which Yang and Gonzalez-Bailon refer to as "salience and framing" - more on this, from Table 3.1 in Y-G.  See my lecture slides p. 25

We will construct networks where the nodes are terms extracted from the "Data Sovereignty" website text [refer to previous posts] and a weighted edge between two terms indicates the number of times they co-occur in the website text. We will explore two types of co-occurrence: (1) bigrams (the terms co-occur because they are positioned directly next to each other in a sentence on a website), (2) co-occurrence within sentences (the terms co-occur because they are located in the same sentence, are but not necessarily positioned next to each other).

The aim is that by constructing semantic networks in this way, we might be able to identify clusters of terms that can be interpreted as frames [some basics on frames...see my lecture slides].

## 2. Semantic network using bigrams

First, we read in the dataframe constructed in [post].

```{r}
library(knitr)
textContent <- readRDS("textContent2.rds")
```

As a reminder, here is a summary of what is contained in this dataframe.

```{r}
colnames(textContent)
kable(head(textContent[,c(1,3,4)]))
#kable seems to have trouble with following...
head(substr(textContent$text,1,80))
```

With framing semantic network analysis, we will only be making use of `text` and `type` columns. First we will attempt to identify potential frames that are present in the website text authored by all organisations in our dataset, and then we will see if there is any difference in the framing by Global North and Global South organisations.

For this part of the exercise, we will use `tidytext`.

```{r}
library(tidytext)
library(tidyverse)
library(dplyr)

df1 <- textContent %>% unnest_tokens(ngram, text, token = "ngrams", n = 2)

#split the bigrams so we have a dataframe with: node1, node2, frequency
#there are some warning messages to with apostrophe in word in bigram (separate doesn't know how to deal)
df2 <- df1 %>%
  count(ngram, sort = TRUE) %>%
  separate(ngram, c("node1","node2"))

head(df2, 20)

nrow(df2)

```

It is apparent that many of the highly-frequent bigrams involve one or more stopwords: these bigrams (edges) will dominate the semantic network, and will not provide much analytical insight and further, they will make the semantic network very highly connected and therefore less likely for clusters (which may be interpreted as frames) to emerge.

We will remove the bigrams that involve one or more stopwords.

```{r}
df2 <- df2 %>% anti_join(stop_words, join_by(node1 == word)) %>% 
  anti_join(stop_words, join_by(node2 == word))

head(df2, 20)

nrow(df2)
```

This has significantly reduced the number of bigrams (by about two thirds), and perusal of the top-20 bigrams (in terms of frequency) indicates that they are informative.

Before processing the bigrams to create a semantic network, it is somewhat useful to check if the top phrases on the bigrams seem to be all relevant for analysis. For example, the phrase 'google scholar' above might be informative or might not. To do this, we can locate the text into the wider context by using `kwic` (keyword-in-context) syntax from `quanteda` package.

```{r kwic}
library(quanteda)

#create corpus (text stored in 'text' column so don't need to specify text_field argument)
corpus1 <- corpus(textContent)

#remove numbers, punctuations, separators, and urls
tokens1  <- tokens(corpus1, remove_numbers = TRUE, remove_punct = TRUE, remove_separators = TRUE, remove_url = TRUE )

#convert the tokens to lower case
tokens1_lower  <- tokens_tolower(tokens1)
#remove stopwords
tokens1_clean  <- tokens_remove(tokens1_lower, c(stopwords('en')))

tokens1_ngram <- tokens_ngrams(tokens1_clean, n = 1:2)

#explore keywords in context
gs <- kwic(tokens1_ngram, pattern = phrase("google scholar"))
head(gs, 10)

```
It seems that the terms 'google scholar' appear simply as of references or indexes of scholarly publications cited on the webpages. So we decided to remove that phrase from our data frame.

```{r}
#remove a row containing 'google scholar' from the data frame
df2 <- df2[-5,]
```

However, the number of bigrams (`r nrow(df2)`) will make semantic network visualisation challenging, and regardless, it is often analytically more interesting to focus on those bigrams that are more commonly used (this is similar to word frequency analysis, where word clouds and comparison clouds are only constructed for the top 100 or 200 words, for example).

The following shows that the vast majority of bigrams only appeared once or twice on the web pages in the study: we will remove these uncommonly-occurring bigrams and focus on those that occurred three or more times. 

```{r}

table(df2$n)

df2 <- df2 %>% filter(n>=3)

nrow(df2)

```


We will now use `igraph` to construct a semantic network using the dataframe containing the bigrams and their frequency counts.

```{r}
#igraph won't accepting a tibble, so convert to standard dataframe
library(igraph)
g_sem <- graph_from_data_frame(as.data.frame(df2), directed=FALSE)
g_sem <- delete.vertices(g_sem, which(V(g_sem)$name=="NA"))       #remove NA node (problem with parsing)

vcount(g_sem)
ecount(g_sem)
```


```{r}

#print to graphml, so we can visualise in Gephi
write.graph(g_sem, "semantic_network_frames.graphml", format="graphml")


```


## 3. Semantic network using co-occurrence in sentence 





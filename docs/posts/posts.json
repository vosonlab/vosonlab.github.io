[
  {
    "path": "posts/2023-07-21-changes-to-apis-what-we-know-and-what-can-we-do-with-the-voson-lab-suite-of-tools/",
    "title": "Changes to APIs: Mapping the implications in data collection using the VOSON Lab suite of tools",
    "description": "Addressing what we know about the changes, what we can currently do with the VOSON Lab tools and future developments.",
    "author": [
      {
        "name": "Robert Ackland",
        "url": "https://orcid.org/0000-0002-0008-1766"
      },
      {
        "name": "Francisca Borquez V.",
        "url": "https://orcid.org/0009-0009-7755-374X"
      },
      {
        "name": "Bryan Gertzel",
        "url": {}
      }
    ],
    "date": "2023-07-21",
    "categories": [
      "rstats",
      "Twitter",
      "Reddit",
      "API",
      "vosonSML",
      "VOSONDash",
      "voson.tcn"
    ],
    "contents": "\n1. Introduction\nRecently, social media companies have announced major changes to API access and data, specifically for Twitter and Reddit. In this post we cover what we have found so far, and the implications in the use of the VOSON lab suit of tools: vosonSML, VOSONDash and voson.tcn, as at July 2023.\n2. Reddit\n2.1 Changes to Reddit API\nReddit announced changes to its API effective on 30 June 2023. This publication summarises the changes to the API. Basically, Reddit is restricting API access to commercial entities that require large-scale data usage. Reddit data can still be accessed for free for academic purposes (non-profit), as long as it is below the published data-usage threshold.\nAs of July 1, 2023, Reddit enforced two different rate limits for the free access tier:\n100 queries per minute per OAuth client id (i.e. those using OAuth for authentication)\n10 queries per minute, for those who are not using OAuth for authentication.\n2.2 Can I collect Reddit data using the VOSON Lab tools?\nAs at July 2023, it is possible to access Reddit data using vosonSML (v. 0.37.7) and VOSONDash (0.5.11). The Reddit API end-point used does not require OAuth authentication for vosonSML, which means that there should not be problems with small collections.\n2.3 Future developments\nThere is currently a default wait time of 3-5 seconds between collection requests. Users can change this default behaviour by modifying the WaitTime parameter in the vosonSML Collect() function, as documented here.\nGiven Reddit’s statement that there is a maximum of 10 API requests per minute (when OAuth authentication is not used), then users should set the WaitTime parameter to a minimum of 6 seconds so as to avoid hitting the rate limit. However as above, we expect the current version of vosonSML will be fine for small Reddit collections, and therefore, VOSON Dashboard can be used for small collections, with the current default wait time. We will be updating the default wait time in a future version of vosonSML.\n3. Twitter\n3.1 Changes to Twitter API\nFor Twitter, changes to the API access have been significant. This post by rtweet developers summarises the changes of API access since February until June 2023. On 9 February 2023, Twitter cut off free access to both of its APIs (v2 and v1.1) and included a paid basic tier. Legacy Twitter API access tiers, such as Standard (v1.1), Essential (v2), Elevated (v2), and Premium were deprecated by June 2023. Any applications and projects that a user had not switched over to one of the new tiers were automatically placed in the default new Free access tier, which supports V1.1 Media Endpoints, V1.1 OAuth endpoints (Login with Twitter), V2 Manage Tweet endpoints\nand V2 users/me endpoint, for write only use cases.\n3.2 Can I collect Twitter data using the VOSON Lab tools?\nBased on the information above, if you have a developer account, with access to the v.1.1 API endpoint, you should be able to collect Twitter data with vosonSML using a paid tier (note: as of July 2023 we in the VOSON Lab have not tested collection using a paid tier).\nAlso, please note that vosonSML only accesses the Twitter v1.1 API via rtweet and does not support the newer v2 API. You can refer to the voson.tcn package if you are interested in using the v2 API to collect and analyse Twitter conversation networks.\nAs of July 2023, there is still a lot of uncertainty around Twitter data collection, and in particular, what will happen to the previous Twitter access for academics.\n3.3 Future developments\nWe are keeping a close eye on developments with Twitter, or X(!?), but we are also extending the frontiers of VOSON by looking into other data sources such as Mastodon.\n4. YouTube\nAs of July 2023, there have been no changes to the YouTube API and tne VOSON tools can collect YouTube comment networks.\n5. WWW hyperlink networks\nOur collection of WWW hyperlink networks is, of course, unaffected by any changes to social media platforms.\n\n\n\n",
    "preview": "posts/2023-07-21-changes-to-apis-what-we-know-and-what-can-we-do-with-the-voson-lab-suite-of-tools/redddit.jpg",
    "last_modified": "2023-08-03T12:07:29+10:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-04-27-semantic-network-analysis-with-website-text/",
    "title": "Semantic network analysis with website text",
    "description": "How to construct semantic networks, based on word co-occurrence, using text extracted from websites.",
    "author": [
      {
        "name": "Robert Ackland",
        "url": "https://orcid.org/0000-0002-0008-1766"
      },
      {
        "name": "Francisca Borquez",
        "url": {}
      },
      {
        "name": "Sidiq Madya",
        "url": "https://orcid.org/0000-0002-8444-3145"
      }
    ],
    "date": "2023-05-26",
    "categories": [
      "rstats",
      "tidytext",
      "quanteda",
      "text analysis",
      "visualisation",
      "semantic networks",
      "frame analysis",
      "data sovereignty"
    ],
    "contents": "\n\nContents\n1. Introduction\n1.1 Semantic network analysis\n\n2. Semantic network using bigrams\n2.1 Removal of stopwords\n2.2 Constructing the semantic network using igraph\n2.3 Visualising the semantic network using Gephi\n2.4 Visualising the semantic network using igraph\n\n\n1. Introduction\nIn our three previous posts in this series (Hyperlink networks: data pre-processing techniques, Hyperlink networks and website text content and Analysis of website text content using quanteda), we processed and quantitatively analysed text extracted from websites of organisations involved in data sovereignty. In the present post we will conduct a type of semantic network analysis to understand how concepts relate to each other, and to observe framing structures emerging from the corpus.\n1.1 Semantic network analysis\nSemantic network analysis extends the social network analysis (SNA) approach, to measuring relationships between semantic units. In semantic networks, nodes are semantic concepts (words, topics, themes) and ties are associations between concepts (e.g. co-occurrence). Yang & González-Bailón (2018) (see Table 13.1 in p.330) distinguish different types of semantic network analysis according to the level of analysis (individual, interpersonal, collective), and the type of semantic network (concept-to-concept, actor-to-actor). In this blogpost, we conduct a type of semantic network analysis referred to as “salience and framing” (Yang & González-Bailón, 2018). This approach aggregates semantic units across a collective of individuals, therefore focusing on the concepts and their relationships, rather than on opinions/concepts associated with particular individuals, or in our case organisations, (this is referred to as “discourse network analysis”).\nOur approach involves constructing networks where the nodes are terms extracted from website text and a weighted edge between two terms indicates the number of times they co-occur in the website text. There are two ways of identifying co-occurrence of terms:\nBigrams – the terms co-occur because they are positioned directly next to each other in a sentence on a website\nCo-occurrence within sentences – the terms co-occur because they are located in the same sentence, are but not necessarily positioned next to each other.\nIn this blogpost we will use bigrams to identify co-occurrence of terms and in a later blogpost we will use sentences to identify co-occurence of terms.\nFinally, we will identify clusters of terms in semantic networks and will assess to what extent these clusters can be interpreted as frames. In this blogpost we will do this for all websites in our dataset, while in a later blogpost we will explore whether we can observe significant differences between frames used by Global North and Global South organisations.\n2. Semantic network using bigrams\nFirst, we read in the dataframe constructed in this post.\n\n\nlibrary(knitr)\ntextContent <- readRDS(\"textContent2.rds\")\n\n\nAs a reminder, here is a summary of what is contained in this dataframe.\n\n\ncolnames(textContent)\n\n[1] \"page\"   \"text\"   \"domain\" \"type\"  \n\nkable(head(textContent[,c(1,3,4)]))\n\npage\ndomain\ntype\nhttps://womeninlocalization.com/partners/\nwomeninlocalization.com\nNorth\nhttps://womeninlocalization.com/data-localization-laws-around-the-world/\nwomeninlocalization.com\nNorth\nhttps://iwgia.org/en/network.html\niwgia.org\nNorth\nhttps://www.iwgia.org/en/indigenous-data-sovereignty/4699-iw-2022-indigenous-data-sovereignty.html?filter_tag[0]=37\niwgia.org\nNorth\nhttps://indigenousdatalab.org/networks/\nindigenousdatalab.org\nNorth\nhttps://indigenousdatalab.org/projects-working/\nindigenousdatalab.org\nNorth\n\n#kable seems to have trouble with following...\nhead(substr(textContent$text,1,80))\n\n[1] \"Women in Localization (W.L.) is certified as a 501 (c)(3) tax-exempt nonprofit. \"         \n[2] \" Data localization laws can be a challenge for some and nearly nothing for other\"         \n[3] \"\\r\\n\\r\\n\\t\\tWritten on 11 March 2011. Posted in About IWGIA\\r\\n\\t Since 1968, IWGIA has w\"\n[4] \"\\r\\n\\r\\n\\t\\tWritten on 01 April 2022. Posted in Indigenous Data Sovereignty\\r\\n\\t Indigen\"\n[5] \"Collaboratory for Indigenous Data Governance \\n\\t\\t\\tResearch, Policy, and Practice \"     \n[6] \"Collaboratory for Indigenous Data Governance \\n\\t\\t\\tResearch, Policy, and Practice \"     \n\nWith framing semantic network analysis, we will only be making use of text and type columns. First we will attempt to identify potential frames that are present in the website text authored by all organisations in our dataset, and then we will see if there is any difference in the framing by Global North and Global South organisations.\nFor this part of the exercise, we will use tidytext, to identify bigrams, i.e. terms that co-occured because they are positioned directly next to each other in a sentence or in a page.\n\n\nlibrary(tidytext)\nlibrary(tidyverse)\nlibrary(dplyr)\n\n#we will also remove bigrams that contain numbers\ndf1 <- textContent %>% unnest_tokens(ngram, text, token = \"ngrams\", n = 2) %>%\n  filter(!grepl(\"[[:digit:]]\", ngram))\nnrow(df1)\n\n[1] 131819\n\n#split the bigrams so we have a dataframe with: node1, node2, frequency\n#there are some warning messages to with apostrophe in word in bigram (separate doesn't know how to deal)\ndf2 <- df1 %>%\n  count(ngram, sort = TRUE) %>%\n  separate(ngram, c(\"node1\",\"node2\"))\n\nhead(df2, 20)\n\n        node1       node2   n\n1          of         the 873\n2          in         the 593\n3          to         the 370\n4         and         the 310\n5        data sovereignty 266\n6         for         the 252\n7          of        data 246\n8          on         the 236\n9  indigenous        data 217\n10         is           a 214\n11       with         the 203\n12         at         the 181\n13     access          to 176\n14       data       space 174\n15 indigenous     peoples 161\n16         by         the 159\n17        the        data 157\n18         in           a 153\n19         as           a 146\n20         it          is 143\n\nnrow(df2)\n\n[1] 60071\n\n2.1 Removal of stopwords\nThe resulting data frame contains 23,842 rows. It is apparent that many of the highly-frequent bigrams involve one or more stopwords: these bigrams (edges) will dominate the semantic network, and will not provide much analytical insight and further, they will make the semantic network very highly connected and therefore less likely for clusters (which may be interpreted as frames) to emerge.\nWe will remove the bigrams that involve one or more stopwords. We will also retain only those bigrams where both words have more than three characters.\n\n\ndf2 <- df2 %>% anti_join(stop_words, join_by(node1 == word)) %>% \n  anti_join(stop_words, join_by(node2 == word))\n\nhead(df2, 20)\n\n           node1       node2   n\n1           data sovereignty 266\n2     indigenous        data 217\n3           data       space 174\n4     indigenous     peoples 161\n5         google     scholar 127\n6           data  governance 126\n7           data     sharing 125\n8           data      spaces 108\n9       national    security  89\n10        health        care  74\n11    indigenous communities  74\n12          data    exchange  70\n13          data  protection  70\n14         human      rights  64\n15      personal        data  63\n16       primary        care  56\n17        pubmed      google  56\n18          data      driven  53\n19         cross      border  48\n20 international        data  48\n\nnrow(df2)\n\n[1] 20422\n\ndf2 <- df2 %>% filter(nchar(node1)>2 & nchar(node2)>2)\nnrow(df2)\n\n[1] 19505\n\nThis action has significantly reduced the number of bigrams (by about two thirds), and perusal of the top-20 bigrams (in terms of frequency) indicates that they are informative.\nBefore processing the bigrams to create a semantic network, it is somewhat useful to check if the top phrases on the bigrams seem to be all relevant for analysis. For example, the phrase ‘google scholar’ above might be informative or might not. To do this, we can locate the text into the wider context by using kwic (keyword-in-context) syntax from quanteda package.\n\n\nlibrary(quanteda)\n\n#create corpus (text stored in 'text' column so don't need to specify text_field argument)\ncorpus1 <- corpus(textContent)\n\n#remove numbers, punctuations, separators, and urls\n#tokens1  <- tokens(corpus1, remove_numbers = TRUE, remove_punct = TRUE, remove_separators = TRUE, remove_url = TRUE )\ntokens1  <- tokens(corpus1)\n\n#convert the tokens to lower case\n#tokens1_lower  <- tokens_tolower(tokens1)\n#remove stopwords\n#tokens1_clean  <- tokens_remove(tokens1_lower, c(stopwords('en')))\n\n#tokens1_ngram <- tokens_ngrams(tokens1_clean, n = 1:2)\n\n#explore keywords in context\ngs <- kwic(tokens1, pattern = phrase(\"google scholar\"))\n#gs <- kwic(corpus1, pattern = phrase(\"google scholar\"))\nhead(gs, 10)\n\nKeyword-in-context with 10 matches.                                                                   \n [text47, 5140:5141]    ). PubMed PubMed Central | Google Scholar |\n [text47, 5171:5172]          2019 ). CAS PubMed | Google Scholar |\n [text47, 5214:5215]    ). PubMed PubMed Central | Google Scholar |\n [text47, 5293:5294] . CAS PubMed PubMed Central | Google Scholar |\n [text47, 5397:5398]            ( 2006 ). PubMed | Google Scholar |\n [text47, 5439:5440]               ( 2019 ). CAS | Google Scholar |\n [text47, 5492:5493]               502 ( 2015 ). | Google Scholar |\n [text47, 5548:5549]    ). PubMed PubMed Central | Google Scholar |\n [text47, 5594:5595]    ). PubMed PubMed Central | Google Scholar |\n [text47, 5634:5635] . CAS PubMed PubMed Central | Google Scholar |\n                   \n Amann, R. I       \n Salter, B.&       \n David-Chavez, D. M\n Rainie, S. C      \n Garrison, N. A    \n Cohn, E. G        \n Scherr, C. L      \n Bonevski, B. et   \n Martin, A. R      \n Geary, J.,        \n\nWhile prominent, the output shows that the bigram ‘google scholar’ appears simply as of references or indexes of scholarly publications cited on the webpages. So, we decided to remove that phrase from our data frame.\n\n\n#remove the row containing 'google scholar' from the data frame\n#base R\ndf2a <- df2[-which(df2$node1==\"google\" & df2$node2==\"scholar\"),]\nnrow(df2a)\n\n[1] 19504\n\n#dplyr\n#note: dplyr::filter also removes rows containing NA (none here)\ndf2b <- df2 %>% filter(!(node1==\"google\" & node2==\"scholar\"))\nnrow(df2b)\n\n[1] 19504\n\n#the filtered dataframes are identical\ndf2 <- df2a\n\n\nHowever, the number of bigrams (19504) will make semantic network visualisation challenging, and regardless, it is often analytically more interesting to focus on those bigrams that are more commonly used (this is similar to word frequency analysis, where word clouds and comparison clouds are only constructed for the top 100 or 200 words, for example).\nThe following shows that the vast majority of bigrams only appeared once or twice on the web pages in the study: we will remove these uncommonly-occurring bigrams and focus on those that occurred three or more times.\n\n\ntable(df2$n)\n\n\n    1     2     3     4     5     6     7     8     9    10    11 \n12755  5055   439   491   142   203    50    66    32    42    12 \n   12    13    14    15    16    17    18    19    20    21    22 \n   74    13    19     8    17     8    10     4     7     3     8 \n   23    24    25    26    27    28    29    30    31    32    33 \n    2     2     1     1     1     2     2     1     1     3     3 \n   34    40    41    46    48    53    56    63    64    70    74 \n    2     1     3     1     3     1     2     1     1     2     2 \n   89   108   125   126   161   174   217   266 \n    1     1     1     1     1     1     1     1 \n\ndf2 <- df2 %>% filter(n>=3)\n\nnrow(df2)\n\n[1] 1694\n\n2.2 Constructing the semantic network using igraph\nWe will now use igraph to construct a semantic network using the dataframe containing the bigrams and their frequency counts.\n\n\n#igraph won't accepting a tibble, so convert to standard dataframe\nlibrary(igraph)\ng <- graph_from_data_frame(as.data.frame(df2), directed=FALSE)\ng <- delete.vertices(g, which(V(g)$name==\"NA\"))       #remove NA (parsing problem)\n\nvcount(g)\n\n[1] 1320\n\necount(g)\n\n[1] 1694\n\n#rename edge attribute 'n' to 'weight' - used in community/cluster identification\nE(g)$weight <- E(g)$n\ng <- delete_edge_attr(g, \"n\")\n\n\nWe do have some instances of loops and multiple edges. This is not surprising since, for example, “data sharing” and “sharing data” have different meanings but will result in a multiple edges connecting “data” and “sharing”. For this exercise, we could simply ignore the loops and multiple edges since they are unlikely to have a material impact on our analysis, but instead we will use the igraph simplify function to remove the loops and multiple edges. Note that by default, the simplify function will sum the edge weights of multiple edges.\n\n\nE(g)[which(is.loop(g, E(g)))]\n\n+ 3/1694 edges from b2d24eb (vertex names):\n[1] pubmed--pubmed data  --data   tech  --tech  \n\nE(g)[which(is.multiple(g, E(g)))]\n\n+ 33/1694 edges from b2d24eb (vertex names):\n [1] https     --doi        data      --exchange  \n [3] data      --sharing    data      --usage     \n [5] data      --indigenous doi       --http      \n [7] data      --based      cipesa    --org       \n [9] data      --privacy    schema    --org       \n[11] digital   --services   data      --access    \n[13] indigenous--women      data      --related   \n[15] south     --africa     maggie    --walter    \n[17] data      --challenge  data      --including \n[19] data      --makes      data      --sovereign \n+ ... omitted several edges\n\necount(g)\n\n[1] 1694\n\ng <- simplify(g)\necount(g)\n\n[1] 1658\n\nNow we write the graphml file so we can visualise the semantic network in Gephi.\n\n\nwrite.graph(g, \"semantic_network_frames.graphml\", format=\"graphml\")\n\n\n2.3 Visualising the semantic network using Gephi\nFirst, we visualise semantic network in Gephi. Note that we have created a sub network of the giant component, which contains 937 nodes and 1370 edges. We have used the modularity clustering algorithm and it identified 28 communities or clusters, and these are denoted by colours in the network visualisation. The visualisation layout is the ForceAtlas 2 (with options: LinLog mode, Prevent Overlap, Scaling=6).\n\n\n\nFigure 1: Semantic network\n\n\n\n\n\n\nFigure 2: Semantic network - zoomed in\n\n\n\nNext we visualise two of the clusters from the semantic network (health and national security), with updated node layouts.\n\n\n\nFigure 3: Semantic network - health cluster\n\n\n\n\n\n\nFigure 4: Semantic network - national security cluster\n\n\n\n2.4 Visualising the semantic network using igraph\nNext, we visualise selected clusters in the semantic network using igraph. Note that we will not attempt to visualise the entire semantic network using igraph: we regard Gephi to be more suitable for visualising large-scale networks such as the full semantic network constructed here.\nTo visualise selected clusters in the semantic network we will attempt to follow a similar workflow to what used above with Gephi. First, we identify the giant component and then use the Louvain modularity cluster algorithm (which is the same algorithm as used in Gephi) to identify clusters in the giant component.\n\n\ncc <- components(g)\ng2 <- induced_subgraph(g,which(cc$membership == which.max(cc$csize)))\n\nset.seed(222)                     #set random seed (there is randomness in cluster generation)\nmod <- cluster_louvain(g2)         #community identification using modularity (same algorithm as in Gephi)\n\ntable(mod$membership)\n\n\n  1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17 \n175  75  61  71   8  21  55  69  17  71  37  35   9  17  35  23  27 \n 18  19  20  21  22  23  24  25  26  27  28  29  30 \n 21   4   3   5  64   2   8   8   5   2   3   2   4 \n\n#sizes(mod)           #easier way of doing the same thing...\n\n\nNote that due to the fact that the modularity clustering algorithm produces slightly different clusters each time it is run (due to use of random number generation in the algorithm), the number of clusters identified (30) is different to what was found with Gephi.\nThe following code defines some functions that we will use for plotting the networks; note that some of this code has been adapted from the code used in VOSON Dashboard.\n\n\nnorm_values <- function(x) {\n  # all values the same\n  if (var(x) == 0) { return(rep(0.1, length(x))) }\n\n  min_x <- min(x)\n  diff_x <- max(x) - min_x\n  s <- sapply(x, function(y) { (y - min_x) / diff_x })\n}\n\nplot_cluster <- function(cid){\n  \n    g2t <- induced.subgraph(g2, which(mod$membership == cid))\n    #cat(\"cluster number:\", cid, \"size:\", vcount(g2t), \"\\n\")\n    #print(vcount(gt))\n    \n    plot_parameters <- list(g2t, edge.arrow.size = 0.4)\n    plot_parameters['vertex.label.color'] <- \"#000000\"\n    plot_parameters['vertex.label.family'] <- \"Arial\"\n    plot_parameters[['vertex.label.cex']] <- (norm_values(degree(g2t))) + 0.6\n    plot_parameters['vertex.label.dist'] <- 0.6\n    plot_parameters['vertex.label.degree'] <- -(pi)/2\n    plot_parameters[['vertex.size']] <- 4\n    plot_parameters[['edge.width']] <- (norm_values(E(g2t)$weight)) + 1\n      \n    #graph_layout <- layout_nicely(g2t, dim = 2)\n    graph_layout <- layout_with_graphopt(g2t)\n      \n    graph_layout <- igraph::norm_coords(graph_layout, ymin = -1, ymax = 1, xmin = -1, xmax = 1)\n    plot_parameters['rescale'] <- FALSE\n    plot_parameters[['layout']] <-  graph_layout * 1\n    par(mar = rep(0, 4))\n    #png(\"file.png\", width=800, height=800)\n    do.call(plot.igraph, plot_parameters)\n    #dev.off()\n}\n\n\nWe will only plot the largest clusters from the semantic network: we use an arbitrary threshold for cluster size (for a cluster to be plotted) of 30.\n\n\nwhich(sizes(mod) > 30)\n\n 1  2  3  4  7  8 10 11 12 15 22 \n 1  2  3  4  7  8 10 11 12 15 22 \n\nFor an overview, we will first plot the clusters in groups of 4.\n\n\npar(mfrow=c(2,2))\nplot_cluster(1)\nplot_cluster(2)\nplot_cluster(3)\nplot_cluster(4)\n\n\n\nFigure 5: Clusters 1, 2, 3, 4\n\n\n\n\n\n\nFigure 6: Clusters 7, 8, 10, 11\n\n\n\n\n\n\nFigure 7: Clusters 12, 15, 22\n\n\n\nNext we look at three clusters more closely and attempt to discern whatt issue (or issues) these clusters pertain to.\nNote that in what follows we are using a similar approach to that recommended by Miller (1997): we identify the issue (or issues) represented by a cluster, or interpret the cluster as a frame, by writing a simple statement that includes terms extracted from the cluster. It should be emphasised that for this illustrative example we have not conducted an in-depth examination of exactly how the identified terms are being used on the websites. If we were to fully implement this approach in an actual research project, we would be conducting (for example) keyword-in-context to manually check that our interpretation of the issues/frames is valid.\n2.4.1 Cluster 2 - Indigenous people\n\n\n\nFigure 8: Cluster 2\n\n\n\nIn this cluster, we identify a number of issues related to problems experienced by indigenous people and communities with regard to data sovereignty. These problems include the protection of status of minority populations, the fundamental right of indigenous people, their living conditions and the conditions of their environments such as the lands, forests and languages. The figure shows the connections between the term ‘people’ and other words such as ‘displaced’, ‘million’, ‘living’, which may indicate the presence of narratives describing vulnerable indigenous populations who are subject to displacement. The co-occurrence of ‘rights’ and ‘human’ suggests that the problem of indigenous rights is discussed within the broader context of human rights or at least both are inseparably connected. As a community of native, tribe, and nation, indigenous people are also described being surrounded with risks and they have to face another challenges such as gender inequality which imply that indigenous women are in a vulnerable position and in need of support. Based on this preliminary analysis, our interpretation is that data sovereignty notions are being discussed in the context of addressing these complex problems faced by indigenous people.\n2.4.2 Cluster 3 - national security\n\n\n\nFigure 9: Cluster 3\n\n\n\nThis cluster pertains to security issues. It appears that security issues were discussed in at least two main areas: the problem of the national security and the practice of intelligence agencies. When it comes to national security, we identify issues relating to technical/legal requirements to support the data sovereignty agenda, national borders, resources or funding, and the roles of universities. Issues around the practice of intelligence agencies include their surveillance activities, the role of officials and law enforcement in dealing with issues such as counterspionage, cybersecurity and cryptography. Semantic connections between the term ‘intelligence’ with ‘artificial’, ‘signals’, ‘gathering’ might express the emergence of AI as a new security issue. Meanwhile, the practices of intelligence agencies for gathering information or data which create a security threat to other states are indeed part of security concerns for nation states. Based on this figure, our interpretation is that data sovereignty has implications for the two interrelated issues of the practice of intelligence agencies and threats to national security.\n2.4.3 Cluster 10 - data policy and accessibility\n\n\n\nFigure 10: Cluster 10\n\n\n\nThis cluster appears to map issues centered around data policy and accessibility, addressing how local governments and authorities deal with foreign companies or firms (from China and the United States) that invest in and supply services and equipment relating to data infrastructure. The on-going rivalry between Chinese and American suppliers of technology is likely to feature in discussion on these websites, as is the connection of the Chinese private sector to the Chinese Communist Party. In terms of policy, we discern that discussions also touch upon ICT development and the threat of surveillance from foreign entities, and concerns of local governments over the rules and regulations can be read as a response to such conditions.\n\n\n\nMiller, M. M. (1997). Frame mapping and analysis of news coverage of contentious issues. Social Science Computer Review, 15, 367–378.\n\n\nYang, S. J. & González-Bailón, S. (2018). Semantic networks and applications in public opinion research (chapter 13). In J. Victor, A. Montgomery & M. Lubell (Eds.), The Oxford Handbook of Political Networks. Oxford University Press.\n\n\n\n\n",
    "preview": "posts/2023-04-27-semantic-network-analysis-with-website-text/gephi_giant_comp_zoomed.png",
    "last_modified": "2023-05-26T15:07:29+10:00",
    "input_file": {},
    "preview_width": 1013,
    "preview_height": 981
  },
  {
    "path": "posts/2023-04-13-website-text-analysis-with-quanteda/",
    "title": "Analysis of website text content using quanteda",
    "description": "Introduction to basic text analysis using the quanteda package, in the context of organisational website content.",
    "author": [
      {
        "name": "Robert Ackland",
        "url": "https://orcid.org/0000-0002-0008-1766"
      },
      {
        "name": "Sidiq Madya",
        "url": "https://orcid.org/0000-0002-8444-3145"
      },
      {
        "name": "Francisca Borquez",
        "url": {}
      }
    ],
    "date": "2023-04-27",
    "categories": [
      "rstats",
      "rvest",
      "quanteda",
      "text analysis",
      "visualisation",
      "word cloud",
      "comparison cloud",
      "data sovereignty"
    ],
    "contents": "\n\nContents\n1. Introduction\n2. Getting the text data ready for analysis\n3. Introducing text analysis with quanteda - a toy example\n3.1 What is text analysis (content analysis)?\n3.2 Steps in text analysis\n3.3 Toy example: Constructing the corpus\n3.4 Toy example: Text pre-processing\n3.5 Toy example: Text pre-processing\n\n4. Application to text content from ‘data sovereignty’ websites\n4.1 Initial exploration of particular keywords, using keywords-in-context (kwic)\n4.2 Frequency of terms and the document-feature matrix\n4.3 Using dictionary-based coding\n4.4 Comparison cloud\n\n\n1. Introduction\nIn our previous post in this series (Hyperlink networks and website text content), we used tidytext and other packages to conduct text analysis on organisational websites’ content, specifically word frequency counts, word clouds, comparison clouds. In this post, we will demonstrate how to use quanteda to undertake similar text analysis. The post will also introduce some more of the basics of text analysis e.g. constructing a corpus and document-feature matrix and dictionary-based coding. quanteda is an R package for natural language processing and analysis, and tutorials on using quanteda can be found here.\nThe text content used in this example involves text content from organisational websites involved in the discussion of data sovereignty; these data were collected using rvest (see Hyperlink networks: data pre-processing techniques).\n2. Getting the text data ready for analysis\nIn the first place, we load up the text dataframe that was created in the previous post in this series:\n\n\nlibrary(dplyr)\nlibrary(knitr)\n\ntextContent <- readRDS(\"textContent.rds\")\nstr(textContent)\n\n'data.frame':   75 obs. of  2 variables:\n $ page: chr  \"https://womeninlocalization.com/partners/\" \"https://womeninlocalization.com/data-localization-laws-around-the-world/\" \"https://iwgia.org/en/network.html\" \"https://www.iwgia.org/en/indigenous-data-sovereignty/4699-iw-2022-indigenous-data-sovereignty.html?filter_tag[0]=37\" ...\n $ text: chr  \"Women in Localization (W.L.) is certified as a 501 (c)(3) tax-exempt nonprofit. Its mission is to foster a glob\"| __truncated__ \" Data localization laws can be a challenge for some and nearly nothing for others. After all, everything depend\"| __truncated__ \"\\r\\n\\r\\n\\t\\tWritten on 11 March 2011. Posted in About IWGIA\\r\\n\\t Since 1968, IWGIA has worked for protecting, \"| __truncated__ \"\\r\\n\\r\\n\\t\\tWritten on 01 April 2022. Posted in Indigenous Data Sovereignty\\r\\n\\t Indigenous Peoples have alway\"| __truncated__ ...\n\nnrow(textContent)\n\n[1] 75\n\nThere are 75 URLs which we used rvest to collect website text content from. In the previous post we noted that a few of these pages are duplicates (http and https versions of the same page) and that there are some URLs that are links to PDF files (from which, rvest does not extract text data). Please note that, to simplify the process we are not pre-processing these issues this exercise.\nThe next step involves extracting the domain names from the URLs and use a manually-coded .csv file to categorise websites as to whether the represented organisations are based on the Global North or South (see the previous post for more on this).\n\n\n#function from https://stackoverflow.com/questions/19020749/function-to-extract-domain-name-from-url-in-r\ndomain <- function(x) strsplit(gsub(\"http://|https://|www\\\\.\", \"\", x), \"/\")[[c(1, 1)]]\n\ndd <- sapply(textContent$page, domain)\ntextContent$domain <- as.character(dd)\n\n#code websites as from Global North/South\ncoded <- read.csv(\"domains_coded.csv\")\n#> head(coded)\n#  X                       x  type\n#1 1 womeninlocalization.com north\n#2 2               iwgia.org north\n#3 3   indigenousdatalab.org north\n#4 4           botpopuli.net south\n#5 5              cipesa.org south\n#6 6              mydata.org north\n\n#now get the north/south classification into the dataframe containing the tokens\ntextContent$type <- coded$type[match(textContent$domain, coded$x)]\n\n#capitalise the labels, for the plot\ntextContent$type <- ifelse(textContent$type==\"north\", \"North\", \"South\")\n\nstr(textContent)\n\n'data.frame':   75 obs. of  4 variables:\n $ page  : chr  \"https://womeninlocalization.com/partners/\" \"https://womeninlocalization.com/data-localization-laws-around-the-world/\" \"https://iwgia.org/en/network.html\" \"https://www.iwgia.org/en/indigenous-data-sovereignty/4699-iw-2022-indigenous-data-sovereignty.html?filter_tag[0]=37\" ...\n $ text  : chr  \"Women in Localization (W.L.) is certified as a 501 (c)(3) tax-exempt nonprofit. Its mission is to foster a glob\"| __truncated__ \" Data localization laws can be a challenge for some and nearly nothing for others. After all, everything depend\"| __truncated__ \"\\r\\n\\r\\n\\t\\tWritten on 11 March 2011. Posted in About IWGIA\\r\\n\\t Since 1968, IWGIA has worked for protecting, \"| __truncated__ \"\\r\\n\\r\\n\\t\\tWritten on 01 April 2022. Posted in Indigenous Data Sovereignty\\r\\n\\t Indigenous Peoples have alway\"| __truncated__ ...\n $ domain: chr  \"womeninlocalization.com\" \"womeninlocalization.com\" \"iwgia.org\" \"iwgia.org\" ...\n $ type  : chr  \"North\" \"North\" \"North\" \"North\" ...\n\nNote that there are two domains/organisations for which we do not have a North/South code, but in these two cases there was no text data extracted from the web pages. There is an additional row in the dataframe where the text column is empty. So, we will remove the three domains from our analysis.\n\n\n#two domains are for web pages where no text data was collected\ntextContent %>% filter(is.na(type))\n\n                                               page text\n1 https://digitallibrary.un.org/record/218450?ln=en     \n2                             http://unstats.un.org     \n                 domain type\n1 digitallibrary.un.org <NA>\n2        unstats.un.org <NA>\n\n#we will remove these rows from the dataframe\ntextContent <- textContent %>% filter(!is.na(type))\n\n#note that there is still one other row in the dataframe where the text column is empty, \ntextContent %>% filter(text==\"\")\n\n                                        page text  domain  type\n1 https://doi.org/10.1016/j.jnma.2021.03.008      doi.org North\n\n#so we may as well remove this row here\ntextContent <- textContent %>% filter(text!=\"\")\n\n#number of web pages for which we have text content and the domain has been coded (Global North or South)\ntextContent %>% count()\n\n   n\n1 72\n\n#Number of unique domains\ntextContent %>% distinct(domain) %>% count()\n\n   n\n1 32\n\n#Number of domains from Global North and South\ntextContent %>% distinct(domain, .keep_all = TRUE) %>% count\n\n   n\n1 32\n\n#save this dataframe for use in later blog posts\nsaveRDS(textContent, \"textContent2.rds\")\n\n\nThe above shows that we now have 72 web pages from which we have collected text data. These pages are from 32 unique domains which we have coded as organisations from the Global North (21) or Global South (11).\n3. Introducing text analysis with quanteda - a toy example\nBefore proceeding with our analysis of text from websites discussing data sovereignty, it is useful to take a step back and introduce some more of the fundamentals of quantitative text analysis. This will also allow us to introduce the use of quanteda.\n3.1 What is text analysis (content analysis)?\nIn this post we use the terms content analysis and (quantitative) text analysis interchangeably. While we do not provide a thorough introduction to text analysis, there are many resources available such as: Popping (2017) who provides an overview of content analysis and some of the relevant software tools, and Welbers et al. (2017) who introduce computational text analysis using the R statistical software. Ackland (2013) (Section 2.4) provides a brief introduction to text analysis using data from the web.\nThe following quotes are useful:\n\n“Content analysis is a systematic reduction of a flow of text to a standard\nset of statistically manipulable symbols representing the presence, the\nintensity, or the frequency of some characteristics, which allows making\nreplicable and valid inferences from text to their context. In most\nsituations the source of the text is investigated, but this should not\nexclude the text itself, the audience or the receivers of the text. It\ninvolves measurement. Qualitative data are quantified for the purpose of\naffording statistical inference.” Popping (2017, p. 2)\n\n\n“Content analysis is a technique which aims at describing, with optimum\nobjectivity precision, and generalizability, what is said on a given subject\nin a given place at a given time….Who (says) What (to) Whom (in) What Channel (with) What Effect.” Lasswell et al. (1952, p. 34)\n\nContent analysis can be used to identify themes and the relationships between themes. The occurrence of themes, in combination with analysis of social structure (e.g. using network analysis), can be used to address research questions such as: What issues are being promoted by environmental activist organisations?, Who are the “agenda setters” with regards to issues in Australian politics?, How prominent is a theme in an online discussion (where prominence could be measured by the network position the person/people promoting the theme)?\n3.2 Steps in text analysis\nWe can identify the following main steps in text analysis.\nFirst, process the ‘flow of text’ into:\nDocuments: collections of words (spoken or written) with associated metadata such as date when the text was written, who authored the text etc. In our data sovereignty websites example, a document is the text collected from a web page.\nCorpus: collection of documents stored in one place.\nSecond, pre-process the corpus:\nTokenisation: splitting the text into tokens (most often, these are words). More generally, a token is a sequence of characters (usually delimited by space or punctuation) that make a semantic unit; could include words, multi-word expressions, named entities, stems or lemma.\nNormalisation: Lowercasing, stemming (words with suffixes removed, using a set of rules), lemmatisation (involves identifying the intended meaning of a word in a sentence or in a document).\nRemoving stopwords: Common words such as “the” in the English language are rarely informative about the content of a text.\nThe goal of pre-processing is to reduce the total number of types, where a type is a unique token. Note that the pre-processing stage may involve the use of a dictionary or lexicon - a controlled list of codes (tags) to reduce the number of types (e.g. by identifying synonyms) and to develop or identify themes, in the context of a particular research topic or question.\nThird, represent the corpus as a document-term matrix (DTM) or document-feature matrix (DFM):\nA DTM is a matrix in which the rows are documents, columns are terms, and cells indicate frequency of occurrence of terms in document\nKnown as a “bag-of-words” format\nAllows text data to be analysed using matrix algebra (we have now moved from text to numbers).\nNote that some authors distinguish “term” and “feature” but we use these interchangeably here.\n3.3 Toy example: Constructing the corpus\n\n\nlibrary(quanteda)\n#library(readtext)\n\n# create some documents\ntoydocs  <- c(\"A corpus is a set of documents.\", \n              \"This is the second document in the corpus.\", \n              \"A document contains tokens or various types\")\n\n# create some document variables\ntypedf  <- data.frame(type = c('definition', 'example', 'definition'), \n                      author = c(\"Adrian\", \"Rob\", \"Adrian\"))\n\n# Create text corpus\ntoycorpus <- corpus(toydocs, docvars = typedf)\n\n# Create tokens from corpus\ntoytokens  <-  tokens(toycorpus, remove_punct = TRUE)\n\ntoytokens\n\nTokens consisting of 3 documents and 2 docvars.\ntext1 :\n[1] \"A\"         \"corpus\"    \"is\"        \"a\"         \"set\"      \n[6] \"of\"        \"documents\"\n\ntext2 :\n[1] \"This\"     \"is\"       \"the\"      \"second\"   \"document\" \"in\"      \n[7] \"the\"      \"corpus\"  \n\ntext3 :\n[1] \"A\"        \"document\" \"contains\" \"tokens\"   \"or\"       \"various\" \n[7] \"types\"   \n\ndocvars(toycorpus)\n\n        type author\n1 definition Adrian\n2    example    Rob\n3 definition Adrian\n\nAs you can see, the corpus has three documents, called text1, text2, text3. Each document has a range of tokens. Even in the toy example, there are variations in the tokens that we might want to minimise: the aim is to reduce the number of “types” (unique tokens).\n\n\nndoc(toycorpus)\n\n[1] 3\n\nntoken(toytokens)\n\ntext1 text2 text3 \n    7     8     7 \n\nntype(toytokens)\n\ntext1 text2 text3 \n    7     7     7 \n\n3.4 Toy example: Text pre-processing\nHere are some typical text cleaning or pre-processing steps. Note the use of the pipe operator (%>%), which allows us to chain operations together.\n\n\n# remove stopwords\ntoytokens %>% tokens_remove(stopwords('en'))\n\nTokens consisting of 3 documents and 2 docvars.\ntext1 :\n[1] \"corpus\"    \"set\"       \"documents\"\n\ntext2 :\n[1] \"second\"   \"document\" \"corpus\"  \n\ntext3 :\n[1] \"document\" \"contains\" \"tokens\"   \"various\"  \"types\"   \n\n# other cleaning - lower case\ntoytokens %>% tokens_tolower()\n\nTokens consisting of 3 documents and 2 docvars.\ntext1 :\n[1] \"a\"         \"corpus\"    \"is\"        \"a\"         \"set\"      \n[6] \"of\"        \"documents\"\n\ntext2 :\n[1] \"this\"     \"is\"       \"the\"      \"second\"   \"document\" \"in\"      \n[7] \"the\"      \"corpus\"  \n\ntext3 :\n[1] \"a\"        \"document\" \"contains\" \"tokens\"   \"or\"       \"various\" \n[7] \"types\"   \n\n# stem the corpus to reduce the number of types\ntoytokens %>% tokens_wordstem()\n\nTokens consisting of 3 documents and 2 docvars.\ntext1 :\n[1] \"A\"        \"corpus\"   \"is\"       \"a\"        \"set\"      \"of\"      \n[7] \"document\"\n\ntext2 :\n[1] \"This\"     \"is\"       \"the\"      \"second\"   \"document\" \"in\"      \n[7] \"the\"      \"corpus\"  \n\ntext3 :\n[1] \"A\"        \"document\" \"contain\"  \"token\"    \"or\"       \"various\" \n[7] \"type\"    \n\n# extend tokens to include multi-word\ntoytokens %>% tokens_ngrams(n = 1:2)\n\nTokens consisting of 3 documents and 2 docvars.\ntext1 :\n [1] \"A\"         \"corpus\"    \"is\"        \"a\"         \"set\"      \n [6] \"of\"        \"documents\" \"A_corpus\"  \"corpus_is\" \"is_a\"     \n[11] \"a_set\"     \"set_of\"   \n[ ... and 1 more ]\n\ntext2 :\n [1] \"This\"            \"is\"              \"the\"            \n [4] \"second\"          \"document\"        \"in\"             \n [7] \"the\"             \"corpus\"          \"This_is\"        \n[10] \"is_the\"          \"the_second\"      \"second_document\"\n[ ... and 3 more ]\n\ntext3 :\n [1] \"A\"                 \"document\"          \"contains\"         \n [4] \"tokens\"            \"or\"                \"various\"          \n [7] \"types\"             \"A_document\"        \"document_contains\"\n[10] \"contains_tokens\"   \"tokens_or\"         \"or_various\"       \n[ ... and 1 more ]\n\n# chaining all of these together, and creating a new \"cleaned\" tokens object\ntoytokens_clean  <- toytokens %>% tokens_tolower() %>% \n  tokens_remove(stopwords('en')) %>% \n  tokens_wordstem() %>%\n    tokens_ngrams(n = 1)\n\n\n3.5 Toy example: Text pre-processing\nThe final step for this toy example is to create a document-feature matrix and explore the most frequent features.\n\n\ntoydfm  <-  dfm(toytokens_clean)\ntoydfm\n\nDocument-feature matrix of: 3 documents, 8 features (54.17% sparse) and 2 docvars.\n       features\ndocs    corpus set document second contain token various type\n  text1      1   1        1      0       0     0       0    0\n  text2      1   0        1      1       0     0       0    0\n  text3      0   0        1      0       1     1       1    1\n\ntopfeatures(toydfm)\n\ndocument   corpus      set   second  contain    token  various \n       3        2        1        1        1        1        1 \n    type \n       1 \n\nThe top features in the document-feature matrix could become keywords for content analysis.\n4. Application to text content from ‘data sovereignty’ websites\n\n\n#create corpus (text stored in 'text' column so don't need to specify text_field argument)\ncorpus1 <- corpus(textContent)\n#the other columns in the dataframe become the document variables\nhead(docvars(corpus1))\n\n                                                                                                                 page\n1                                                                           https://womeninlocalization.com/partners/\n2                                            https://womeninlocalization.com/data-localization-laws-around-the-world/\n3                                                                                   https://iwgia.org/en/network.html\n4 https://www.iwgia.org/en/indigenous-data-sovereignty/4699-iw-2022-indigenous-data-sovereignty.html?filter_tag[0]=37\n5                                                                             https://indigenousdatalab.org/networks/\n6                                                                     https://indigenousdatalab.org/projects-working/\n                   domain  type\n1 womeninlocalization.com North\n2 womeninlocalization.com North\n3               iwgia.org North\n4               iwgia.org North\n5   indigenousdatalab.org North\n6   indigenousdatalab.org North\n\nndoc(corpus1)\n\n[1] 72\n\nAs noted, the next step involves extracting standard linguistic units or tokens from the documents (website content). As before, it is a usual practice to ‘clean the corpus’ by removing stopwords, numbers and punctuation. However, there may be reasons why the researcher does not want to do such cleaning: it depends on the research question. It is worth noting that content analysis – as a counting-based method – is strongly affected by how the corpus is tokenised.\n\n\ntokens1  <- tokens(corpus1, remove_numbers = TRUE, remove_punct = TRUE, remove_separators = TRUE, remove_url = TRUE )\nprint(tokens1)\n\nTokens consisting of 72 documents and 3 docvars.\ntext1 :\n [1] \"Women\"        \"in\"           \"Localization\" \"W.L\"         \n [5] \"is\"           \"certified\"    \"as\"           \"a\"           \n [9] \"c\"            \"tax-exempt\"   \"nonprofit\"    \"Its\"         \n[ ... and 68 more ]\n\ntext2 :\n [1] \"Data\"         \"localization\" \"laws\"         \"can\"         \n [5] \"be\"           \"a\"            \"challenge\"    \"for\"         \n [9] \"some\"         \"and\"          \"nearly\"       \"nothing\"     \n[ ... and 1,179 more ]\n\ntext3 :\n [1] \"Written\" \"on\"      \"March\"   \"Posted\"  \"in\"      \"About\"  \n [7] \"IWGIA\"   \"Since\"   \"IWGIA\"   \"has\"     \"worked\"  \"for\"    \n[ ... and 2,959 more ]\n\ntext4 :\n [1] \"Written\"     \"on\"          \"April\"       \"Posted\"     \n [5] \"in\"          \"Indigenous\"  \"Data\"        \"Sovereignty\"\n [9] \"Indigenous\"  \"Peoples\"     \"have\"        \"always\"     \n[ ... and 3,944 more ]\n\ntext5 :\n [1] \"Collaboratory\" \"for\"           \"Indigenous\"    \"Data\"         \n [5] \"Governance\"    \"Research\"      \"Policy\"        \"and\"          \n [9] \"Practice\"      \"for\"           \"Indigenous\"    \"Data\"         \n[ ... and 102 more ]\n\ntext6 :\n [1] \"Collaboratory\" \"for\"           \"Indigenous\"    \"Data\"         \n [5] \"Governance\"    \"Research\"      \"Policy\"        \"and\"          \n [9] \"Practice\"      \"for\"           \"Indigenous\"    \"Data\"         \n[ ... and 1,536 more ]\n\n[ reached max_ndoc ... 66 more documents ]\n\n#number of tokens in the first document\nntoken(tokens1)[1]\n\ntext1 \n   80 \n\n#number of types in the first document\nntype(tokens1)[1]\n\ntext1 \n   53 \n\n#convert the tokens to lower case\ntokens1_lower  <- tokens_tolower(tokens1)\n#remove stopwords\ntokens1_clean  <- tokens_remove(tokens1_lower, c(stopwords('en')))\n\n#the dplyr way of doing the above\n#tokens1_clean  <- tokens1 %>% tokens_tolower() %>% tokens_remove(stopwords('en'))\n\n#number of tokens in the first document\nntoken(tokens1_clean)[1]\n\ntext1 \n   50 \n\n#number of types in the first document\nntype(tokens1_clean)[1]\n\ntext1 \n   37 \n\n# You could also stem the tokens to further reduce the number of types in the corpus\n# For the time being, we'll use data where stemming hasn't been applied\n\n\nThe above shows that converting to lower case and removing stopwords significantly reduces the number of tokens and the number of unique tokens (types): for document 1 the number of tokens decreases from 80 to 50 and the number of types decreases from 53 to 37.\nIn the following step, we transform the tokens to ngrams (specifically, unigrams (one-word concept) and bigrams (two-word concepts), so as to find multi-word tokens appearing in the corpus.\n\n\ntokens1_ngram <- tokens_ngrams(tokens1_clean, n = 1:2)\n\n\n4.1 Initial exploration of particular keywords, using keywords-in-context (kwic)\nGiven the documents are now loaded as corpus, you could begin content analysis by exploring some keywords. These keywords would depend on your research topic. For our exercise, we use keywords-in-context (kwic) to explore how the term “data” is being used in the web pages we have collected. This may allow us to identify associated terms that might become additional keywords for investigation. That is, keywords-in-context might lead to the discovery of new themes or sub-themes.\nKeywords-in-context can be plotted in quanteda using the textplot_xray command. The xray or ‘lexical dispersion’ plot provides a sense of variations in the usage of keywords across documents. The following figure shows that in some documents (e.g. 8 and 9) the word ‘data’ appears very frequently, while it doesn’t appear at all in document 1.\n\n\n#explore keywords and plot their distribution within documents\nkwic1 <- kwic(tokens1_ngram, c('data'))\nhead(kwic1)\n\nKeyword-in-context with 6 matches.                                                                     \n  [text2, 1]                                                   | data\n [text2, 14]          everything depends located sector making | data\n [text2, 25]             see increasing number countries adopt | data\n [text2, 33]              laws step better protection personal | data\n [text2, 39]                care everyone using computer sends | data\n [text2, 45] according australian government's guides personal | data\n                                                    \n | localization laws can challenge nearly           \n | localization due countries specific good         \n | localization sovereignty laws step better        \n | care everyone using computer sends               \n | according australian government's guides personal\n | covered laws include clearly together            \n\nlibrary(quanteda.textplots)\nlibrary(ggplot2)\ntextplot_xray(kwic1) + aes(color = keyword) + ggtitle(paste('Keywords in context:', 'data'))\n\n\n\n(#fig:kwic_xray)quanteda xray plot - 1\n\n\n\nThe following xray plot shows the frequency of usage of the keywords ‘data’, ‘personal’, and ‘localization’ across the corpus.\n\n\nkwic1 <- kwic(tokens1_ngram, c('data', 'personal', 'localization'))\n#head(kwic1)\n\ntextplot_xray(kwic1) + aes(color = keyword) + ggtitle(paste('Keywords in context:', 'data, personal, localization'))\n\n\n\n(#fig:kwic_xray2)quanteda xray plot - 2\n\n\n\n#png(\"xray_textplot_kwic.png\", width=800, height=800)\n#textplot_xray(kwic1) + aes(color = keyword) + ggtitle(paste('Keywords in context:', 'data, personal, localization'))\n#dev.off()\n\n\n4.2 Frequency of terms and the document-feature matrix\nA basic aspect of content analysis is to assess the importance of terms or features by computing their frequency of usage. This allows the identification of “important” terms in the corpus and it also may be used to assess whether keywords selected for analysis using keywords-in-context are important in the corpus. A further step (later in this exercise) is to assess how terms are related to one another (part of this would involve assessing how keywords studied using keywords-in-context are related other terms in the corpus).\nTo answer these questions we first create a document-feature matrix. Note the shift in terminology. In setting up the corpus, we talk about tokens. Now that the cleaning, simplification and standardisation is done, we talk about features (remember that in this blogpost we use ‘terms’ and ‘features’ interchangeably). Below we print the top-10 features (based on frequency) and we plot a word cloud of the 200 most frequently-used features.\n\n\ndfm1  <-  dfm(tokens1_ngram)\ndfm1 %>% topfeatures(10) %>% kable()\n\n\nx\ndata\n2906\n|\n984\nindigenous\n807\nhealth\n427\nresearch\n378\ncan\n369\nsovereignty\n346\naccess\n346\nservices\n337\nids\n336\n\ntextplot_wordcloud(dfm1, min_size = 1, max_words = 200)\n\n\n\nFigure 1: Word cloud\n\n\n\nAs we found in the previous post where we produced a word cloud produced with the tidytext and wordcloud packages, the word cloud is dominated by the presence of the word ‘data’. In the previous post, we used a customised stop word list to remove ‘data’ from the word cloud. In the present post, we will control for the dominance of the word ‘data’ using another approach. Specifically, we will make use of the term frequency-inverse document frequency (tf-idf) weighting scheme, which weight counts of features according to the how often they appear in that document vs. how many documents contain that feature. The tf-idf weighting scheme can often give a better sense of significance of a keyword in a document and a corpus. In the following code, we also use the tokens_select() function in quanteda to remove tokens that have fewer than three characters in length (this will remove the pipe character “|”, which is presently the second-ranked feature in terms of frequency of use).\n\n\ndfm1 <- tokens1_ngram %>% tokens_select(min_nchar = 3) %>% dfm()\ndfm1 %>% dfm_tfidf() %>% topfeatures(10) %>% kable()\n\n\nx\ndata\n565.4340\nindigenous\n466.9132\nids\n463.7510\nhealth\n247.0532\ndata_space\n210.8858\ngoogle_scholar\n197.6504\nindigenous_data\n179.1825\npubmed\n174.3059\nfraunhofer\n168.0807\nscholar\n161.9302\n\ndfm1 %>% dfm_tfidf() %>% textplot_wordcloud(min_size = 1, max_words = 200)\n\n\n\nFigure 2: Word cloud - 2\n\n\n\n4.3 Using dictionary-based coding\nCreating a dictionary is one way to code documents according to a specific set of questions or problems. A dictionary can also be used to handle synonyms.\n\n\ndict1  <-  dictionary(list(identity = c('identity', 'identities', 'ids'),\n                                member = c('member', 'members')))\n\ntokens2  <-  tokens_lookup(tokens1_ngram,\n    dictionary = dict1,  exclusive = FALSE)\n\ndfm2 <- tokens2 %>% tokens_select(min_nchar = 3) %>% dfm()\ndfm2 %>% dfm_tfidf() %>% topfeatures(10) %>% kable()\n\n\nx\ndata\n565.4340\nindigenous\n466.9132\nhealth\n247.0532\nidentity\n245.0384\ndata_space\n210.8858\ngoogle_scholar\n197.6504\nindigenous_data\n179.1825\npubmed\n174.3059\nfraunhofer\n168.0807\nscholar\n161.9302\n\ndfm2 %>% dfm_tfidf() %>% textplot_wordcloud(min_size = 1, max_words = 200)\n\n\n\nFigure 3: Word cloud - 3\n\n\n\nWith the use of the above dictionary, we find that ‘health’ becomes the third-top feature (when using the tf-idf weighting) and ‘identity’ moves to fourth. While we do not examine this here, by setting exclusive=FALSE in tokens_lookup() we can reduce the corpus to only those terms in the dictionary. A dictionary might cover several themes, or you might have separate dictionaries for each theme.\n4.4 Comparison cloud\nAs a final step in this blog post, we show how to produce a comparison cloud (comparing word use by organisations from the Global North and Global South) using quanteda.\n\n\ndfm3 <- dfm2 %>% dfm_group(type) %>% dfm_trim(min_termfreq = 3)\ntextplot_wordcloud(dfm3, comparison = TRUE, min_size = 1, max_words = 100, color = c(\"red\", \"blue\"))\n\n\n\nFigure 4: Comparison cloud\n\n\n\nThe word cloud above compares prominent words and phrases produced by the Global North and the Global South organisations. It appears that indigenous-related issues and health are the most prominent topic in the North. Using quanteda to perform unigram and bigram, it becomes possible to detect themes and a number of potential sub themes such as a topic of ‘indigenousity’ which is discussed in relation to the problem of ‘indigenous data’ and ‘indigenous peoples’ in the North. Other words such as ‘health’ and ‘services’ are also popular which give us an impression that another potential themes discussed could be related to the problem of health services. It can be said that indigenous data sovereignty and health services become two major concerns for organisations in the North when discussing data sovereignty issues.\nMeanwhile in the South, the word ‘digital’ and ‘policy’ along with names such as ‘India’, ‘Africa’ and ‘countries’ are interestingly dominant in the discussion. It may indicate that digital policy issues are central for organisations in the South compared at least to the North. Probably, India and African countries inform about the context of where such issues emerged. It is interesting to notice that other potentially meaningful phrases such as ‘right’, ‘development’ and ‘cross-border data flows’ are most frequently discussed in the South. A concept of the North/South divide might provide some clues to answer why organisations in different regions raise different concerns of issues when data sovereignty notions are debated on the web.\n\n\n\nAckland, R. (2013). Web social science: Concepts, data and tools for social scientists in the digital age. SAGE Publications.\n\n\nLasswell, H. D., Lerner, D. & S. Pool, I. d. (1952). Comparative study of symbols: An introduction. Stanford University Press.\n\n\nPopping, R. (2017). Online tools for content analysis. In R. M. L. N. Fielding & G. Blank (Eds.), The SAGE handbook of online research methods. SAGE.\n\n\nWelbers, K., Attenveldt, W. V. & Benoit, K. (2017). Text analysis in r. Communication Methods and Measures, 11, 245–265.\n\n\n\n\n",
    "preview": "posts/2023-04-13-website-text-analysis-with-quanteda/xray_textplot_kwic.png",
    "last_modified": "2023-05-26T13:45:50+10:00",
    "input_file": {},
    "preview_width": 800,
    "preview_height": 800
  },
  {
    "path": "posts/2023-02-23-hyperlink-networks-and-website-text-content/",
    "title": "Hyperlink networks and website text content",
    "description": "Conducting basic text analysis (frequency counts, word clouds, comparison clouds) of organisational website text content.",
    "author": [
      {
        "name": "Robert Ackland",
        "url": "https://orcid.org/0000-0002-0008-1766"
      },
      {
        "name": "Sidiq Madya",
        "url": "https://orcid.org/0000-0002-8444-3145"
      }
    ],
    "date": "2023-04-13",
    "categories": [
      "rstats",
      "rvest",
      "tidytext",
      "hyperlink networks",
      "vosonsml",
      "text analysis",
      "visualisation",
      "word cloud",
      "comparison cloud",
      "data sovereignty"
    ],
    "contents": "\n\nContents\n1. Introduction\n2. Identifying the web pages from which to collect text content\n3. Collecting the website text content\n3.1 Example web page\n3.2 Collecting paragraph text from all the web pages\n\n4. Word frequency analysis\nWord frequency barplot\nWordcloud\nComparison cloud\n\n\n1. Introduction\nThis is the second of a series of blog posts about collecting and analysing WWW hyperlink networks and text data. The first post focused on how to collect hyperlink data using the web crawler in vosonSML, and how to process the hyperlink data so as to produce hyperlink networks that can be used for research. The present post is focused on how to collect and process website text content, and we also provide some basic frequency analysis of words/terms extracted from the web pages.\n2. Identifying the web pages from which to collect text content\nThe vosonSML web crawler uses the rvest R package to crawl web pages but only the hyperlinks are extracted from these web pages: website text content is not retained.\nSo our plan is to use rvest directly (not via vosonSML) to crawl the relevant web pages and this time, we will extract and work with the text content (not the hyperlinks).\nTo find what pages we need to crawl, our starting point is the “seeds plus important websites” network that we created in the previous post. We saved this to a graphml file.\n\n\nlibrary(igraph)\nlibrary(dplyr)\nlibrary(knitr)\nlibrary(rvest)\n\ng <- read.graph(\"g_seedsImp2.graphml\", format=\"graphml\")\ng\n\nIGRAPH 417d86a DNW- 31 31 -- \n+ attr: type (g/c), name (v/c), seed (v/l), keep (v/n), id\n| (v/c), weight (e/n)\n+ edges from 417d86a (vertex names):\n[1] cipesa.org            ->un.org                     \n[2] cipesa.org            ->itu.int                    \n[3] datainnovation.org    ->itif.org                   \n[4] datasovereigntynow.org->internationaldataspaces.org\n[5] globaldatajustice.org ->itu.int                    \n[6] id4africa.com         ->un.org                     \n[7] id4africa.com         ->id-day.org                 \n+ ... omitted several edges\n\nWe want to collect text content for the 31 sites in this network. But it is important to remember that we are only interested in website text content that relates to our research topic (data sovereignty) and so we want to target the web pages on these sites that are are most likely to be related to our research topic.\nSo what we will do is use the raw web crawl data returned by vosonSML (see the previous post) to identify exactly what pages we need to collect website text content from.\n\n\n#read in the raw crawl data returned by vosonSML\ncrawlDF <- readRDS(\"crawlDF_20_sites_depth1.rds\")\n\n#find the pages that correspond to the sites in the \"seeds plus important\" network\ntextPages <- NULL\nfor (i in 1:vcount(g)){\n  #cat(\"site:\", V(g)$name[i], \", seed:\",V(g)$seed[i],\"\\n\")\n  ind <- grep(paste0(V(g)$name[i],\"$\"), crawlDF$parse$domain)\n  #cat(\"\\tnumber of pages:\", length(ind), \"\\n\")\n  textPages <- rbind(textPages, data.frame(domain=V(g)$name[i], \n                                           page=crawlDF$url[ind], seed=V(g)$seed[i]))\n}\n \n#it is possible that pages are not-unique (e.g. two seed pages can link to the same third page)\n#remove duplicate pages\ntextPages <- textPages %>% distinct(page, .keep_all = TRUE)\n\nhead(textPages)\n\n     domain                                                page seed\n1 ada-x.org          https://www.ada-x.org/a-propos/partenaires TRUE\n2 ada-x.org                            https://www.ada-x.org/en TRUE\n3 ada-x.org      https://www.ada-x.org/en/about/mandate-history TRUE\n4 ada-x.org             https://www.ada-x.org/en/about/partners TRUE\n5 ada-x.org                 https://www.ada-x.org/en/about/team TRUE\n6 ada-x.org https://www.ada-x.org/en/about/volunteer-internship TRUE\n\nnrow(textPages) \n\n[1] 1085\n\nThe above shows that if were were to collect text from all pages for the 31 websites in the “seeds plus important” network that have been identified (via the vosonSML web crawl), then we would need to collect 1085 web pages.\nThe following shows the number of pages identified for each of the seed sites.\n\n\n#number of pages identified for seed sites\nkable(textPages %>% filter(seed==TRUE) %>% group_by(domain) %>% summarise(n = n()))\n\ndomain\nn\nada-x.org\n34\nbotpopuli.net\n35\ncipesa.org\n86\ncis-india.org\n35\ncksindia.org\n69\ncryptoaltruism.org\n35\ndatainnovation.org\n70\ndatasovereigntynow.org\n13\ndigitalasiahub.org\n89\nedri.org\n35\nglobaldatajustice.org\n25\nid4africa.com\n41\nid4africakhub.org\n38\nindigenousdatalab.org\n24\ninternationaldataspaces.org\n85\nitif.org\n81\niwgia.org\n63\nmydata.org\n42\norfonline.org\n143\nwomeninlocalization.org\n1\n\nThe following shows the number of pages identified for each of the non-seed important sites.\n\n\n#number of pages identified for non-seed important sites\nkable(textPages %>% filter(seed==FALSE) %>% group_by(domain) %>% summarise(n = n()))\n\ndomain\nn\ndoi.org\n12\ngida-global.org\n2\nid-day.org\n2\nid4africa-ambassadors.org\n2\nitu.int\n2\nlawfareblog.com\n4\nmaiamnayriwingara.org\n3\nun.org\n3\nwhitehouse.gov\n2\nworldbank.org\n2\nwto.org\n7\n\nIt is apparent that there are many more web pages identified for seed sites, compared with non-seed sites. This makes sense, because only the seed sites have been crawled by vosonSML: so the crawler has identified these web pages by crawling the seed pages. In contrast, only a handful of pages have been identified for each non-seed important websites: these are pages that were linked to by the seed sites.\nIt should be noted out that only one unique page has been identified for the seed site womeninlocalization.org. However this is due to the fact that in the previous post we used pagegrouping to merge womeninlocalization.org and womeninlocalization.com together into a site labelled womeninlocalization.org. The following shows that the crawler in fact found over 60 pages for womeninlocalization.com.\n\n\nind <- grep(paste0(\"womeninlocalization.org\",\"$\"), crawlDF$parse$domain)\nlength(ind)\n\n[1] 2\n\ncrawlDF$url[ind]\n\n[1] \"https://womeninlocalization.org/membership-login\"\n[2] \"https://womeninlocalization.org/membership-login\"\n\nind <- grep(paste0(\"womeninlocalization.com\",\"$\"), crawlDF$parse$domain)\nlength(ind)\n\n[1] 67\n\nhead(crawlDF$url[ind])\n\n[1] \"https://womeninlocalization.com\"                        \n[2] \"https://womeninlocalization.com/about\"                  \n[3] \"https://womeninlocalization.com/about/advisory-board\"   \n[4] \"https://womeninlocalization.com/about/board-members\"    \n[5] \"https://womeninlocalization.com/about/program-directors\"\n[6] \"https://womeninlocalization.com/become-a-sponsor\"       \n\nFor the purposes of this exercise we do not want to be collecting text content from 1085 web pages: this would be a significant undertaking to collect and process these data. Another issue is that many of the web pages identified for the seed sites will not contain content relevant to our research topic of data sovereignty. This is because the web crawler is quite a blunt object: it is simply finding hyperlinks to pages within the seed site, and not taking account of whether these pages are relevant or not to our research topic.\nSo our strategy (in order to keep the exercise simple and also maximise the likelihood of collecting relevant text data) is as follows:\nFor the seed sites, we will collect text content only from the web pages we originally identified for the crawling\nFor the non-seed important sites, we will collect text content from the web pages identified by the crawler. By construction, these are pages that are linked to by two or or more seed sites (this is how we constructed the “seeds plus important” network in the previous post), and so we can expect that the text content on these pages is relevant to our study.\n\n\npages <- read.csv(\"seed_sites_20.csv\")\nkable(head(pages))\n\npage\ntype\nmax_depth\ndomain\ncountry\nhttps://womeninlocalization.com/partners/\nint\n1\nwomeninlocalization.com\nUS\nhttps://womeninlocalization.com/data-localization-laws-around-the-world/\nint\n1\nwomeninlocalization.com\nUS\nhttps://iwgia.org/en/network.html\nint\n1\niwgia.org\nDenmark\nhttps://www.iwgia.org/en/indigenous-data-sovereignty/4699-iw-2022-indigenous-data-sovereignty.html?filter_tag[0]=37\nint\n1\niwgia.org\nDenmark\nhttps://indigenousdatalab.org/networks/\nint\n1\nindigenousdatalab.org\nUS\nhttps://indigenousdatalab.org/projects-working/\nint\n1\nindigenousdatalab.org\nUS\n\n#just keep the columns we will work with\n#note that why \"domain\" is specified in this csv file, we will extract this\n#programmatically from the URL\npagesForText <- pages %>% select(page)\n\n#function from https://stackoverflow.com/questions/19020749/function-to-extract-domain-name-from-url-in-r\ndomain <- function(x) strsplit(gsub(\"http://|https://|www\\\\.\", \"\", x), \"/\")[[c(1, 1)]]\n#following could be done more elegantly, but it works...\ndd <- sapply(pagesForText$page, domain)\npagesForText$domain <- as.character(dd)\n\n#modify domain to account for fact we use (in seeds+important network) \n#womeninlocalization.org not womeninlocalization.com\n\npagesForText$domain[which(pagesForText$domain==\"womeninlocalization.com\")] <- \"womeninlocalization.org\"\n\n#not sure if we need this\npagesForText$seed <- TRUE\n\n#check that we've got all the seed pages\n#just a sanity check...\nseedDom <- V(g)$name[V(g)$seed==TRUE]\n#all OK\nlength(which(is.na(match(seedDom, unique(pagesForText$domain)))))\n\n[1] 0\n\nSo we have the web pages (for text collection) for the seed sites. Now we will add in the web pages for non-seed important sites.\n\n\n#dataframe with pages for non-seed sites\ndd <- textPages %>% filter(seed==FALSE) %>% relocate(page, domain, seed)\n\n#append to dataframe for seed sites\npagesForText <- rbind(pagesForText, dd)\n\nnrow(pagesForText)\n\n[1] 79\n\nSo we have 79 pages which we will use rvest to collect text content from. The following shows that there are some duplicated web pages for non-seed sites - the “http” and “https” version of the same page. We may adjust for this later on.\n\n\nkable(pagesForText %>% filter(grepl(\"gida-global.org|worldbank.org\", pagesForText$page)))\n\npage\ndomain\nseed\nhttp://www.gida-global.org\ngida-global.org\nFALSE\nhttps://www.gida-global.org\ngida-global.org\nFALSE\nhttp://www.worldbank.org\nworldbank.org\nFALSE\nhttps://www.worldbank.org\nworldbank.org\nFALSE\n\n3. Collecting the website text content\nHere we make use of the rvest R package (“Easily Harvest (Scrape) Web Pages”), which provides “wrappers around the ‘xml2’ and ‘httr’ packages to make it easy to download, then manipulate, HTML and XML.”\n3.1 Example web page\nUsing rvest can be quite a complicated, requiring an understanding of how web pages are structured. Here we use rvest in a very simple way: we only use it to extract website text that is contained in HTML paragraph tags.\nBefore using rvest to collect text content for all our web pages, it is useful to first look at its use for an example web page: Bot Populi which is an “alternative media platform dedicated to looking at all things digital from a social justice and Global South perspective.”\nFigure 1: Bot Populi “about us” web pageIf we look at the source code for this web page, we see the use of the HTML paragraph tags.\nFigure 2: Bot Populi “about us” web page - excerpt of source codeNow we will use rvest to download the web page and extract text content from the HTML paragraph tags.\n\n\ntesturl <- pagesForText$page[7]\ntesturl\n\n#fetch the webpage\ntestpage <- read_html(testurl)\n\n#As explained in following post:\n#https://stackoverflow.com/questions/57764585/how-to-save-and-read-output-of-read-html-as-an-rds-file\n#we need to use as.character() on the object returned by read_html()\n#if we want to save it to RDS file\n#we save the object returned by read_html() to file because below we are \n#just extracting text from paragraph html tags, but in future might want to\n#extract other tags\nsaveRDS(as.character(testpage), \"testpage.rds\")\n\n\n\n\ntestpage <- readRDS(\"testpage.rds\")\n#note that use of read_html() in following is because we above used\n#as.character() to save the object to RDS file\ntestpage <- read_html(testpage)\n\n#extract the paragraphs\np_text <- html_nodes(testpage, 'p') %>% html_text()\nhead(p_text, n=3)\n\n[1] \"Bot Populi is an alternative media platform dedicated to looking at all things digital from a social justice and Global South perspective. It was founded in 2018 by seven global, digital justice organizations.\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n[2] \"Digital technologies have been the transformative force of the 21st century, changing the way we live, work, and interact. Those who control data mark and mediate our social location in society. They also decide who is included and excluded from the digital ecosystem. Data, and the digital intelligence harvested from it, are today the purveyors of destiny. As data-based systems become the basis of development, important questions about justice and rights emerge.\\nWe explore these questions about the ways in which the digital world affects different aspects of our lives — in obvious and unexpected ways — through articles, podcasts, and videos. Our contributors include passionate and committed scholars, activists and visionaries from around the world working on issues of digital justice.\"\n[3] \"We aim to meaningfully shape the public discourse on digitalization. In the process, we seek to influence the norms, rules, and practices that contribute to transformative change.\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n\nIn this exercise we will just work with the paragraph text, but it should be noted that useful text content might be stored in other HTML tags such as titles (header tags “h1”, “h2” etc.) and anchor tags.\n3.2 Collecting paragraph text from all the web pages\nIn the code below we use the rvest read_html() function to collect the web pages for all our seed and non-seed sites, and we store the objects returned by read_html() in a list, and save this list object to an rds file for later use.\nBefore we collect the html data, we are going to remove some web pages that caused problems (rvest failed while collecting the html). For this exercise, we don’t try to figure out why these URLs are causing problems for rvest and we do not try to improve the code to make it more robust (e.g. via error handling).\nWe also note that some of the URLs are for pdf files - these would normally be removed.\n\n\n#first remove problematic URLs\n\nurl_problem <- c(\n  \"https://docs.wto.org/dol2fe/Pages/SS/directdoc.aspx?filename=q:/WT/GC/W798.pdf&Open=True xxiv\",\n\"https://doi.org/10.1002/ajs4.141\",  #Error in open.connection(x, \"rb\") : HTTP error 503.\n\"https://www.id4africa-ambassadors.org\",  #Error in open.connection(x, \"rb\") : HTTP error 404.\n\"https://www.id4africa-ambassadors.org/iic\"  #Error in open.connection(x, \"rb\") : HTTP error 404.\n)\n \npagesForText <- pagesForText %>% filter(!page %in% url_problem)\n\n#now collect the html and store in list\nL_html <- list()\n\nfor (i in 1:nrow(pagesForText)){\n  url_i <- pagesForText$page[i]\n  cat(\"working on:\", url_i, \"\\n\")  \n  L_html[[url_i]] <- read_html(url_i)\n  #if (i > 5)\n  #  break\n}\n\n#See note above about needing to use as.character() on \n#the object returned by read_html()\nfor (i in names(L_html)){\n  L_html[[i]] <- as.character(L_html[[i]])   \n}\nsaveRDS(L_html, \"L_html.rds\")\n\n\nNow we iterate over the webpages in the list, extracting and joining the paragraphs and putting them in the textContent dataframe that we will later use for text analysis.\n\n\nL_html <- readRDS(\"L_html.rds\")\n\n#this is not an efficient way of creating a dataframe, but will do here...\ntextContent <- NULL\nfor (i in names(L_html)){\n  \n  #cat(\"working on:\", i, \"\\n\") \n  \n  #note that use of read_html() in following is because we above used\n  #as.character() to save the object returned by read_html() to RDS file\n  p_text <- html_nodes(read_html(L_html[[i]]), 'p') %>% html_text()\n  #print(head(p_text))\n  textContent <- rbind(textContent, \n                       data.frame(page=i, text=paste0(p_text, collapse=\" \")))\n}\n\n#save the text content data frame for later use (in another blog post)\nsaveRDS(textContent, \"textContent.rds\")\n\n\n4. Word frequency analysis\nThe final part of this blog post involves a basic analysis of the website text content: computing word frequencies. We will use the tidytext package. In the following, we will only work with unigrams (we’ll look at bigrams in a later post).\nWord frequency barplot\nFirst, we will present the word frequencies in a barplot.\n\n\nlibrary(tidytext)\n\n#creating copy of the dataframe containing text, so can reuse code\ndf <- textContent\n\n#dplyr syntax\ndf <- df %>% unnest_tokens(word, text)\n\nnrow(df)\n\n[1] 138293\n\n#df %>% count(word)\n\n#remove stopwords (not really necessary for this dataset)\n#df %>%\n#  anti_join(get_stopwords())\n\n#Word frequency bar chart\nlibrary(ggplot2)\nlibrary(crop)\n\npng(\"word_frequency_barplot.png\", width=800, height=800)\ndf %>%\n    anti_join(stop_words) %>%\n    count(word, sort=TRUE) %>%\n    top_n(10) %>%\n    mutate(word = reorder(word, n)) %>%\n    ggplot(aes(word, n)) +\n    geom_col() +\n    xlab(NULL) +\n    coord_flip() +\n    theme_grey(base_size = 22)\ndev.off.crop(\"word_frequency_barplot.png\")\n\n\nFigure 3: Word frequency barplot.Wordcloud\nNext, we will present the word frequencies in a wordcloud.\n\n\nlibrary(wordcloud)\n\npng(\"wordcloud.png\", width=800, height=800)\ndf %>% \n  anti_join(stop_words) %>%\n  count(word) %>%\n  with(wordcloud(word, n, max.words = 200, random.order = FALSE, scale=c(5, 1)))\ndev.off.crop(\"wordcloud.png\")\n\n\nFigure 4: Wordcloud.The wordcloud is simply another way of visualising the word frequency counts: in the above wordcloud, the words that are used more often are larger and towards the centre of the figure. The wordcloud shows what we saw in the frequency barplot above: the term ‘data’ was the most frequently use term. This shouldn’t come as a surprise since by construction, issues that are being focused on by the websites in our data set are related to data, more specifically ‘data sovereignty’.\nWhile the wordcloud looks OK, there are a couple of simple things we can do to improve it. First, there are some numbers in the wordlcoud, so we will remove numbers from the terms extracted from the web pages. Second, the dominant word is ‘data’ but this is not really that informative since all the websites are by construction going to have something to do with data, so we will add the word ‘data’ to a custom stopword list.\n\n\n#remove tokens that are numbers\ndf <- df %>% filter(!grepl(\"[[:digit:]]\", word))\n\n#create a custom stopword dataframe\nstop_words2 <- rbind(stop_words, data.frame(word=\"data\", lexicon=\"custom\"))\n\npng(\"wordcloud2.png\", width=800, height=800)\ndf %>% \n  anti_join(stop_words2) %>%\n  count(word) %>%\n  with(wordcloud(word, n, max.words = 200, random.order = FALSE, scale=c(5, 1)))\ndev.off.crop(\"wordcloud2.png\")\n\n\nFigure 5: Improved wordcloud.The improved wordcloud (with ‘data’ removed) now more clearly reveals other key terms such as ‘indigenous’, ‘research’, and ‘health’. These words can be seen as a representation of texts indicating that a variety of issues such as indigenousity, research and health are being discussed by organisations addressing the ‘data sovereignty’ topic. Different organisations might be concerned with different issues while using the same term. For example, some organisations mention the term ‘data sovereignty’ when discussing about indigenous-related issues. Some others direct their discussion to address ‘research’ or ‘health’ related issues. This wordcloud provides a visual representation of texts which is very useful as a first step in our text analysis.\nComparison cloud\nFinally, we will present a comparison cloud showing the words used by (or most associated with) the Global North versus the those used by the Global South. To do this, we first need to manually create a Global North/South classification. We will do this by extracting the domains from the URLs, and then manually coding these domains as indicating that the organisation is from the Global North or South.\n\n\n#we use the function defined above, for extracting the domain from a URL\ndd <- sapply(df$page, domain)\ndf$domain <- as.character(dd)\n\n#now create a csv file containing the unique domains\nwrite.csv(unique(df$domain), \"domains.csv\")\n\n#we code up the domains according to whether the organisations are from global north or south\n#then read the coded csv file back into R\ncoded <- read.csv(\"domains_coded.csv\")\n#> head(coded)\n#  X                       x  type\n#1 1 womeninlocalization.com north\n#2 2               iwgia.org north\n#3 3   indigenousdatalab.org north\n#4 4           botpopuli.net south\n#5 5              cipesa.org south\n#6 6              mydata.org north\n\n#now get the north/south classification into the dataframe containing the tokens\ndf$type <- coded$type[match(df$domain, coded$x)]\n\n#capitalise the labels, for the plot\ndf$type <- ifelse(df$type==\"north\", \"North\", \"South\")\n\n#finally, we will create the comparison cloud\nlibrary(reshape2)\nlibrary(RColorBrewer)\n\ndark2 <- brewer.pal(8, \"Dark2\")\n\npng(\"comparison_cloud.png\", width=800, height=800)\ndf %>%\n    anti_join(stop_words2) %>%\n    group_by(type) %>%\n    count(word) %>%\n    acast(word ~ type, value.var = \"n\", fill = 0) %>%\n    comparison.cloud(color = dark2, title.size = 3,  scale = c(5,  1), random.order = FALSE, max.words = 200)\ndev.off.crop(\"comparison_cloud.png\")\n\n\nFigure 6: Comparison cloud.The comparison cloud gives us insights into the differences in keywords used by organisations in Global North and South when discussing the topic ‘data sovereignty’ on the web. It is assumed that North/South divide will be reflected in differences of issue of concern among organisations in both regions proxied by the use of vocabularies or words in their narratives. The graph shows that the terms such as ‘indigenous’, ‘health’, and ‘ids’ are more prominent in the North compared to their South counterpart. Whereas popular terms in the South mention the names of country or region such as ‘India’ and ‘Africa’, along with other terms include ‘digital, ’internet’, and ‘policy’. Our interpretation is that organisations in the North discuss ‘data sovereignty’ in relation to issues about indigenousity, health, and ids or identities. In the Global South, it might be the case that organisations involved in ‘data sovereignty’ debates are simply coming from India and Africa so that they mention these words most frequently, or they are mainly concerned with data issues in India or Africa. Furthermore, other keywords such as ‘digital’, ‘internet’, ‘policy’ appear frequently, and this suggests that the ‘data sovereignty’ topic is discussed by Global South organisations along with their concern of digital and internet policy issues in the South.\nA limitation of comparison clouds (and word clouds more generally) is that the location of words in the comparison cloud does not reflect semantic connections between the words and for this reason it is sometimes difficult to understand the context and meaning of words appearing in word clouds. In a future blog post we will use other quantitative text analysis techniques in an attempt to gain more insight into the differences between organisations in the Global North and South, in terms of how they are using the web to promote issues or present narratives relating to data sovereignty.\n\n\n\n",
    "preview": "posts/2023-02-23-hyperlink-networks-and-website-text-content/comparison_cloud.png",
    "last_modified": "2023-05-26T14:04:31+10:00",
    "input_file": {},
    "preview_width": 668,
    "preview_height": 698
  },
  {
    "path": "posts/2023-01-20-hyperlink-networks-pre-processing/",
    "title": "Hyperlink networks: data pre-processing techniques",
    "description": "Steps for collecting hyperlink networks with vosonSML and processing hyperlink data using R tools.",
    "author": [
      {
        "name": "Robert Ackland",
        "url": "https://orcid.org/0000-0002-0008-1766"
      },
      {
        "name": "Sidiq Madya",
        "url": "https://orcid.org/0000-0002-8444-3145"
      },
      {
        "name": "Francisca Borquez",
        "url": {}
      }
    ],
    "date": "2023-02-22",
    "categories": [
      "rstats",
      "hyperlink networks",
      "vosonsml"
    ],
    "contents": "\n\nContents\n1. Introduction\n2. Collecting hyperlink data via web crawling\n2.1 The seed set\n2.2 Data collection\n\n3. Creating hyperlink networks\n3.1 Activity network\n3.2 Actor network\n\n4. Processing nodes in the hyperlink actor network\n4.1 Pagegrouping\n4.2 Pruning\n4.3 Preserving\n\n\n1. Introduction\nThe VOSON software web app – first made publicly available in 2006 – was a tool designed to enable researchers to study Web 1.0 websites as online social networks (Ackland, 2010, 2013; Ackland & O’Neil, 2011; Lusher & Ackland, 2011). Hyperlink collection was reintroduced in VOSON R tools (vosonSML) in 2021) and it was made available in VOSONDash version 0.6.1 in February 2023. This post provides the methodological steps and code for collecting hyperlink data using vosonSML and pre-processing hyperlink network data via R, including the specific approaches that were previously available via VOSON software web app (pruning, preserving and pagegrouping).\n2. Collecting hyperlink data via web crawling\n2.1 The seed set\nThe organisations represented in this collection are a sample of non-state or non-government entities (for-profit and non-for-profit), which actively engage in ‘data sovereignty’ debates as part of their concern in contemporary data politics. These organisations include NGOs, research think tanks, companies, industry associations, and movements or initiatives from communities. These different group of organisations deal with various issues and values when promoting their agenda ranging from security, privacy, innovation, entrepreneurship, to human rights and social justice.\nBeing involved in the emerging issues of global data politics, these organisations are based or headquartered in different countries across the globe including the US, UK, Canada, Germany, The Netherlands, Belgium, and Denmark – which represent the Global North – and South Africa, India and Hong Kong, representing the Global South. The websites are being used by these organisations to participate in the emerging debates on data politics. Their participation in the debates are becoming more intense in the midst of the ongoing process of massive ‘digitisation’ and ‘datafication’ in societies. We have selected 1 or 2 pages from each website and are using these as “seed pages” i.e. starting points for the crawler. In this way, we are directing the crawler to the pages we think will be most “productive” in terms of containing hyperlinks and text that are of interest to the study.\n2.2 Data collection\nThe first step involves the collection of hyperlink data using vosonSML. To do so, we read a .csv file which includes the URLs of 20 organisations actively involved in data sovereignty debates and we create a dataframe containing those pages as seeds.\n\n\nlibrary(magrittr)             #only need to load this if we use %>% as pipe, otherwise can use |>\nlibrary(vosonSML)\nlibrary(igraph)\nlibrary(dplyr)\nlibrary(knitr)\nlibrary(stringr)\n\n#For more information on collecting hyperlink networks using vosonSML, see: #https://vosonlab.github.io/posts/2021-03-15-hyperlink-networks-with-vosonsml/.\n\n# The dataframe needs to have the folllowing fields, \n#for hyperlink collection via vosonSML:\n# page: the seed pages\n# type: the type of crawl, with allowed values:\n#   int: collect all hyperlinks but only follow links that have same domain as \n#        seed page (internal)\n#   ext: collect all hyperlinks but only follow links that have different domain\n#        as seed page (external)\n#   all: collect and follow all hyperlinks\n# max_depth: how many levels of hyperlinks to follow from seed page\n#For example:\n#pages <- data.frame(page = c(\"http://vosonlab.net\",\n#                             \"https://rsss.cass.anu.edu.au\",\n#                             \"https://www.ansna.org.au\"),\n#                    type = c(\"int\", \"ext\", \"all\"),\n#                    max_depth = c(1, 1, 1))\n\npages <- read.csv(\"seed_sites_20.csv\")\nkable(head(pages))\n\npage\ntype\nmax_depth\ndomain\ncountry\nhttps://womeninlocalization.com/partners/\nint\n1\nwomeninlocalization.com\nUS\nhttps://womeninlocalization.com/data-localization-laws-around-the-world/\nint\n1\nwomeninlocalization.com\nUS\nhttps://iwgia.org/en/network.html\nint\n1\niwgia.org\nDenmark\nhttps://www.iwgia.org/en/indigenous-data-sovereignty/4699-iw-2022-indigenous-data-sovereignty.html?filter_tag[0]=37\nint\n1\niwgia.org\nDenmark\nhttps://indigenousdatalab.org/networks/\nint\n1\nindigenousdatalab.org\nUS\nhttps://indigenousdatalab.org/projects-working/\nint\n1\nindigenousdatalab.org\nUS\n\n#remove pages that caused error with crawler\n#note: this error may no longer be present in latest version of vosonSML\npages <- pages %>% filter(!grepl(\"ispa.org.za\", pages$page))\n\n\nNote that for the crawler all we need is a data frame with three columns: page (URLs), type (int), max_depth (1). See code commented above for an explanation of these terms and also see the VOSON Lab blog post on crawling using vosonSML) for detailed steps. It might be useful to include other meta data in this file that is used in analysis later, for example the field Country as presented in the table above.\nNow, we run the crawl and save an .rds file with the data:\n\n\n#Remember to set `verbose=TRUE` to see the crawler working\ncrawlDF <- Authenticate(\"web\") %>% Collect(pages, verbose=TRUE)\n#crawlDF\n\n#We will save this dataframe, for use later\n#saveRDS(crawlDF, \"crawlDF.rds\")\nsaveRDS(crawlDF, \"crawlDF_20_sites_depth1.rds\")\n\n\n3. Creating hyperlink networks\nLet’s now create networks from the crawl data. First, we read in and inspect the dataframe structure. This data contains 2,826 rows and 9 columns.\n\n\ncrawlDF <- readRDS(\"crawlDF_20_sites_depth1.rds\")\n\n# explore dataframe structure\nglimpse(crawlDF)\n\nRows: 2,826\nColumns: 9\n$ url       <chr> \"http://goap-global.com\", \"http://www.reddit.com/s…\n$ n         <int> 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1,…\n$ page_err  <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ page      <chr> \"https://womeninlocalization.com/partners\", \"https…\n$ depth     <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ max_depth <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ parse     <df[,6]> <data.frame[23 x 6]>\n$ seed      <chr> \"https://womeninlocalization.com/partners\", \"ht…\n$ type      <chr> \"int\", \"int\", \"int\", \"int\", \"int\", \"int\", \"int\", \"…\n\n3.1 Activity network\nFirst, we create the activity network using vosonSML. In hyperlink activity networks, the nodes are web pages and edges are hyperlinks. For the purpose of visualisation, we will simplify the network by removing loops and multiple or parallel edges, using igraph.\n\n\n# create activity network: nodes are pages hyperlinks were collected from\nnet_activity <- crawlDF %>% Create(\"activity\")\ng_activity <- net_activity %>% Graph()\n#simplify the network - remove loops and multiple edges\ng_activity <- simplify(g_activity)\n\n\n\n\npng(\"activity_network.png\", width=800, height=800)\nplot(g_activity, layout=layout_with_fr(g_activity), vertex.label=\"\", \n     vertex.size=3, edge.width=1, edge.arrow.size=0.5)\ndev.off()\n\n\nFigure 1: Hyperlink activity network - Nodes are web pages and edges are hyperlinks between pages.We can see from the visualisation that the hyperlink activity network consists of a number of connected components (sets of nodes that are connected either directly or indirectly). We can look further at these clusters or components.\n\n\ncc <- components(g_activity)\nstr(cc)\n\nList of 3\n $ membership: Named num [1:1736] 1 2 3 4 5 4 3 3 3 3 ...\n  ..- attr(*, \"names\")= chr [1:1736] \"http://1001lakes.com/products\" \"http://abo-peoples.org\" \"http://aimitindia.com\" \"http://artexte.ca/lang/en\" ...\n $ csize     : num [1:15] 195 222 77 55 253 191 68 203 103 92 ...\n $ no        : int 15\n\nsummary(cc$csize)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   25.0    52.5    92.0   115.7   193.0   253.0 \n\nThe above indicates we have 15 weakly-connected components, and these range in size from 25 nodes to 253 nodes. It is not unexpected that the hyperlink activity network is quite disconnected since two seed pages \\(i\\) and \\(j\\) will only be directly connected if they link to each other and they will be indirectly connected if they link to the same third page.\n3.2 Actor network\nNow, we will create the actor network using vosonSML. With hyperlink actor networks, the nodes are web domains (in this document we use “site” and “domain” interchangeably) and edges are hyperlinks between them. Again, for the purpose of visualisation, we will simplify the network by removing loops and multiple (parallel) edges. Multiple edges between sites can arise because multiple pages within a domain can link to the same or multiple pages within another domain. We will also create an edge weight field (“weight”) to store this information on multiple links between domains.\n\n\n# create actor network: nodes are site domains of pages hyperlinks were collected from\nnet_actor <- crawlDF %>% Create(\"actor\")\ng_actor <- net_actor %>% Graph()\nvcount(g_actor)\n\n[1] 497\n\necount(g_actor)\n\n[1] 2826\n\n#simplify the network - remove loops and multiple edges\n#we will also create an edge attribute \"weight\"\nE(g_actor)$weight <- 1\ng_actor <- simplify(g_actor)\necount(g_actor)\n\n[1] 559\n\n\n\npng(\"actor_network.png\", width=800, height=800)\nplot(g_actor, layout=layout_with_fr(g_actor), vertex.label=\"\", vertex.size=3, \n     edge.width=E(g_actor)$weight, edge.arrow.size=0.5)\ndev.off()\n\n\nFigure 2: Hyperlink actor network - Nodes are web domains and edges are hyperlinks.\n\ncc <- components(g_actor)\nstr(cc)\n\nList of 3\n $ membership: Named num [1:497] 1 1 1 1 1 1 1 1 1 1 ...\n  ..- attr(*, \"names\")= chr [1:497] \"1001lakes.com\" \"2022.mydata.org\" \"abo-peoples.org\" \"ada-x.org\" ...\n $ csize     : num 497\n $ no        : int 1\n\nThe actor network has 497 nodes and 559 edges. The above indicates we have a single (weakly-connected) component: with hyperlink actor networks seeds are more likely to be connected to one another (either directly or indirectly) as they only need to link to a common domain.\n4. Processing nodes in the hyperlink actor network\nFor the rest of this exercise, we will use the actor network. We will now look at three approaches for processing hyperlink network data: pagegrouping, pruning, and preserving.\n4.1 Pagegrouping\nPagegrouping refers to merging nodes within a hyperlink network. A common example of a situation where you may want to apply pagegrouping is when you would like to ensure that a subdomain node is not shown separately in the network i.e. it is merged with its domain node. For example, we might want to merge www.anu.edu.au with anu.edu.au (or rename www.anu.edu.au to anu.edu.au if the latter does not already exist). It should be noted that whether this pagegrouping is enacted is dependent on the research setting: it might be the case that you want to keep particular subdomains separate to their parent domain. For example we might want rsss.anu.edu.au or cass.anu.edu.au to be separate nodes, and not merged with anu.edu.au.\nThere are two approaches we can used to undertake pagerouping: (1) operating on the vosonSML network object (that is then used to create the igraph graph object); (2) operating on the igraph graph object. In this document we will demonstrate method (2).\n4.1.1 Merging the “www” subdomain into a canonical domain\nFirst, we are going to consolidate nodes into a single canonical domain. This will involve stripping “www” from www.x.y if only www.x.y exists and if both www.x.y and x.y exist then the former will be merged into the latter. Note that the following code also works if the canonical domain contains three or more parts e.g. x.y.z. Also note that the following step will only modify the www subdomain (e.g. www.x.y): other subbdomains (e.g. othersub.x.y) are handled below.\nLet’s focus on an example present in this dataset:\n\n\ng <- g_actor             #easier to write code\n\nV(g)$name[grep(\"mydata.org\", V(g)$name)]\n\n[1] \"2022.mydata.org\"   \"mydata.org\"        \"oldwww.mydata.org\"\n[4] \"www.mydata.org\"   \n\nThis first step will involve merging www.mydata.org into mydata.org.\n\n\ng <- g_actor             #easier to write code\n\n#create vector of the sites that start with \"www\"\nt2 <- 1\nwww_sites <- V(g)$name[grep(\"^www\\\\.\", V(g)$name)]\nfor (c in www_sites){\n\n  #cat(\"working on:\", c, \"\\n\")\n  if (t2%%100==0)\n    cat(\"Finished working on\", c, \"(\", t2, \"of\", length(www_sites), \")\\n\")\n\n  ind_i <- grep(paste0(\"^\",c,\"$\"), V(g)$name)\n  i <- str_remove(c,\"^www.\")\n  \n  #if (c==\"www.mydata.org\")\n  #  break\n  #next\n  #print(i)\n  #print(V(g)$name[grep(i, V(g)$name)])\n  #num_parts <- str_count(c, \"\\\\.\")\n  #print(num_parts)\n  #ind <- grep(paste0(i,\"$\"), as.character(V(g)$name))\n  #ind <- grep(paste0(\"\\\\.\",i,\"$\"), as.character(V(g)$name))\n  #this is a hack...want to match on \".abc.com\" or \"abc.com\"\n  #ind <- union(grep(paste0(\"\\\\.\",i,\"$\"), as.character(V(g)$name)), \n  #             grep(paste0(\"^\",i,\"$\"), as.character(V(g)$name)))\n  \n  ind <- grep(paste0(\"^\",i,\"$\"), as.character(V(g)$name))\n\n  if (!length(ind)){     #only the \"www\" version, just rename node to domain\n    V(g)$name[ind_i] <- i\n    t2 <- t2 + 1\n    next\n  }\n    \n  #Otherwise, we have two versions: www.x.y and x.y: merge these\n  #Note we will deal with situation where we have e.g. othersubdomain.x.y below\n  \n  ind <- sort(c(ind_i, ind))\n  #ind <- sort(ind)\n  #print(ind)\n\n  #if (length(ind)>1)\n  #  break\n    \n  #merging nodes involves creating a map of vertices e.g. if we have 5 vertices \n  #to merge nodes 1 and 2 and also merge nodes 3 and 4 the map needs to be:\n  #1 1 2 2 3\n  #i.e. nodes 1 and 2 become node 1, nodes 3 and 4 become node 2, \n  #and node 5 becomes node 3\n  map_i <- 1:ind[1]\n  t <- ind[1]+1\n  for (j in (ind[1]+1):vcount(g)){\n    #print(j)\n    if (j %in% ind){     #node to merge \n      map_i <- c(map_i, ind[1])\n    }else{               #not node to merge\n      map_i <- c(map_i, t)\n      t <- t + 1\n    }\n  }\n  \n  #need to use vertex.attr.comb=\"first\" or else get weird lists in attribute\n  #and it messes things up.  Replaced anyway...\n  g <-contract(g, map_i, vertex.attr.comb=\"first\")\n  V(g)$name[ind[1]] <- i                #rename the node to the non-www version\n  \n  t2 <- t2 + 1\n  \n}\n\nFinished working on www.globalhealthstrategies.com ( 100 of 262 )\nFinished working on www.privacyshield.gov ( 200 of 262 )\n\n#We have reduced the number of nodes from:\nvcount(g_actor)\n\n[1] 497\n\n#to\nvcount(g)\n\n[1] 478\n\n4.1.2 Merging other subdomains into the canonical domain\nNow have only a single canonical version of the mydata.org domain, but there are still two subdomains (other than www).\n\n\nV(g)$name[grep(\"mydata.org\", V(g)$name)]\n\n[1] \"2022.mydata.org\"   \"mydata.org\"        \"oldwww.mydata.org\"\n\nIn this next step, we will merge all remaining subdomains into the canonical domain. However, there might be examples of subdomains that we want “preserved” (not merged into the canonical domain); they should be specified in advance. A good example is websites created using Wordpress.\n\n\nV(g)$name[grep(\"wordpress.com\", V(g)$name)]\n\n[1] \"coporwa1en.wordpress.com\"              \n[2] \"datasovereigntynow.files.wordpress.com\"\n[3] \"en.wordpress.com\"                      \n[4] \"indigenousdatalab.wordpress.com\"       \n[5] \"institutdayakologi.wordpress.com\"      \n[6] \"isocbg.wordpress.com\"                  \n[7] \"ourgovdotin.files.wordpress.com\"       \n[8] \"subscribe.wordpress.com\"               \n[9] \"wordpress.com\"                         \n\nIf we don’t “preserve” these Wordpress websites (which are run by different organisations or groups) then, they will all be merged into a single node Wordpress.com.\nWe will implement two types of preserving: (1) preserve specific subdomains, (2) preserve all subdomains via a wildcard match.\nIn the example below we will preserve specific Wordpress subdomains (and the rest will be merged into the canonical domain wordpress.com) and we will preserve all subdomains for mydata.org.\nA final point to note is that the “wildcard” approach to preserving subddomains (what we are doing for mydata.org) will only work if mydata.org exists in the network. From the following, we can see that mydata.org exists, but fraunhofer.de does not exist.\n\n\nV(g)$name[grep(\"mydata.org\", V(g)$name)]\n\n[1] \"2022.mydata.org\"   \"mydata.org\"        \"oldwww.mydata.org\"\n\nV(g)$name[grep(\"fraunhofer.de\", V(g)$name)]\n\n[1] \"isst.fraunhofer.de\"              \n[2] \"websites.fraunhofer.de\"          \n[3] \"dataspaces.fraunhofer.de\"        \n[4] \"iee.fraunhofer.de\"               \n[5] \"iml.fraunhofer.de\"               \n[6] \"medical-data-space.fraunhofer.de\"\n\nIf the canonical domain doesn’t exist, as in the case of fraunhofer.de, then we need a separate process to undertake pagegrouping. We do this for fraunhofer.de in the section on “custom merging” below.\n\n\n#vector containing the information for preserving subdomains\n#we would most likely store this as a csv file with notes on choices made\npreserve <- c(\"*mydata.org\", \"coporwa1en.wordpress.com\", \n              \"institutdayakologi.wordpress.com\",\n              \"indigenousdatalab.wordpress.com\")\n\nfor (i in V(g)$name){\n\n  #cat(\"working on:\", i, \"\\n\")\n  ind_i <- which(V(g)$name==i)\n  \n  #want to match only on \".x.y\" not \"x.y\"\n  ind <- grep(paste0(\"\\\\.\",i,\"$\"), as.character(V(g)$name))\n  \n  #skip any subdomains that are to be preserved\n  ind_g <- grep(i, preserve)\n  for (j in ind_g){\n    if ( grepl(\"^\\\\*\",preserve[j]) ){        #wildcard, skip all subdomains\n      ind <- NULL\n      break\n    }else{\n      ind_rem <- which(V(g)$name==preserve[j])\n      ind <- ind[-which(ind==ind_rem)]\n    }\n  }\n      \n  if (!length(ind))              #there is no subdomain(s)\n    next\n\n  cat(\"working on:\", i, \"\\n\")\n  \n  #We have one or more subdomains\n\n  print(V(g)$name[ind])\n\n  ind <- sort(c(ind_i, ind))\n  print(ind)\n\n  map_i <- 1:ind[1]\n  t <- ind[1]+1\n  for (j in (ind[1]+1):vcount(g)){\n    #print(j)\n    if (j %in% ind){     #node to merge\n      map_i <- c(map_i, ind[1])\n    }else{               #not node to merge\n      map_i <- c(map_i, t)\n      t <- t + 1\n    }\n  }\n  \n  g <-contract.vertices(g, map_i, vertex.attr.comb=\"first\")\n  V(g)$name[ind[1]] <- i\n\n}\n\nworking on: arizona.edu \n[1] \"nni.arizona.edu\"           \"nnigovernance.arizona.edu\"\n[1]   9 163 164\nworking on: innovalia.org \n[1] \"idsa.innovalia.org\"\n[1] 118 127\nworking on: internationaldataspaces.org \n[1] \"docs.internationaldataspaces.org\"\n[1]  66 128\nworking on: wordpress.com \n[1] \"datasovereigntynow.files.wordpress.com\"\n[2] \"en.wordpress.com\"                      \n[3] \"isocbg.wordpress.com\"                  \n[4] \"ourgovdotin.files.wordpress.com\"       \n[5] \"subscribe.wordpress.com\"               \n[1]  50  79 131 168 200 228\nworking on: amazon.com \n[1] \"aws.amazon.com\"\n[1]  13 240\nworking on: google.com \n[1] \"docs.google.com\"     \"maps.google.com\"     \"podcasts.google.com\"\n[4] \"policies.google.com\"\n[1]  65 143 172 173 323\nworking on: linkedin.com \n[1] \"de.linkedin.com\"\n[1]  53 364\nworking on: microsoft.com \n[1] \"azure.microsoft.com\"\n[1]  14 376\nworking on: nativeweb.org \n[1] \"ayf.nativeweb.org\"   \"saiic.nativeweb.org\"\n[1] 248 380 413\nworking on: wto.org \n[1] \"docs.wto.org\"\n[1]  67 456\n\n#We have reduced the number of nodes from:\nvcount(g_actor)\n\n[1] 497\n\n#to\nvcount(g)\n\n[1] 459\n\nThe following shows that the correct subdomains have been preserved. The two subdomains of mydata.org have been preserved.\n\n\nV(g)$name[grep(\"mydata.org\", V(g)$name)]\n\n[1] \"2022.mydata.org\"   \"mydata.org\"        \"oldwww.mydata.org\"\n\nWe have preserved three subdomains of wordpress.com, and the rest have been merged into the canonical domain.\n\n\nV(g)$name[grep(\"wordpress.com\", V(g)$name)]\n\n[1] \"coporwa1en.wordpress.com\"        \n[2] \"wordpress.com\"                   \n[3] \"indigenousdatalab.wordpress.com\" \n[4] \"institutdayakologi.wordpress.com\"\n\n4.1.3 Custom merging of domains\nThe above code showed how to merge subdomains “in bulk”, and we also showed how to control this merging by “preserving” subdomains. But we might need to undertake an additional step for merging domains and subdomains. When an organisation uses two or more websites, it might be desirable to merge the relevant domains into a single node in the actor network. The following approach also takes account of the situation we found above for fraunhofer.de: the bulk merging did not work because the canonical domain fraunhofer.de doesn’t yet exist in our hyperlink actor network.\nThe following code does this with reference to the following examples:\n\n\nV(g)$name[grep(\"womeninlocalization\", V(g)$name)]\n\n[1] \"womeninlocalization.com\" \"womeninlocalization.org\"\n\nV(g)$name[grep(\"fraunhofer\", V(g)$name)]\n\n[1] \"isst.fraunhofer.de\"              \n[2] \"websites.fraunhofer.de\"          \n[3] \"dataspaces.fraunhofer.de\"        \n[4] \"iee.fraunhofer.de\"               \n[5] \"iml.fraunhofer.de\"               \n[6] \"medical-data-space.fraunhofer.de\"\n\nV(g)$name[grep(\"undocs|undp|uneca|unpo|unhcr|unstats|unwomen|unesco|un\\\\.org\", \n               V(g)$name)]\n\n [1] \"digitallibrary.un.org\" \"en.unesco.org\"        \n [3] \"lac.unwomen.org\"       \"tierracomun.org\"      \n [5] \"undocs.org\"            \"unstats.un.org\"       \n [7] \"undp.org\"              \"uneca.org\"            \n [9] \"unhcr.org\"             \"unpo.org\"             \n\nAs with the above, this step involves pre-specification of how the sites are to be merged. We will use a wildcard to merge the two womeninlocalization sites. We will also bulk merge subdomains of fraunhofer.de, preserving two subdomains. Finally, we will merge selected domains from the UN into a canonical domain un.org. Note that in the case of fraunhofer and the UN, the decision of exactly which subdomains and domains to merge is arbitrary: this is just for purposes of showing example code.\n\n\n#vector containing the information for preserving subdomains\n#we would most likely store this as a csv file with notes on choices made\npreserve <- c(\"dataspaces.fraunhofer.de\",\"medical-data-space.fraunhofer.de\")\n\n#the above process of merging subdomains will only work if there was www.x.y in\n#the original dataset if this is not the case, and want to merge subdomains\n#using a wildcard, then specify the canonical domain here note that here we use\n#a dataframe with the first column being the canonical domain (what other\n#domains/subdomains will be merged into), and subsequent columns are the other\n#domains/subdomains.  If only the first column is specified, then a bulk merge\n#will be undertaken. There are two types of merge below:\n# (1) bulk merge: this is when there is only one column in the data frame, \n#     the canonical domain.  \n#     This domain needs to be contained in the subdomains that are to be merged\n# (2) bespoke merge: this is when the first column contains the domain which\n#all the other subdomains/domains will be merged into.  Note that this domain\n#does not even need to exist in the hyperlink actor network the following will\n#only work if the canonical domain is contained in at least one. \n#Note that this would normally be stored in csv file\nex1 <- data.frame(V1=\"womeninlocalization.org\", \n                  V2=\"womeninlocalization.com\")    #bespoke merge\nex2 <- data.frame(V1=\"fraunhofer.de\")              #bulk merge\nex3 <- data.frame(V1=\"un.org\", V2=\"en.unesco.org\", \n                  V3=\"lac.unwomen.org\", V4=\"undocs.org\", \n                  V5=\"unstats.un.org\", V6=\"undp.org\", \n                  V7=\"uneca.org\")         #bespoke merge\nmergeDF <- bind_rows(ex1, ex2, ex3)\n#note that we preserve particular subdomains of fraunhofer.de by including\n#in preserve above\n\n#seems most efficient to iterate over nodes first, \n#rather than iterating over merges first\nfor (i in V(g)$name){\n  \n  #cat(\"working on:\", i, \"\\n\")\n  ind_i <- which(V(g)$name==i)\n\n  for (m in 1:nrow(mergeDF)){\n    col1 <- mergeDF[m,1]\n    othcol <- NULL\n    for (m2 in 2:ncol(mergeDF)){\n      if ( !is.na(mergeDF[m,m2]) )\n        othcol <- c(othcol, mergeDF[m,m2])\n      else\n        break\n    }\n    #print(col1)\n    #print(othcol)\n\n    if (is.null(othcol))                 #bulk match - grep match on first col\n      ind_g <- grep(col1,i)              #should search on \".[pattern]\"\n    else\n      ind_g <- match(i,othcol)           #exact match on othcol\n\n    if (!length(ind_g))\n      ind_g <- NA\n      \n    if (!is.na(ind_g))                   #we have a match\n      break\n  }\n\n  if (is.na(ind_g))        #no merging for this node\n    next\n    \n  #we are going to merge only node i (this is different to the above code)\n    \n  #break\n  \n  #but first check that this node is not to be preserved\n  #can be exact match for this\n  if (!is.na(match(i, preserve))){\n    cat(i, \"is to be preserved\\n\")\n    next\n  }\n  \n  #Also check that the node we are merging is not the same as the node\n  #we are merging to i.e. not merging\n  #fraunhofer.de -> fraunhofer.de\n  if (i==col1){\n    cat(i, \", trying to merge node with itself...\\n\")\n    next\n  }\n    \n  #break\n  \n  cat(i, \"merged to\", col1, \"\\n\") \n\n  #next we check if the node that i is being merged with even exists\n  #if it doesn't then we just rename i, and we are done\n  dom_match <- match(col1, V(g)$name)\n  if (is.na(dom_match)){\n    V(g)$name[ind_i] <- col1\n    cat(\"renaming\", i, \"to\", col1, \"\\n\")\n    next\n  }\n   \n  #break\n  \n  #the following will only have two nodes in it, but use it \n  #so can re-use code from above\n  ind <- sort(c(ind_i, dom_match))\n  \n  #break\n  \n  map_i <- 1:ind[1]\n  t <- ind[1]+1\n  for (j in (ind[1]+1):vcount(g)){\n    #print(j)\n    if (j %in% ind){     #node to merge\n      map_i <- c(map_i, ind[1])\n    }else{               #not node to merge\n      map_i <- c(map_i, t)\n      t <- t + 1\n    }\n  }\n  \n  g <-contract.vertices(g, map_i, vertex.attr.comb=\"first\")\n  V(g)$name[ind[1]] <- col1\n  \n}\n\nen.unesco.org merged to un.org \nrenaming en.unesco.org to un.org \nisst.fraunhofer.de merged to fraunhofer.de \nrenaming isst.fraunhofer.de to fraunhofer.de \nlac.unwomen.org merged to un.org \nundocs.org merged to un.org \nunstats.un.org merged to un.org \nwebsites.fraunhofer.de merged to fraunhofer.de \nwomeninlocalization.com merged to womeninlocalization.org \ndataspaces.fraunhofer.de is to be preserved\niee.fraunhofer.de merged to fraunhofer.de \niml.fraunhofer.de merged to fraunhofer.de \nmedical-data-space.fraunhofer.de is to be preserved\nundp.org merged to un.org \nuneca.org merged to un.org \n\n#We have reduced the number of nodes from:\nvcount(g_actor)\n\n[1] 497\n\n#to\nvcount(g)\n\n[1] 450\n\nThe following confirms that the merges have taken place correctly.\n\n\nV(g)$name[grep(\"womeninlocalization\", V(g)$name)]\n\n[1] \"womeninlocalization.org\"\n\nV(g)$name[grep(\"fraunhofer\", V(g)$name)]\n\n[1] \"fraunhofer.de\"                   \n[2] \"dataspaces.fraunhofer.de\"        \n[3] \"medical-data-space.fraunhofer.de\"\n\nV(g)$name[grep(\"undocs|undp|uneca|unpo|unhcr|unstats|unwomen|unesco|un\\\\.org\",\n               V(g)$name)]\n\n[1] \"digitallibrary.un.org\" \"un.org\"               \n[3] \"tierracomun.org\"       \"unhcr.org\"            \n[5] \"unpo.org\"             \n\n4.2 Pruning\nPruning refers to removing nodes that are considered not relevant to the analysis. It is highly likely that the web crawler will pick up pages that are not relevant to the study, and so we use pruning to identify and remove these irrelevant pages.\n4.2.1 Creatng the network containing just the seed sites\nWe will first create an actor network containing just the seed sites: in effect we are “pruning” non-seed sites. To start with, we need to identify what are the seed sites: we do this using the pages dataframe we created above for the web crawl. Recall that the pages dataframe contains web pages that we have crawled, but the hyperlink actor network contains websites (or domains). So we need to process the pages dataframe so as to identify what are the seed sites (not seed pages).\n\n\n# identify the seed pages and set a node attribute\n# we are also removing http tag and trailing forward slash\nseed_sites <- pages %>%\n  mutate(site = str_remove(page, \"^http[s]?://\"), seed = TRUE)\n# also remove trailing \"/\"\nseed_sites <- seed_sites %>%\n  mutate(site = str_remove(site, \"/$\"))\n\n#The following will return just the domain name\na <- str_match(seed_sites$site, \"(.+?)/\")\nseed_sites$site <- ifelse(grepl(\"/\", seed_sites$site), a[,2], seed_sites$site)\n\n#remove redundant column \"page\" and also user-created column \"domain\"\n#(to avoid confusion, since not using) and put \"site\" column first\nseed_sites <- seed_sites %>% select(-c(page,domain)) %>% relocate(site)\n\nseed_sites <- seed_sites %>% distinct(site, .keep_all=TRUE)\n\nkable(head(seed_sites))\n\nsite\ntype\nmax_depth\ncountry\nseed\nwomeninlocalization.com\nint\n1\nUS\nTRUE\niwgia.org\nint\n1\nDenmark\nTRUE\nwww.iwgia.org\nint\n1\nDenmark\nTRUE\nindigenousdatalab.org\nint\n1\nUS\nTRUE\nbotpopuli.net\nint\n1\nIndia\nTRUE\ncipesa.org\nint\n1\nSouth Africa\nTRUE\n\nnrow(seed_sites)\n\n[1] 21\n\nWe have created a dataframe seed_sites which contains the domains extracted from the seed pages used for crawling - this is stored in the column site. We have created a column seed and set this to TRUE (this will be used below when we use this dataframe to identify seed sites in the hyperlink actor network). Note that there was also a column domain but this was manually created at the time the seed pages were identified (in the same way that the column was country was manually created); we do not make use of that column in what follows, and so we removed it from the seed_sites dataframe.\nBefore proceeding, we need to first make some modifications to the seed_sites dataframe. The pagegrouping step above means that we no longer have the “www” subdomain present in the hyperlink actor network, and so we need to modify seed_sites by stripping “www” from the seed sites stored in the column site. Also, the pagegrouping means that we no longer have womeninlocalization.com in the network, so we need to change the seed site to womeninlocalization.org.\n\n\nseed_sites <- seed_sites %>% mutate(site=gsub(\"^www\\\\.\",\"\",site))\n\nseed_sites$site[which(seed_sites$site == \"womeninlocalization.com\")] <-\n  \"womeninlocalization.org\"\n\nseed_sites <- seed_sites %>% distinct(site, .keep_all=TRUE)\n\nkable(head(seed_sites))\n\nsite\ntype\nmax_depth\ncountry\nseed\nwomeninlocalization.org\nint\n1\nUS\nTRUE\niwgia.org\nint\n1\nDenmark\nTRUE\nindigenousdatalab.org\nint\n1\nUS\nTRUE\nbotpopuli.net\nint\n1\nIndia\nTRUE\ncipesa.org\nint\n1\nSouth Africa\nTRUE\nmydata.org\nint\n1\nFinland\nTRUE\n\nnrow(seed_sites)\n\n[1] 20\n\nSo we have 20 seed sites and we are now ready to create the “seeds only” hyperlink network. To do this, we will create a node attribute “seed” which indicates whether a site is a seed or not.\n\n\nV(g)$seed <- seed_sites$seed[match(V(g)$name, seed_sites$site)]\n\ntable(V(g)$seed)\n\n\nTRUE \n  20 \n\nThe above indicates that there are now 20 nodes in the network with the attribute seed equal to 1 (the rest have values of NA), so we have correctly identified our seed sites in the network. We can now visualise the seeds only hyperlink network.\n\n\n#for the remainder of this exercise we will work with the simiplified network\n#simplify the network - remove loops and multiple edges\nE(g)$weight <- 1\ng <- simplify(g)\n\ng_seeds <- induced.subgraph(g, which(V(g)$seed==1))\n\n\n\n\npng(\"seeds.png\", width=800, height=800)\nplot(g_seeds, vertex.label.color=\"black\", vertex.size=3, \n     edge.width=E(g_seeds)$weight, edge.arrow.size=0.5)\ndev.off()\n\n\nFigure 3: Seeds hyperlink network.The above figure shows that the seed sites are very disconnected, with only four pairs of sites connected; this is to be expected given we have only crawled to depth 1 (i.e. only the seed page is crawled). If we increased the crawl depth to 2, then it is to be expected that the seeds hyperlink network would become more connected.\n4.2.2 Creatng the network containing the seeds plus “important” non-seed sites\nNext we will create the “seeds plus important” network: this contains the seed sites plus those other sites that are hyperlinked to by two or more seeds.\n\n\ng_seedsImp <- induced.subgraph(g, which(V(g)$seed==1 | (degree(g)>=2)))\n\n#red nodes are seeds, blue nodes are importnant non-seeds\nV(g_seedsImp)$color <- ifelse(V(g_seedsImp)$seed==1, \"red\", \"blue\")\n\n\n\n\npng(\"seeds_important.png\", width=800, height=800)\nplot(g_seedsImp, vertex.label.color=\"black\", \n     vertex.size=3+degree(g_seedsImp, mode=\"in\"), \n     edge.width=E(g_seedsImp)$weight, edge.arrow.size=0.5)\ndev.off()\n\n\n\nThe above visualisation indicates that we now have a much more connected network, indeed it is fully-connected (no isolates) but a casual inspection indicates that many of the so-called “important” sites are probably not relevant to the analysis. For example, the most central nodes are social media sites (e.g. twitter.com, youtube.com). This brings us to the final step in the processing of the hyperlink data: manually pruning irrelevant sites.\n4.2.3 Manually pruning irrelevant sites\nWhether a site is relevant to the research is something only the researcher will know, so this final step is necessarily a manual one. We recommend that the “seeds plus important” network nodes are written to csv file, and then this csv file is manually coded so either the relevant sites are kept or the irrelevant sites are removed. In the example below, we are keeping those sites that are deemed relevant.\n\n\n#write the sites to csv file, for manual coding\n#also write seed status, since this will be used to determine whether a site is relevant\nwrite.csv(data.frame(site=V(g_seedsImp)$name, seed=V(g_seedsImp)$seed), \"seeds_plus_important.csv\")\n\n\nThe sites are manually coded - a new column “keep” is created, with a 1 indicating that the site is either a seed or it is a relevant non-seed site. The coded csv file is then used to create the new sub-network.\n\n\ndf2 <- read.csv(\"seeds_plus_important_coded.csv\")\nV(g_seedsImp)$keep <- df2$keep[match(V(g_seedsImp)$name, df2$site)]\ng_seedsImp2 <- induced.subgraph(g_seedsImp, which(V(g_seedsImp)$keep==1))\n#save to graphml for later use\n#seed logical attribute wasn't writing to graphml correctly,\n#maybe because because presence of NAs (which are FALSE)\nV(g_seedsImp2)$seed[which(is.na(V(g_seedsImp2)$seed))] <- FALSE\nwrite.graph(g_seedsImp2, \"g_seedsImp2.graphml\", format=\"graphml\")\n\n\n\n\npng(\"seeds_important2.png\", width=800, height=800)\nplot(g_seedsImp2, vertex.label.color=\"black\", \n     vertex.size=3+degree(g_seedsImp2, mode=\"in\"), \n     edge.width=E(g_seedsImp2)$weight, edge.arrow.size=0.5)\ndev.off()\n\n\n\nThe result is a graph presenting a connection or disconnection among organisations discussing ‘data sovereignty’ issues on the web. It shows that the debates interestingly place the already powerful institutions such as the UN and the World Bank at the center which makes them appear to be influential actors in the network of data sovereignty debates. Given their prominent positions in the network connecting otherwise separated organisations across countries or regions interested in data politics, it is interesting to gain insight into how organisations such as the UN and the World Bank impose their interest on data sovereignty agenda.\n4.3 Preserving\nWe have already mentioned preserving of subdomains above (to prevent all subdomains being merged into a single node when pagregrouping is undertaken). Another example of preserving is when we want to preserve sub-directories. By default, vosonSML does not preserve sub-directories and so www.example.com/subdir1 and www.example.com/subdir2 would, by default, be merged to the node www.example.com. Preserving these sub-directories would involve working with the activity network object returned by vosonSML:\n\n\nhead(net_activity$edges)\n\n# A tibble: 6 × 2\n  from                                     to                         \n  <chr>                                    <chr>                      \n1 https://womeninlocalization.com/partners http://goap-global.com     \n2 https://womeninlocalization.com/partners http://www.reddit.com/subm…\n3 https://womeninlocalization.com/partners https://csa-research.com   \n4 https://womeninlocalization.com/partners https://euatc.org          \n5 https://womeninlocalization.com/partners https://locworld.com       \n6 https://womeninlocalization.com/partners https://slator.com         \n\nIt would be necessary to process the URLs in the from and to using similar techniques to those presented above e.g. converting URLs to the domain names, but preserving certain sub-directories. Then the vosonSML Create() function can be used to create the igraph graph object. We will not cover this in full here, but leave this for a future blog post.\n\n\n\nAckland, R. (2010). WWW hyperlink networks. In D. L. Hansen, B. Shneiderman & M. A. Smith (Eds.), Analyzing social media networks with NodeXL: Insights from a connected world. Morgan-Kaufmann.\n\n\nAckland, R. (2013). Web social science: Concepts, data and tools for social scientists in the digital age. SAGE Publications.\n\n\nAckland, R. & O’Neil, M. (2011). Online collective identity: The case of the environmental movement. Social Networks, 33, 177–190. https://doi.org/https://doi.org/10.1016/j.socnet.2011.03.001\n\n\nLusher, D. & Ackland, R. (2011). A relational hyperlink analysis of an online social movement. Journal of Social Structure, 12(5).\n\n\n\n\n",
    "preview": "posts/2023-01-20-hyperlink-networks-pre-processing/seeds_important2.png",
    "last_modified": "2023-05-26T13:52:07+10:00",
    "input_file": {},
    "preview_width": 800,
    "preview_height": 800
  },
  {
    "path": "posts/2022-06-05-egocentric-networks-from-twitter-timelines/",
    "title": "Egocentric Networks from Twitter timelines",
    "description": "Demonstration of how to use rtweet and vosonSML to construct an ego net from Twitter users timelines.",
    "author": [
      {
        "name": "Bryan Gertzel",
        "url": {}
      }
    ],
    "date": "2022-08-22",
    "categories": [
      "rstats",
      "twitter",
      "networks",
      "egocentric",
      "rtweet",
      "vosonsml",
      "timeline"
    ],
    "contents": "\n\nContents\nIntroduction\nCollect the ego timeline\nCollect timelines of\nalters\nCreate an actor network\nOptionally add\nuser metadata as node attributes\nCreate an ego subgraph\nVisualise with an\ninteractive visNetwork plot\n\n\nUpdated for rtweet v1.0.2 and\nvosonSML\nv0.32.7.This article does not require Twitter API keys, but does\nrequire a twitter account and for the user to authorise the rtweet\nrstats2twitter app when prompted.\n\nIntroduction\nEgocentric networks or ego nets are networks that focus on a\nparticular actor (the ego) and map out their connections to other\nactors. In an ego net other actors are referred to as alters, and by\ncollecting the outward expanding connections of alters, ego nets of\nvarying degrees can be constructed (see Hogan,\n2011, p. 168). Some literature and software refer to these as\nneighborhood networks, with varying orders instead of degrees. For\nexample in a friendship network, a neighborhood network of the first\norder (1.0 degree) contains just the friends of the ego, whereas a\nnetwork of the second order (sometimes second step) also contains\n“friends of friends” (a 2.0 degree ego net).\nBy collecting the tweets in a Twitter users timeline, and the\ntimelines of users referenced, we can create a 1.0, 2.0 or 1.5 degree\nnetwork for the ego. A 1.5 degree network is similar to the 1.0 degree,\nexcept it also contains relationships or ties between the alters, or\n“between friends” of the ego from the previous friendship network\nexample.\nIt should be noted that by using user timelines that this is not\nnecessarily a friendship network, but instead a network of twitter users\nwho are associated through tweet activity. This kind of ego net can lead\nto insights beyond simply declared friendships (obtained from Twitter’s\nfriend/follower metadata) as the network structure is the result of\nusers interactions on the platform over a period of time.\nThis post will demonstrate how to construct an ego networks from a\ntwitter timelines using the rtweet package to collect\ntweets and vosonSML to create an actor network.\nCollect the ego timeline\nThe first step is to collect the ego’s timelime. In this post we will\nbe using the VOSON Lab @vosonlab twitter account, and\ncollecting the Twitter timeline using rtweet. The Twitter\nAPI restricts the number of timeline tweets that can be collected to the\nmost\nrecent 3,200 tweets, but we can set this to a lesser value e.g most\nrecent 100 tweets, and also use the same parameter for alters timelines\nfor the purposes of this demonstration.\n\n\nlibrary(dplyr)\nlibrary(DT)\nlibrary(rtweet)\nlibrary(vosonSML)\n\n# get twitter user timeline\nego_tweets <- get_timeline(c(\"vosonlab\"), n = 100, token = NULL)\n\n\n\n\n# convert rtweet data into vosonSML format\nego_tweets <- ego_tweets |> ImportRtweet()\n\n# create actor network from timeline tweets\nego_net <- ego_tweets |> Create(\"actor\", verbose = TRUE)\n\nGenerating twitter actor network...\n-------------------------\ncollected tweets | 100\ntweet mention    | 62\ntweet            | 25\nretweet          | 66\nquote mention    | 13\nquote            | 9 \nnodes            | 41\nedges            | 175\n-------------------------\nDone.\n\nA result of 41 nodes indicates that there are\n40 alters in the network with ties to the ego.\n\n\n\nFigure 1: vosonlab 1.0 degree actor network\n\n\n\nCollect timelines of alters\nFrom the previous step we created an actor network represented as\nnodes and edges dataframes from the ego’s tweet timeline. We can now use\nthis to extract all of the user ids of the alters in the network.\nNote that we have not specified the degree of the ego net at this\nstage, however by virtue of the twitter data (timeline tweets) having\nall been created by the ego user, we can assume all of the alters\n(referenced users) are connected to the ego in this network.\n\n\n# get ego user id\nego_user_id <- ego_net$nodes |>\n  filter(screen_name == \"vosonlab\") |> pull(user_id)\n\n# get list of alter user ids from network\nalter_user_ids <- ego_net$nodes |>\n  filter(user_id != ego_user_id) |> distinct(user_id) |> pull()\n\n\nUsing the alters user ids the timeline tweets can be collected and\nimported into vosonSML as follows:\n\n\n# get 100 most recent tweets from all of the alters timelines\n# and convert to vosonSML format\nalters_tweets <- alter_user_ids |>\n  get_timeline(n = 100, retryonratelimit = TRUE) |>\n  ImportRtweet()\n\n# Error: Number of tweet observations does not match number of users. 3526 != 99\n\n\n\nPlease note there seems to be an inconsistency in timeline results for\nthis version of rtweet and the following workaround can be used instead:\n\n\n\n# workaround for rtweet timeline users issue\nget_alters_timelines <- function(x) {\n  ImportRtweet(get_timeline(user = x, n = 100, retryonratelimit = TRUE))\n}\n\n# collects timelines individually and place into a list\nrequire(purrr)\nalters_tweets <- map(alter_user_ids, get_alters_timelines)\n\n\nAlternatively, if you have your own API access the\nvosonSML Collect function can also be used\nwith the endpoint = \"timeline\" parameter:\n\n\n# requires a previously saved vosonSML twitter auth object\nauth_twitter <- readRDS(\"~/.vsml_auth_tw\")\n\nalters_tweets2 <- auth_twitter |>\n  Collect(\n    endpoint = \"timeline\",\n    users = alter_user_ids,\n    numTweets = 100,\n    verbose = TRUE\n  )\n\n# Collecting timeline tweets for users...\n# Requested 4000 tweets of 150000 in this search rate limit.\n# Rate limit reset: 2022-08-22 06:21:30\n# \n# tweet        | status_id           | created            \n# --------------------------------------------------------\n# Latest Obs   | 1560130378366562304 | 2022-08-18 05:04:02\n# Earliest Obs | 1544727926645596162 | 2022-07-06 17:00:12\n# Collected 3525 tweets.\n# Done.\n\n\nCreate an actor network\nNow that all of the tweets from the alters timelines have also been\ncollected, the data can be merged and a single actor network created.\nThis actor network can be considered a 2.0 degree\nnetwork, as it contains not only the associations or “friends” from the\nego’s timeline, but also the associations or “friends” of the alters\nfrom their timelines.\n\n\n# combine all of the tweets from ego and alters timelines using vosonSML merge\ntweets <- do.call(Merge, alters_tweets)\ntweets <- Merge(ego_tweets, tweets)\n\n# create actor network from combined timeline tweets\nactor_net <- tweets |> Create(\"actor\", verbose = TRUE)\n\nGenerating twitter actor network...\n-------------------------\ncollected tweets | 3626\ntweet mention    | 1030\ntweet            | 1160\nretweet          | 1657\nreply mention    | 735\nreply            | 584\nquote mention    | 187\nquote            | 232\nnodes            | 1818\nedges            | 5585\n-------------------------\nDone.\n\nHere we can see an actor network of 1818 nodes and\n5585 edges, substantially larger than our initial actor\nnetwork.\n\n\n\nFigure 2: vosonlab 2.0 degree actor network\n\n\n\nOptionally add\nuser metadata as node attributes\nAt this point we can optionally add some user metadata to our network\nas node attributes. This allows us to change visual properties of the\nnetwork graph based on actor attributes. For example, we could map the\nnode size to number of followers a twitter user may have.\nPlease note this step requires a vosonSML\ntwitter auth object if you want to use the look up feature for\ncomplete users’ metadata.\n\n\n# this step requires a previously saved vosonSMML twitter auth object\nauth_twitter <- readRDS(\"~/.vsml_auth_tw\")\n\n# add user profile metadata\nactor_net_meta <- actor_net |>\n  AddUserData(tweets, lookupUsers = TRUE, twitterAuth = auth_twitter)\n\n\nHere is a sample of the actor metadata available and an example of\nhow it can be presented and explored using a data table:\n\n\n# node attributes\nnames(actor_net_meta$nodes)\n\n [1] \"user_id\"                 \"screen_name\"            \n [3] \"u.user_id\"               \"u.name\"                 \n [5] \"u.screen_name\"           \"u.location\"             \n [7] \"u.description\"           \"u.url\"                  \n [9] \"u.protected\"             \"u.followers_count\"      \n[11] \"u.friends_count\"         \"u.listed_count\"         \n[13] \"u.created_at\"            \"u.favourites_count\"     \n[15] \"u.verified\"              \"u.statuses_count\"       \n[17] \"u.profile_banner_url\"    \"u.default_profile\"      \n[19] \"u.default_profile_image\" \"u.withheld_in_countries\"\n[21] \"u.derived\"               \"u.withheld_scope\"       \n[23] \"u.utc_offset\"            \"u.time_zone\"            \n[25] \"u.geo_enabled\"           \"u.lang\"                 \n[27] \"u.has_extended_profile\" \n\n# explore actors metadata\nactors_table <- actor_net_meta$nodes |>\n  filter(user_id %in% c(ego_user_id, alter_user_ids)) |>\n  mutate(u.screen_name = paste0(\"@\", screen_name)) |>\n  select(name = u.screen_name,\n         display = u.name,\n         locationu = u.location,\n         description = u.description,\n         followers = u.followers_count,\n         tweets = u.statuses_count) |>\n  slice_head(n = 5)\n\nlibrary(reactable)\n\nreactable(actors_table, bordered = TRUE, striped = TRUE, resizable = TRUE,\n          wrap = FALSE, searchable = TRUE, paginationType = \"simple\")\n\n\n\nCreate an ego subgraph\nA 1.5 degree network can be useful to reveal the associations between\nan ego’s alters. This can be achieved by creating a subgraph of the 2.0\nego network that retains only the previously identified alters (see\nbelow igraph::induced_subgraph). As we know every alter is\nconnected to the ego so it is also often useful to visualise ego\nnetworks without the ego as it is then easier to observe clustering.\n\n\nlibrary(igraph)\n\n# use the vosonSML to convert the network dataframes into an igraph object\ng <- actor_net_meta |> Graph()\n\n# create a subgraph with ego removed\ng2 <- induced_subgraph(g, c(alter_user_ids))\n\ng2\n\nIGRAPH 953ba4f DN-- 40 1922 -- \n+ attr: type (g/c), name (v/c), screen_name (v/c), u.user_id\n| (v/c), u.name (v/c), u.screen_name (v/c), u.location (v/c),\n| u.description (v/c), u.url (v/c), u.protected (v/l),\n| u.followers_count (v/n), u.friends_count (v/n),\n| u.listed_count (v/n), u.created_at (v/c), u.favourites_count\n| (v/n), u.verified (v/l), u.statuses_count (v/n),\n| u.profile_banner_url (v/c), u.default_profile (v/l),\n| u.default_profile_image (v/l), u.withheld_in_countries\n| (v/x), u.derived (v/c), u.withheld_scope (v/l), u.utc_offset\n| (v/l), u.time_zone (v/l), u.geo_enabled (v/l), u.lang (v/l),\n| u.has_extended_profile (v/l), status_id (e/c), created_at\n| (e/c), edge_type (e/c)\n+ edges from 953ba4f (vertex names):\n\nAs we saw in our initial actor network constructed from only the\nego’s timeline we now have 40 nodes again, matching the\nnumber of alters. This actor network has many more edges however, as\n1922 ties or relations between the alters were captured\nfrom the collection of the alters timelines.\nVisualise with an\ninteractive visNetwork plot\nUsing the igraph and visNetwork package we\ncan create a simplified and undirected ego network graph of alters.\nCommunity detection can be performed and visualised using the\nigraph walktrap clustering algorithm and\nFruchterman-Reingold force-directed layout. We can further map some\nvisual properties of nodes to attributes - with node size corresponding\nto the node degree, edge width to combined weight, and color to\nclustering community group.\n\n\nlibrary(visNetwork)\n\n# combine and weight the edges between nodes\nE(g2)$weight <- 1\ng2 <- igraph::simplify(g2, edge.attr.comb = list(weight = \"sum\"))\ng2 <- as.undirected(g2)\n\n# perform some community detection using a random walk algorithm\nc <- walktrap.community(g2)\nV(g2)$group <- membership(c)\n\n# map visual properties of graph to attributes\nE(g2)$width <- ifelse(E(g2)$weight > 1, log(E(g2)$weight) + 1, 1.1)\n\nV(g2)$size <- degree(g2) + 5\nV(g2)$label <- paste0(\"@\", V(g2)$u.screen_name)\n\nvisIgraph(g2, idToLabel = FALSE) |>\n  visIgraphLayout(layout = \"layout_with_fr\") |>\n  visOptions(\n    nodesIdSelection = TRUE,\n    highlightNearest = TRUE\n  )\n\n\n\n\nFigure 3: vosonlab 1.5 degree actor network\n\n\n\nThe final result is an ego net with some clear associations. Colours\nand placement of nodes found to represent some interesting domains and\ncommunity relationships between the Twitter @vosonlab\naccount and its timeline network alters. Isolates represent more distant\nconnections with no detected community ties.\n\n\n\nHogan, B. (2011). Chapter 11 - visualizing and interpreting facebook\nnetworks [Book Section]. In D. L. Hansen, B. Shneiderman & M. A.\nSmith (Eds.), Analyzing social media networks with NodeXL (pp.\n165–179). Morgan Kaufmann. https://doi.org/10.1016/B978-0-12-382229-1.00011-4\n\n\n\n\n",
    "preview": "posts/2022-06-05-egocentric-networks-from-twitter-timelines/egocentric_500x500.png",
    "last_modified": "2022-08-25T15:58:00+10:00",
    "input_file": {},
    "preview_width": 500,
    "preview_height": 500
  },
  {
    "path": "posts/2021-12-06-collecting-youtube-comments-with-vosondash/",
    "title": "Collecting YouTube comments with VOSONDash",
    "description": "Collect YouTube comments and create networks for analysis with VOSONDash",
    "author": [
      {
        "name": "Francisca Borquez",
        "url": {}
      }
    ],
    "date": "2021-12-06",
    "categories": [
      "rstats",
      "SNA",
      "VOSONDash",
      "networks",
      "YouTube",
      "visualisation"
    ],
    "contents": "\nThis guide provides a practical demonstration for collecting comments from YouTube videos and constructing networks, using the VOSON Lab’s interactive R/Shiny app VOSONDash.\nThe GitHub page provides instructions to install and run VOSONDash. More information on features can be accessed in the VOSONDash Userguide(Borquez et al. 2020).\nCollecting YouTube data\nAs for Twitter, YouTube collection requires API keys, which are provided via the Google API console. Similarly, we enter the key in the API Keys window, YouTube tab in VOSONDash and the token can be saved to disk for future use.\nIn this example, we are collecting comments from a YouTube video titled Update on reinfection caused by Omicron variant, which was uploaded by the World Health Organization (WHO) on 5th December 2021 and had attracted 182 comments at the time of data collection (17 December 2021).\nYouTube networks\nVOSONDash (via vosonSML) provides two types of YouTube networks:\nActivity networks – nodes are either comments or videos (videos represent a starting comment), and edges are interactions between comments. In this example, there are 119 nodes and 118 edges. The most central node is the initial post, which receives 100 comments.\nActor networks – nodes are users who have commented on videos and the videos themselves are included in the network as special nodes. Edges are the interactions between users in the comments. We can distinguish two types of edges “top-level comment,” which is a reply to the initial post (video), and “reply to a reply,” when users mention the username of the person they are replying to. In this example, there are 81 nodes and 119 edges. The most central node is the initial post, which receives 110 comments.\nFigure 1: VOSONDash – YouTube collection, Activity and Actor networksThe Blog post Analysing online networks with VOSONDash provides more detail into features for network analysis, network visualisation and text analysis.\n\n\n\nBorquez, F., B. Gertzel, X. Cai, and R. Ackland. 2020. VOSON Dashboard Userguide. Canberra, Australia: VOSON Lab, Australian National University. https://vosonlab.github.io/VOSONDashDocs/.\n\n\n\n\n",
    "preview": "posts/2021-12-06-collecting-youtube-comments-with-vosondash/youtube.png",
    "last_modified": "2022-08-04T14:14:49+10:00",
    "input_file": {},
    "preview_width": 1283,
    "preview_height": 635
  },
  {
    "path": "posts/2021-11-25-collecting-twitter-data-with-vosondash/",
    "title": "Collecting Twitter data with VOSONDash",
    "description": "A short guide to collecting Twitter data and constructing networks for analysis with VOSONDash.",
    "author": [
      {
        "name": "Francisca Borquez",
        "url": {}
      }
    ],
    "date": "2021-11-25",
    "categories": [
      "rstats",
      "SNA",
      "VOSONDash",
      "networks",
      "twitter",
      "visualisation"
    ],
    "contents": "\nThis guide provides a practical demonstration for collecting Twitter data and constructing networks, using VOSON Lab’s interactive R/Shiny app VOSONDash.\nThe GitHub page provides instructions to install and run VOSONDash. More information on features can be accessed in the VOSONDash Userguide(Borquez et al. 2020).\nCollecting Twitter data\nTwitter collection require authentication with OAuth1.0 API keys, provided via Twitter Developer account. Simply, enter the fields in the API Keys window in VOSONDash. The token can be saved to disk for future use.\nIn this example, data were collected on 6 December 2021 and include 200 recent tweets with the hashtags #auspol and #COVID-19 (Fig. 1).\nTwitter networks\nVOSONDash – via vosonSML– provides four types of Twitter networks for analysis:\nActivity networks – where nodes represent tweets and edge types are: replies, retweets and quoted retweets. In this example, there are 225 nodes (excluding isolates) and 200 edges (including multiple edges and loops).\nActor networks – where nodes represent users who have tweeted, or else are mentioned or replied to in tweets. Edges represent interactions between Twitter users, and an edge attribute indicates whether the interaction is a mention, reply, retweet, quoted retweet or self-loop. In this example, there are 212 nodes and 213 edges (including multiple edges and loops).\nFigure 1: VOSONDash – Twitter collection, Activity and Actor networksTwo-mode networks – where nodes are actors (Twitter users) and hashtags, and there is an edge from user i to hashtag j if user i authored a tweet containing hashtag j. In this example, we have removed the hashtags we used in our collection #auspol and #COVID-19. The resulting network has 242 nodes and 213 edges. When clicking on Label attribute, we can observe hashtags and handles used in those tweets.\nFigure 2: VOSONDash – Two-mode networkSemantic networks – where nodes represent entities extracted from the tweet text: common words, hashtags and usernames. Edges reflect co-occurrence of terms. In this example, we have removed the terms #auspol and #COVID-19, and set the parameters to include 5% most frequent words and 50% most frequent hashtags. The resulting network has 55 nodes (excluding isolates), and 127 edges. Label attribute option displays terms in network visualisation.\nFigure 3: VOSONDash – Semantic networkTo learn more about VOSONDash network and text analysis features, see our previous post Analysing online networks with VOSONDash.\n\n\n\nBorquez, F., B. Gertzel, X. Cai, and R. Ackland. 2020. VOSON Dashboard Userguide. Canberra, Australia: VOSON Lab, Australian National University. https://vosonlab.github.io/VOSONDashDocs/.\n\n\n\n\n",
    "preview": "posts/2021-11-25-collecting-twitter-data-with-vosondash/VOSONDash-t.png",
    "last_modified": "2021-12-16T23:45:37+11:00",
    "input_file": {},
    "preview_width": 1366,
    "preview_height": 768
  },
  {
    "path": "posts/2021-10-01-testing-bot-status-of-twitter-users/",
    "title": "Testing the bot status of users in Twitter networks collected via vosonSML",
    "description": "We use vosonSML to collect Twitter data and then use Botometer to test the bot status of a subset of Twitter users.",
    "author": [
      {
        "name": "Robert Ackland",
        "url": {
          "https://orcid.org/0000-0002-0008-1766": {}
        }
      },
      {
        "name": "Francisca Borquez",
        "url": {}
      }
    ],
    "date": "2021-10-08",
    "categories": [
      "rstats",
      "python",
      "SNA",
      "vosonSML",
      "networks",
      "Botometer",
      "bot detection"
    ],
    "contents": "\nIntroduction\nAs social media platforms like Twitter become important spaces for information diffusion, discussion and opinion formation, serious concerns have been raised about the role of malicious socialbots in interfering, manipulating and influencing communication and public opinion Badawy, Ferrara, and Lerman (2018). Their detection and the understanding of consequent dynamics of behaviour are relevant to researchers and it is central to the research collaboration the VOSON Lab is involved in (see Cimiano et al. 2020).\nIn this post we will use vosonSML to collect data via the Twitter API and construct a network represented as an igraph graph object in R. Then, we will identify a subset of users and test their bot status using Botometer1 (in python) and include the bot scores as node attributes in the network graph in R.\nCollecting the Twitter network using vosonSML in R\nThe vosonSML vignette (Ackland, Gertzel, and Borquez 2020) provides comprehensive instructions on how to use vosonSML. In this post, we are going to focus on the essential steps for Twitter collection and network generation via vosonSML. The first step involves loading the vosonSML package into the R session, and use the Web Auth approach to create a Twitter API access token:\n\n\nlibrary(magrittr)\nlibrary(vosonSML)\n\ntwitterAuth <-\n   Authenticate(\n      \"twitter\",\n      appName = \"An App\",\n      apiKey = \"xxxxxxxxxxxx\",\n      apiSecret = \"xxxxxxxxxxxx\")\n\n#Optionally, save the access token to disk:\nsaveRDS(twitterAuth, file = \"twitter_auth.rds\")\n\n#The following loads into the current session a previously-created access token:\ntwitterAuth <- readRDS(\"twitter_auth.rds\")\n\n\n\nThen, we collect 100 tweets that contain the Australian politics hashtag #auspol. Data is saved in .rds dataframe format.\n\n\ntwitterData <- twitterAuth %>%\n   Collect(\n      searchTerm = \"#auspol\",\n      numTweets = 100,\n      includeRetweets = FALSE,\n      retryOnRateLimit = TRUE,\n      writeToFile = TRUE)\n\n\n\nTo read the Twitter dataframe from disk, the ImportData() function modifies the class values for the object before it is used with vosonSML:\n\n\ntwitterData <- ImportData(\"2021-09-30_182359-TwitterData.rds\", \"twitter\")\n\n\n\nAnd now we use the vosonSML functions Create(\"actor\") to create an Actor network and Graph() to create an igraph graph object g.\nThe Create(\"actor\") function generates a named list containing two dataframes named nodes and edges. In this Actor network nodes are users who have either tweeted using the search term #auspol, or else are mentioned or replied to in tweets featuring the search terms. Edges represent interactions between Twitter users, and an edge attribute indicates whether the interaction is a mention, reply, retweet, quoted retweet or self-loop.\n\n\nactorNetwork <- twitterData %>% Create(\"actor\", vertbose=TRUE)\ng <- actorNetwork %>% Graph()\ng\n\n\n\nThe output in the console loos like this:\n\n> actorNetwork <- twitterData %>% Create(\"actor\", vertbose=TRUE)\nGenerating twitter actor network...\n-------------------------\ncollected tweets | 100\ntweet mention    | 119\ntweet            | 62\nreply mention    | 27\nreply            | 21\nquote mention    | 5 \nquote            | 17\nnodes            | 226\nedges            | 251\n-------------------------\nDone.\n`> g <- actorNetwork %>% Graph()\nCreating igraph network graph...Done.\n\nThe graph object g prints as follows:\n\n> g\nGRAPH 527682d DN-- 226 251 -- \n+ attr: type (g/c), name (v/c), screen_name (v/c), status_id (e/c), created_at (e/c),\n| edge_type (e/c)\n+ edges from 527682d (vertex names):\n [1] 1362279599191691264->1116612139          43447495           ->43447495           \n [3] 518488471          ->518488471           353381552          ->353381552          \n [5] 576131356          ->576131356           28305154           ->3288075858         \n [7] 1310795233651601408->3079563404          3842652433         ->3842652433         \n [9] 3219321554         ->3219321554          1327357902424666112->29387813           \n[11] 1327357902424666112->1548253015          1327357902424666112->1327357902424666112\n[13] 37891446           ->37891446            1296548400         ->1296548400         \n+ ... omitted several edges\n\nWe now use igraph to manipulate the network. The simplify(g) function removes multiple edges and loops from the network. Then, we proceed to identify a subset of Twitter users (5), based on indegree, which we will later use in our bot status analysis.\n\n\nlibrary(igraph)\n\n#remove multiple and loop edges\ng <- simplify(g)\n\nV(g)$screen_name[order(degree(g, mode=\"in\"), decreasing=TRUE)][1:5]\n\n\n\nGiven this network was created using tweets that contain the #auspol hashtag, it is not surprising that the top 5 Twitter users based on indegree are four politicians and a political commentator:\n\n[1] \"ScottMorrisonMP\" \"DanielAndrewsMP\" \"JoshFrydenberg\"  \"GladysB\"         \"bruce_haigh\"```  \n\nSince we are going to access the Botometer API via Python, first we need to print the Twitter handles we want to check (5) with Botometer to a .csv file.\n\n\nwrite.csv(data.frame(user=V(g)$screen_name[order(degree(g, mode=\"in\"), decreasing=TRUE)][1:5]), \"top-5_auspol.csv\", row.names=FALSE)\n\n\n\nFinding the bot scores using Botometer in python\nGetting started with Botometer\nWe are now going use the Botometer API to find the bot scores for the 5 Twitter accounts. We will use the python client Botometer-python provided by the Botometer team.\nTo use the Botometer API you need to be able to authenticate using Twitter developer app API keys (same keys you use for Dev Auth approach to authenticating for Twitter collection via vosonSML). You also need a free RapidAPI (previously Mashape) account with the Botometer Pro API enabled (the Basic plan is free).\nBelow is a test of the Botometer python client v4 , using code from the Indiana University Network Science Institute GitHub page.\nTo access Botometer, enter the following code in a python shell or script. The second step involves checking the bot status of a single Twitter account:\n\nimport botometer\n\nrapidapi_key = \"xx\"\ntwitter_app_auth = {\n                    'consumer_key': \"xx\",\n                    'consumer_secret': \"xx\",\n                    'access_token': \"xx\",\n                    'access_token_secret': \"xx\"\n                   }\n\nbom = botometer.Botometer(wait_on_ratelimit=True,\n                          rapidapi_key=rapidapi_key,\n                          **twitter_app_auth)\n\n# Check a single account by screen name\nresult = bom.check_account('@clayadavis')\nprint(result)\n\nThe result of our test prints as follows:\n\n{\n    \"cap\": {\n        \"english\": 0.4197222421546159,\n        \"universal\": 0.6608500314332488\n    },\n    \"display_scores\": {\n        \"english\": {\n            \"astroturf\": 0.2,\n            \"fake_follower\": 1.2,\n            \"financial\": 0.0,\n            \"other\": 0.3,\n            \"overall\": 0.4,\n            \"self_declared\": 0.2,\n            \"spammer\": 0.0\n        },\n        \"universal\": {\n            \"astroturf\": 0.2,\n            \"fake_follower\": 0.9,\n            \"financial\": 0.0,\n            \"other\": 0.3,\n            \"overall\": 0.8,\n            \"self_declared\": 0.0,\n            \"spammer\": 0.1\n        }\n    },\n    \"raw_scores\": {\n        \"english\": {\n            \"astroturf\": 0.04,\n            \"fake_follower\": 0.23,\n            \"financial\": 0.0,\n            \"other\": 0.06,\n            \"overall\": 0.08,\n            \"self_declared\": 0.05,\n            \"spammer\": 0.01\n        },\n        \"universal\": {\n            \"astroturf\": 0.04,\n            \"fake_follower\": 0.18,\n            \"financial\": 0.0,\n            \"other\": 0.06,\n            \"overall\": 0.17,\n            \"self_declared\": 0.0,\n            \"spammer\": 0.02\n        }\n    },\n    \"user\": {\n        \"majority_lang\": \"en\",\n        \"user_data\": {\n            \"id_str\": \"11330\",\n            \"screen_name\": \"test_screen_name\"\n        }\n    }\n}\n\nThe descriptions of elements in the response e.g. users, raw scores, etc., are specified in the GitHub page.\nAnalysing bot status with Botometer in phyton\nThis step involves reading in the .csv file with the 5 Twitter handles into python and run them through the botometer API:\n\nimport pandas as pd\nusers = pd.read_csv(\"top-5_auspol.csv\")\nprint(users)\n\n\ncat(\"user\\n 0  ScottMorrisonMP\\n 1  DanielAndrewsMP\\n 2   JoshFrydenberg\\n 3          GladysB\\n 4  bruce_haigh\\n\")\n\nNow, we collect the botscores and print the Complete Automation Probability (CAP) to the csv file.\n\nresults_dict = {}     #use this to save all botometer results to file\ncap = []              #use this for writing botometer CAP to csv\nfor i in users.user:\n   #print(i)\n   result = bom.check_account('@'+i)\n   #print(result)\n   cap.append([i, result['cap']['english']])\n   results_dict[i] = result\n\n#write CAP score to csv\ndf = pd.DataFrame(cap, columns=[\"user\", \"cap\"])\n#print(df)\ndf.to_csv(\"top-5_auspol_cap.csv\")\n\n#write results dictionary to file\nimport json\njson.dump(results_dict, open(\"top-5_auspol_botometer_results.txt\",'w'))\n#can be read back in with\n#d2 = json.load(open(\"top-5_auspol_botometer_results.txt\"))\n\nThe Botometer API provides scores as Complete Automation Probability (CAP), defined as the probability, according to Botometer models, that an account with a certain score or greater is a bot. More information on how to interpret the scores is available here.\nBot scores as node attributes in the graph in R\nFinally, we can read the botometer scores back into R and include them as a node attribute in the graph.\n\n\ndf2 <- read.csv(\"top-5_auspol_cap.csv\")\ndf2\n\n\n\nThe dataframe looks like this:\n\n> df2 <- read.csv(\"top-5_auspol_cap.csv\")\n> df2\n  X            user       cap\n1 0 ScottMorrisonMP 0.7384783\n2 1 DanielAndrewsMP 0.7966467\n3 2  JoshFrydenberg 0.4756770\n4 3         GladysB 0.7966369\n5 4     bruce_haigh 0.7874002\n\nTo add the bot scores as node attributes, we create a new node attribute “cap” and copy in the scores from the csv file.\n\n\nV(g)$cap <- NA\nV(g)$cap[match(df2$user,V(g)$screen_name)] <- df2$cap\nV(g)$screen_name[!is.na(V(g)$cap)]\n\n\n\nThe console output presenting nodes with the node attribute we just created is as follows:\n\n[1] \"bruce_haigh\"     \"DanielAndrewsMP\" \"JoshFrydenberg\"  \"ScottMorrisonMP\" \"GladysB\" \n\nTo inspect the values, we run the following code:\n\n\nV(g)$cap[!is.na(V(g)$cap)]\n\n\n\n\n[1] 0.7874002 0.7966467 0.4756770 0.7384783 0.7966369\n\n\n\n\nAckland, R., B. Gertzel, and F. Borquez. 2020. Introduction to vosonSML. Canberra, Australia: VOSON Lab, Australian National University. https://cran.r-project.org/web/packages/vosonSML/vignettes/Intro-to-vosonSML.html.\n\n\nBadawy, A., E. Ferrara, and K. Lerman. 2018. “Analyzing the Digital Traces of Political Manipulation: The 2016 Russian Interference Twitter Campaign.” In 2018 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM).\n\n\nCimiano, P., F. Muhle, O. Putz, B. Schiffhauer, E. Esposito, R. Ackland, U. Seelmeyer, and T. Veale. 2020. “Bots Building Bridges (3b): Theoretical, Empirical, and Technological Foundations for Systems That Monitor and Support Political Deliberation Online.” Volkswagen Foundation Grant in AI and the Society of the Future stream (2021-2025).\n\n\nRizoiu, M-A., T. Graham, R. Zhang, Y. Zhang, R. Ackland, and L. Xie. 2018. “#DebateNight: The Role and Influence of Socialbots on Twitter During the 1st U.S. Presidential Debate.” In International AAAI Conference on Web and Social Media.\n\n\nBotometer is a joint project of the Observatory on Social Media (OSoMe) and the Network Science Institute (IUNI) at Indiana University, USA↩︎\n",
    "preview": "posts/2021-10-01-testing-bot-status-of-twitter-users/bot.png",
    "last_modified": "2021-12-16T23:58:11+11:00",
    "input_file": {},
    "preview_width": 1260,
    "preview_height": 600
  },
  {
    "path": "posts/2021-08-05-exploring-issues-in-reddit-using-voson-dash/",
    "title": "Exploring issues in Reddit using VOSON Dash",
    "description": "An easy guide to explore issues in Redddit and construct networks for analysis using VOSONDash.",
    "author": [
      {
        "name": "Francisca Borquez",
        "url": {}
      }
    ],
    "date": "2021-09-23",
    "categories": [
      "rstats",
      "SNA",
      "VOSONDash",
      "networks",
      "text analysis",
      "Reddit"
    ],
    "contents": "\nThe following guide provides a practical demonstration for collecting Reddit data and constructing networks, using VOSON Lab’s interactive R/Shiny app VOSONDash. Reddit – a social news aggregation, content rating, and discussion website – provides the opportunity for researchers to access a wide range of themed-based online discussion data, and to understand the dynamics of these conversations.\nSNA approach to studying online networks\nVOSONDash (and vosonSML) method for network construction is based on Ackland and Zhu (2015) approach, whereby edges in Reddit networks represent implicitly directed ties, i.e. reflecting exchange of opinion between users rather than an explicit social relationship. Conversations threads can be analysed as networks and VOSONDash provides two approaches to constructing Reddit networks:\nActor networks – where nodes represent users who have posted original posts and comments, and edges are the comment interactions between users.\nActivity networks – where nodes are comments or initial thread posts and edges represent either replies to the original post, or replies to comments.\nMethodology\nIn this example, we will collect data from a Reddit post relating to the COVID-19 lockdown in Sydney, Australia, and proceed to use VOSONDash features to demonstrate the data outputs and a quick overview of analysis tools.\nThe post titled Sydney Lockdown extended until the end of September was created on 20 August 2021, and by the time of data collection (23 September 2021) it had attracted 557 comments.\nThe GitHub page provides instructions to install and run VOSONDash. More information on features can be accessed in the VOSONDash Userguide(Borquez et al. 2020).\nCollecting Reddit data\nReddit collection does not require API authentication. Simply go to the Reddit view, enter the URL, and click on Collect Threads. The output of the collection is presented in the right pane (Figure 1). In this example, 494 comments were collected. At this stage, the data can be saved as .rds dataframe.\nFigure 1: VOSONDash – Reddit collectionCreating Reddit Activity networks with VOSONDash\nActivity networks represent the three-like structure of conversations, with nodes being comments or the initial post, and edges being replies to comments or replies to initial post. In this example, we selected the Add text option, so the .graphml file contains text data.\nThe console displays the output of the activity network (Figure 2). The Activity network has 495 nodes (including the initial post), and 494 edges (comments). The network can be saved ad .graphml, if you prefer to use a different tool for analysis.\nCreating Reddit Actor networks with VOSONDash\nIn a similar workflow, we can use the data we just collected to create Actor networks, to observe Reddit users interactions. As mentioned earlier, in Actor networks, nodes are users who have commented, or who have created initial thread posts, and edges represent either replies to the original post, or replies to comments. Again, the Add text option was selected, for the .graphml file to contain text data.\nThe console displays the output of the activity network once the network is created. The Activity network has 302 nodes, and 495 edges.\nFigure 2: Reddit Activity and Actor networksVOSONDash provides three approaches to analyse networks: Network graph, Network metrics (SNA), and Text analysis. These tools are presented in more detail in the post Analysing networks with VOSONDash.\nWe hope this guide has been useful and easy to follow. In the next post, we will cover Twitter data collection with VOSONDash.\n\n\n\nAckland, R., and J. Zhu. 2015. “Social Network Analysis.” In Innovations in Digital Research Methods, edited by P. Halfpenny and R. Procter. London: SAGE Publications.\n\n\nBorquez, F., B. Gertzel, X. Cai, and R. Ackland. 2020. VOSON Dashboard Userguide. Canberra, Australia: VOSON Lab, Australian National University. https://vosonlab.github.io/VOSONDashDocs/.\n\n\n\n\n",
    "preview": "posts/2021-08-05-exploring-issues-in-reddit-using-voson-dash/preview.png",
    "last_modified": "2021-12-16T23:46:43+11:00",
    "input_file": {},
    "preview_width": 1266,
    "preview_height": 700
  },
  {
    "path": "posts/2021-08-06-analysing-online-networks-with-vosondash/",
    "title": "Analysing online networks with VOSONDash",
    "description": "A quick introduction to VOSONDash network and text analysis features",
    "author": [
      {
        "name": "Francisca Borquez",
        "url": {}
      }
    ],
    "date": "2021-09-09",
    "categories": [
      "rstats",
      "SNA",
      "VOSONDash",
      "networks",
      "text analysis",
      "visualisation"
    ],
    "contents": "\nThis post introduces VOSONDash network analysis tools, which include network visualisation, network metrics, and text analysis. Users can analyse different networks including those collected with VOSONDash (Twitter, YouTube and Reddit), or import graphml files collected elsewhere.\nAnalysing online networks with VOSONDash is the first of a series of posts where we will cover VOSONDash features. Data collection with VOSONDash is covered in the following posts:\nTwitter – Collecting Twitter data with VOSONDash\nReddit – Exploring issues in Reddit using VOSON Dash\nYouTube – Collecting YouTube comments with VOSONDash\nAbout VOSONDash\nVOSONDash is an output of computational social methods research, designed to be a “Swiss Army knife” for studying online networks. The R/Shiny dashboard tool enables online data collection, and network and text analysis (including visualisation) within the same environment. VOSONDash builds on a number of R packages, in particular vosonSML for data collection and network generation, and igraph for network analysis. The package provides a graphical user interface which does not require users to have R programming skills and it is available on CRAN and GitHub. Bryan Gertzel is the lead developer and maintainer of VOSONDash.\nStarting VOSONDash\nThe GitHub page provides instructions to install VOSONDash via R or Rstudio. Once the package is installed, run VOSONDash from the RStudio console entering the following code; VOSONDash will open in a web browser.\n\n\nlibrary(VOSONDash)\nrunVOSONDash()\n\n\n\nNetwork data\nTo ease replication, in this example we will use the EnviroActivistsWebsite_2006 demo dataset which is provided in the package. The dataset is a hyperlink network collected with VOSON in 2006, as part of a research piece (Ackland and O’Neil 2011). The network has 161 nodes (websites representing environmental organisations) and 1,444 edges representing hyperlinks between these organisations. In this dataset, text data is stored as node attribute and categorical values are assigned depending on type of environmental organisations (Bios, Globals, and Toxics).\nNetwork analysis using VOSONDash\nThere are three main approaches to analysing online networks with VOSONDash: Network graph, Network metrics (SNA), and Text analysis. More information on features can be accessed in the VOSONDash Userguide (Borquez et al. 2020).\nNetwork graph\nIn Network graph provides two options to explore networks: network visualisation via igraph and visNetwork; and tabulations for nodes and edges. The Network graph pane provides the following options for manipulating the network:\nLabels – to display or not labels.\nGraph Filters – to display or not multiple edges, loops, and isolates.\nLayout – to select graph layout and spread.\nNode Size – to select node size by metric e.g. indegree and define size (multiplier).\nCategorical filter – option available when data contains pre-set categorical values. New collections do not have that option.\nComponent filter – to display weak or strong components and define component range.\nNeighbourhood select – to create subnetworks. It uses ego network terminology of order, where order 1 include ties between the alters.\nFigure 1: VOSONDash network visualisationNetwork metrics\nVia the Network metrics pane, we can observe basic SNA metrics, including network level and node level metrics (e.g. centralisation). Network metrics reflect the applied filters for the visualisation; in this example we removed isolates (3 nodes), so network size is 158 and the Component distribution is 1 (one connected component). Degree distribution is only available for undirected networks; Indegree distribution and Outdegree distribution charts are available for directed networks, like this example. Accordingly, in this network, there are 15 nodes receiving one hyperlink, and three nodes receiving 35 hyperlinks. While 19 nodes link out to only one other site, there are two organisations in this network that link out to 50 sites.\nAssortativity metrics (Homogeneity and Homophily indexes, including mixing matrix and population share) are presented for networks with categorical node attributes. In this example, we have selected the categorical attribute Type. The mixing matrix table presents links across the three types of organisations Globals, Bios and Toxics. The Bios and Globals sub-movements show a strong tendency towards linking to their own type. Population shares, Homogeneity indexes and Homophily indexes are presented by type. Controlling for group size, Globals are the group more biased towards its own type, where 53% of their ties to other Global organisations can be explained by homophily.\nFigure 2: SNA and Assortativity metricsText analysis\nFor a network with text data stored as either node or edge attribute, it is possible to conduct basic text analysis with VOSONDash. Text corpus can be pre-processed using Filters to:\nremove common English words that are not relevant to the analysis, such as “and,” “the,” and “but” using Remove Standard Stopwords,\ncreate own stopword list in User-Defined Stopwords,\nreduce words to their stems with Apply word stemming,\nremove URLs, numbers, or punctuation, based on user’s specifications.\nWord lenght, if need to specify number of characters.\nAdvanced options provide HTML Decode and iconv UTF8, specially useful for social media as text often contains encoded characters.\nFor Twitter networks, two other options become available: Remove Twitter hashtags and Remove Twitter Usernames.\nThere are three methods available to visualise text:\nWord frequency bar charts, where further parameters can be applied such as to define the number of results displayed, and frequency to define Minimum frequency, for the text to appear.\nWord clouds where users can adjust Minimum frequency (how many times a word needs to have been used in order for it to feature in the visualisation); Maximum words to control for the number of words appearing in the graph; percentage of vertical words can be set for legibility; and random colours can be assigned to the visualisation. Comparison clouds are only available for datasets with categorical data, like this example where colour represents the node attribute type (Bios, Globals or Toxics).\nThe Sentiment analysis function uses the Syuzhet package and classifies words based on the NRC Emotion Lexicon, which is a list of English words and their associations with eight basic emotions (anger, fear, anticipation, trust, surprise, sadness, joy, and disgust) and two sentiments (negative and positive).\nFigure 3: Text analysisWe hope this guide is useful and easy to follow.\n\n\n\nAckland, R., and M. O’Neil. 2011. “Online Collective Identity: TheCase of the Environmental Movement.” Social Networks 33: 177–90. https://doi.org/https://doi.org/10.1016/j.socnet.2011.03.001.\n\n\nBorquez, F., B. Gertzel, X. Cai, and R. Ackland. 2020. VOSON Dashboard Userguide. Canberra, Australia: VOSON Lab, Australian National University. https://vosonlab.github.io/VOSONDashDocs/.\n\n\n\n\n",
    "preview": "posts/2021-08-06-analysing-online-networks-with-vosondash/Actor_net.png",
    "last_modified": "2022-08-04T14:14:49+10:00",
    "input_file": {},
    "preview_width": 1282,
    "preview_height": 670
  },
  {
    "path": "posts/2021-06-03-us-presidential-debates-2020-twitter-collection/",
    "title": "#DebateNight 2020: Hashtag Twitter Collection of the US Presidential Debates",
    "description": "Methodology for the bulk collection of tweets containing key hashtags for the US Presidential Debates and generation of Networks for Analysis.",
    "author": [
      {
        "name": "Bryan Gertzel",
        "url": {}
      }
    ],
    "date": "2021-06-03",
    "categories": [
      "rstats",
      "twitter",
      "hashtags",
      "networks",
      "election",
      "debate"
    ],
    "contents": "\n\nContents\nCollection Strategy\nHashtags\nTimezones\n\nStreaming Collection\nSearch Collection\nFirst Presidential Debate Preliminary Results\nData Summary\nData Tweet Activity\n\nNetwork Analysis\nMerge Collected Data\nCreate Networks\nReply-network Giant Component\n\n\nThe VOSON Lab undertook a number of Twitter collections on selected hashtags during the 2020 US Presidential debates and townhalls. The Twitter streaming API endpoint was used for a sampled real-time collection of tweets during the debates, and the recent search API was used post-debate to collect all tweets containing hashtags that occurred over the debate telecast periods. The two approaches differ in that the streaming API endpoint allows access to a “roughly 1% random sample of publicly available Tweets in real-time” using provided hashtags or terms as a filter to monitor events. The search API endpoint uses terms in a search query to match all tweets containing hashtags for a recent historical period in time. The streaming collections, a filtered sample, produced much smaller datasets and was useful for a timeley review of the Twitter activity during the debates as well as for identifying tweets to act as bounds for search collections. The retrospective search collections were much larger and produced more comprehensive datasets.\nThe R packages rtweet and vosonSML, the latter of which wraps the functionality of rtweet for its Twitter capability, were used in a semi-automated way to collect data with both streaming and search API’s. These packages use the standard Twitter API v1.1 endpoints.\nTwo US Presidential debates took place between Donald Trump and Joe Biden on September 29th and October 22nd, with one debate scheduled for October 15th cancelled due to COVID-19 concerns. One VP debate between Kamala Harris and Mike Pence took place on October 7th. This article will focus on the datasets collected for the first debate, widely reported as “pure chaos” and “90 minutes of insult and interjection” by commentators, to demonstrate our collection methodology and some simple network analysis.\nFigure 1: Next day headlines, tweet from https://twitter.com/oliverdarcy. Embedded tweet sourced from Twitter.Collection Strategy\nThe US Presidential debates were all scheduled to run for 1.5 hours between 9pm and 10.30pm Eastern Daylight Time (EDT). To capture Twitter discussion surrounding the debate a four hour streaming window was chosen, with the collection starting at 8.30pm and concluding at 12.30am EDT (0.5 hours before and 2 hours after the debate). Streaming collection was performed by multiple team members, each with slightly staggered start times. This is because streaming collections were divided into 15 minute segments, and offsetting allowed tweets to be collected during connections, disconnections and segment changeover intervals. The tweets could then be merged and any duplicates removed in post-processing.\nBecause of the very likely large volume of tweets collected using the search API endpoint the collection window was reduced to 2 hours, starting 15 minutes before and concluding 15 minutes after the debates. Twitter searches are historical and page backwards in time, results are returned from most recent tweet as the first observation to the earliest tweet matching search criteria as last observation collected. There are also limits to how many tweets can be collected as defined by the API rate-limit. Using a bearer authentication token to perform searches allows for 45,000 tweets to be retrieved every 15 minutes.\nAs tweet status ID’s are sequential numbers, with new tweets incrementing the identifier, they can be used to set bounds for searches. Simply, the first tweet in the collected data from a search (earliest tweet in data) can be used as a starting point for subsequent searches as we move backwards in time. This means that given two tweets, one at the beginning and the other at the end of an event - the debate, we can systematically and programatically collect all tweets in-between working backwards from the end of event tweet. To identify which tweets to use as bounds we performed timestamp search of collected streaming tweets. We identified the highest status ID tweet matching our end time for the collection window 10.45pm EDT (search start) and the lowest ID matching our debate start time 8.45pm EDT (search end).\n\nUpdate: Twitter tweet ID’s are large unique 64bit unsigned integers that are not incremented or assigned sequentially, but instead generated from a timestamp, a worker number, and a sequence number (Refer Developer Platform: Twitter ID’s). It is problematic to use and perform operations on these ID’s as integers and they should instead be managed as unique identifier strings. Tweets are retrieved from the API ordered, and it is more reliable to use the first and last observation and/or tweet timestamps to delimit searches. Unlike the Twitter API v1.1 used for this article, the newer API v2 also returns tweet timestamps that record the tweet creation time to the second rather than to the minute.\n\nIn practice, 45,000 tweets were collected with a pause for the 15 minute rate-limit to reset, then a further 45,000 were collected with a pause, and so on until the tweet that we identified as marking the beginning of the debate window was collected. Because this process could take many hours, it was important to perform the search collections within 7 days of the debates.\nHashtags\nA set of hashtags were selected in order to capture tweets related to the presidential debates. The following 12 were used for streaming and search collections for all debates, with 4 addditonal hashtags for the vice presidential debate and 3 for the townhalls.\nTable 1: Debate hashtags\n\nHashtags\nHashtags for all events\n#PresidentialDebate, #PresidentialDebates, #Election2020, #Debates2020, #Debates, #DebateNight, #Biden, #Biden2020, #BidenHarris2020, #Trump, #Trump2020, #TrumpPence2020\nAdditional vice presidential debate\n#VPDebate, #VPDebate2020, #VPDebates, #VPDebates2020\nAdditional televised townhalls\n#BidenTownHall, #TrumpTownHall, #TownHall\nTimezones\nThe first US Presidential debate took place in Cleveland, Ohio at 9pm Eastern Daylight Time (EDT) on the 29th September, 2020. Tweet timestamps are all in Universal Coordinated Time (UTC), meaning times in the data need to be offset by -4 to find the debate time in EDT. As collection took place in Canberra, Australia or Australian Eastern Standard Time (AEST), the local system timestamps produced by scripts for logging are in AEST. The first debate time and timezone conversions can be seen in the table below.\nTable 2: First debate timezone reference\nTimezone\nStart time\nEnd time\nEDT\n2020-09-29 21:00\n2020-09-29 22:30\nUTC\n2020-09-30 01:00\n2020-09-30 02:30\nAEST\n2020-09-30 11:00\n2020-09-30 12:30\nStreaming Collection\nFor the streaming collection a directory was created to for easier post-processing. Streaming data was collected and written to file in JSON format using timestamp formatted file names. The streaming collection period was set to 4 hours and divided into segments or files.\n\n\nShow code\n\nwd <- getwd()\n\n# check paths and create directories if they do not exist\ndata_path <- paste0(wd, \"/data\")\nif (!dir.exists(data_path)) { dir.create(data_path, showWarnings = FALSE) }\n\nstreams_path <- paste0(data_path, \"/pres-debate-streams\")\nif (!dir.exists(streams_path)) { dir.create(streams_path, showWarnings = FALSE) }\n\n# helper functions to write to log file and to create date time based file names\nlog <- function(msg, fn) { cat(msg, file = fn, append = TRUE, sep = \"\\n\") }\nfname <- function(path, ts) { paste0(path, \"/stream-\", gsub(\"[^[:digit:]_]\", \"\", ts)) }\n\n# set stream filter hashtags - comma seperated\nstream_filter <- paste0(\"#PresidentialDebate,#PresidentialDebates,#Election2020,\",\n                       \"#Debates2020,#Debates,#DebateNight,\",\n                       \"#Biden,#Biden2020,#BidenHarris2020,\",\n                       \"#Trump,#Trump2020,#TrumpPence2020\")\n\n# set the time period to collect tweets in seconds\nstream_period <- 4 * 60 * 60 # 4 hours or 14400 seconds\n\n# break up streaming collection into segments\nnum_segs <- 16 # each segment is 15 minutes\nseg_period <- ceiling(stream_period / num_segs)\n\n\nThe streaming collection is performed by the rtweet function stream_tweets which in our operation uses the query or filter parameter q, a timeout period which is the length of time to collect streaming tweets, and an output JSON file_name. The collection is wrapped in a loop which is for the number of 15 minute segments in the collection period. Each iteration sets up a new timestamped data file and log file.\n\n\nShow code\n\nlibrary(rtweet)\n\n# load rtweet auth token\ntoken <- readRDS(\"~/.rtweet_oauth1a\")\n\n# collect streaming tweets with a new file every 15 minutes\nfor (i in 1:num_segs) {\n\n  # create log file and JSON data file\n  timestamp <- Sys.time()\n  log_file <- paste0(fname(streams_path, timestamp), \".txt\")\n  json_file <- paste0(fname(streams_path, timestamp), \".json\")\n  \n  log(paste0(\"timestamp: \", timestamp, \"\\ntimeout: \",\n             seg_period, \" secs\\nfilter: \", stream_filter),\n    log_file)\n  \n  # collect streaming tweets and write to JSON file\n  tryCatch({\n    rtweet::stream_tweets(\n      token = token,\n      q = stream_filter,\n      timeout = seg_period,\n      file_name = json_file,\n      parse = FALSE\n    )\n  }, error = function(e) {\n    cat(paste0(e, \"\\n\"))\n    log(paste0(\"error: \", e), log_file)\n  })\n  \n  log(paste0(\"completed: \", Sys.time()), log_file)\n}\n\n\nLog entries for each 15 minute iteration confirm the collection period and each file matches a JSON data file (note timestamps are in local time which was AEST).\n# data/pres-debate-streams/stream-20200930102859.txt \n\ntimestamp: 2020-09-30 10:28:59\ntimeout: 900 secs\nquery: #PresidentialDebate,#PresidentialDebates,#Election2020,\n#Debates2020,#Debates,#DebateNight,#Biden,#Biden2020,#BidenHarris2020,\n#Trump,#Trump2020,#TrumpPence2020\ncompleted: 2020-09-30 10:43:59\nFor the first streaming collection iteration an 180MB JSON file was written with 111,244 lines. Each line contains the JSON for a single tweet, meaning the same number of tweets were collected.\n\n> first_json_file <- \"./data/pres-debate-streams/stream-20200930102859.json\"\n> file.size(first_json_file)\n[1] 188575977 # 180MB\n\n> length(readLines(first_json_file))\n[1] 111244\n\n16 JSON data files were written, corresponding to the number of 15 minute segments specified. These were then individually processed and converted to dataframes, which were then merged into a single complete streaming collection dataframe for the first debate.\nSearch Collection\nAs with the streaming collection directories were created to store collected data and search parameters set. The search query containing hashtags uses the Twitter OR search operator unlike the streaming filter which was comma seperated. A maximum number of tweets for each collection iteration as well as a maximum number of iterations are set. The number of tweets is required for the search request and is set to the maximum rate-limit value for a bearer token. A maximum number of iterations is set as a precaution to prevent infinite collection should a problem arise. The two tweets found from the streaming collection and used as search bounds are also set. The search will start at the latest id and continue until the earliest id is found, or the maximum iterations has been reached.\n\n\nShow code\n\nwd <- getwd()\n\n# check paths and create directories if they do not exist\ndata_path <- paste0(wd, \"/data\")\nif (!dir.exists(data_path)) { dir.create(data_path, showWarnings = FALSE) }\n\nsearches_path <- paste0(data_path, \"/pres-debate-searches\")\nif (!dir.exists(searches_path)) { dir.create(searches_path, showWarnings = FALSE) }\n\n# set search query hashtags - separated with OR search operator\nq <- paste0(\"#PresidentialDebate OR #PresidentialDebates \",\n            \"OR #Election2020 OR \",\n            \"#Debates2020 OR #Debates OR #DebateNight OR \",\n            \"#Biden OR #Biden2020 OR #BidenHarris2020 OR \",\n            \"#Trump OR #Trump2020 OR #TrumpPence2020\")\n\ntype <- \"recent\"\nnum_tweets <- 45000\nmax_iter <- 40\n\n# pres debate 1 search\nlatest_id <- \"1311121700394807296\"    # start tweet\nearliest_id <- \"1311104723978579968\"  # end tweet\n\n\nThe search collection is performed by the vosonSML function Collect. The process is more involved than the streaming collection in that the reset time for the rate-limit is calculated each collection iteration and the script sleeps for that period of time before continuing. Tracking of search progress is also logged to the console in this approach but was redirected to a log file.\n\n\nShow code\n\nlibrary(vosonSML)\n\nauth <- readRDS(\"~/.vsml_oauth2\")\n\ncat(\"large twitter search\\n\")\ncat(paste0(\"type: \", type, \"\\n\"))\ncat(paste0(\"tweets per iter: \", num_tweets, \"\\n\"))\ncat(paste0(\"max iter: \", max_iter, \" (\", (max_iter * num_tweets), \" tweets)\\n\\n\"))\n\ni <- 1\nwhile (i <= max_iter) {\n  cat(paste0(\"iteration \", i, \" of \", max_iter, \"\\n\"))\n  cat(paste0(\"time: \", Sys.time(), \"\\n\"))\n  cat(paste0(\"set max_id: \", latest_id, \"\\n\"))\n  req_time <- as.numeric(Sys.time())\n  reset_time <- req_time + (15 * 60) + 10 # add 10 sec buffer\n  \n  code_wd <- getwd()\n  setwd(searches_path)\n  \n  data <- tryCatch({\n    auth %>%\n      Collect(searchTerm = q,\n              searchType = type,\n              numTweets = num_tweets,\n              max_id = latest_id,\n              verbose = TRUE,\n              includeRetweets = TRUE,\n              retryOnRateLimit = TRUE,\n              writeToFile = TRUE)\n  }, error = function(e) {\n    cat(paste0(e, \"\\n\"))\n    NULL\n  })\n  \n  setwd(code_wd)\n  \n  if (!is.null(data) && nrow(data)) {\n    data_first_obvs_id <- data$status_id[1]\n    data_last_obvs_id <- data$status_id[nrow(data)]\n    cat(paste0(\"data nrows = \", nrow(data), \"\\n\",\n               \"first row status id = \", data_first_obvs_id, \"\\n\",\n               \"last row status id = \", data_last_obvs_id, \"\\n\"))\n    \n    # set latest id to lowest status id in data for NEXT iteration\n    # this is typically the last observation\n    latest_id <- data_last_obvs_id\n    \n    # if our target id is passed then stop\n    if (earliest_id >= latest_id) {\n      cat(\"earliest id reached\\n\")\n      break\n    }\n    now_time <- as.numeric(Sys.time())\n    if (i < max_iter) {\n      sleep_time <- reset_time - now_time\n      if (sleep_time > 0) {\n        cat(\"sleeping \", sleep_time, \" secs\\n\")\n        Sys.sleep(sleep_time)  \n      }      \n    }\n  } else {\n    cat(\"no data\\n\")\n    break\n  }\n  i <- i + 1\n}\n\ncat(paste0(\"completed: \", Sys.time(), \"\\n\"))\n\n\nThe first search collection iteration collected a full 45,000 tweets and wrote an R dataframe object to an RDS file. The vosonSML output also indicates the minimum and maximum tweet status ID in the data and their timestamp (UTC) to assist with tracking the collection progress, it shows that the first 45,000 tweets were all created within an approximate 2.25 min period. It took 5.5 mins for the first collection to complete, and it slept for over 9.5 mins while the rate-limit reset before iteration 2. Timestamps other than the tweet creation time are in local time AEST.\nlarge twitter search\ntype: recent\ntweets per iter: 45000\nmax iter: 40 (1800000 tweets)\n\niteration 1 of 40\ntime: 2020-10-03 08:50:31\nset max_id: 1311134926167834628\nCollecting tweets for search query...\nSearch term: #PresidentialDebate OR #PresidentialDebates OR\n#Election2020 OR #Debates2020 OR #Debates OR #DebateNight OR\n#Biden OR #Biden2020 OR #BidenHarris2020 OR #Trump OR #Trump2020\nOR #TrumpPence2020\nRequested 45000 tweets of 45000 in this search rate limit.\nRate limit reset: 2020-10-03 09:05:32\nDownloading [=========================================] 100%\n\ntweet  | status_id           | created             | screen_name   \n-------------------------------------------------------------------\nMin ID | 1311134367985565697 | 2020-09-30 02:42:47 | @pxxxxx_xx   \nMax ID | 1311134926167834628 | 2020-09-30 02:45:00 | @ixxxxxxxxxxx\nCollected 45000 tweets.\nRDS file written: ./data/pres-debate-searches/2020-10-03_085550-TwitterData.rds\nDone.\nElapsed time: 0 hrs 5 mins 22 secs (321.79)\ndata nrows = 45000\nfirst row status id = 1311134926167834628\nlast row status id = 1311134367985565697\nsleeping  588.2078  secs\n54 RDS data files containing Twitter collection dataframes were written and the search took approximately 17.5 hours. These files were then merged into a single complete search collection dataframe for the first debate.\nFirst Presidential Debate Preliminary Results\nData Summary\nData was collected by multiple team members, presented are the un-merged results from a single members streaming and search collections for the first presidential debate.\nTable 2: Collection summary\nTwitter API endpoint\nStart time (EDT)\nEnd time (EDT)\nPeriod (hours)\nObservations (unique tweets)\nStreaming\n2020-09-29 20:30\n2020-09-30 00:30\n4.00\n449,102\nSearch\n2020-09-29 20:45\n2020-09-29 22:45\n2.00\n2,387,587\nData Tweet Activity\nTime series plots for the streaming and search collections were created to indicate tweet activity over time. Observations are grouped by tweet type and into 5 minute bins. Perhaps unsurprisingly, retweet activity appears to became more prevalent as the first debate progressed. At around 10.20pm in the search collection just over 100,000 retweets were collected.\nFigure 2: Streaming collection time series plotFigure 3: Search collection time series plotNetwork Analysis\nUsing vosonSML the Twitter data for both streaming and search collections is able to be converted into networks in the same way. The following code will demonstrate the merging of search collection data, and creation of an activity and actor network for the first debate.\nMerge Collected Data\n\n\nlibrary(dplyr)\n\n# combine the search data files and remove duplicate tweets\n\n# get all of the files to combine\nfiles <- list.files(path = \"./data/pres-debate-searches/\",\n                    pattern = \".+TwitterData\\\\.rds$\", full.names = TRUE)\n\n# merge dataframes\ncomb_data <- bind_rows(lapply(files, function(x) { readRDS(x) }))\ndata <- comb_data %>% arrange(desc(status_id))\n\n# find and remove any duplicates\ndupes <- data %>% filter(status_id %in% data$status_id[duplicated(data$status_id)])\nif (nrow(dupes)) { data <- data %>% distinct(status_id, .keep_all = TRUE) }\n\nsaveRDS(data, \"./data/pres_debate1_search.rds\") # save combined data\n\n\nCreate Networks\nBecause the combined search data for the first debate is quite large, it can be useful to reduce it to a much smaller window of time for demonstration and network visualization purposes. The following code will extract 15 minutes of tweet data from between 21:30 - 21:45 EDT.\n\ndata <- readRDS(\"./data/pres_debate1_search.rds\") # load the previously saved data\n\n# filter out tweets with creation timestamps outside of the 15 min window\ndata <- data %>% filter(created_at >= as.POSIXct(\"2020-09-30 01:30:00\", tz = \"UTC\") &\n                        created_at <= as.POSIXct(\"2020-09-30 01:45:00\", tz = \"UTC\"))\n\n> nrow(data)\n[1] 349937\n> min(data$created_at)\n[1] \"2020-09-30 01:30:00 UTC\"\n> max(data$created_at)\n[1] \"2020-09-30 01:45:00 UTC\"\n\nThe data is now comprised of 349,937 unique tweets that all were created during our specified window. We can now create our networks using vosonSML.\n\nlibrary(vosonSML)\n\n# use the vosonsml create function to create networks\n\n# activity network\n> net_activity <- data %>% Create(\"activity2\")\nGenerating twitter activity network...\n-------------------------\ncollected tweets | 349937\ntweet            | 125626\nretweet          | 212842\nreply            | 7316\nquote            | 4210\nnodes            | 366029\nedges            | 349994\n-------------------------\nDone.\n\n# actor network\n> net_actor <- data %>% Create(\"actor2\")\nGenerating twitter actor network...\n-------------------------\ncollected tweets | 349937\ntweet mention    | 15021\ntweet            | 125626\nretweet          | 212842\nreply mention    | 2202 \nreply            | 7316 \nquote mention    | 734  \nquote            | 4210 \nnodes            | 202333\nedges            | 367951\n-------------------------\nDone.\n\nThe activity network has 366,029 nodes or unique tweets, and the actor network has 202,333 nodes or unique actors for our 15 minute window.\nReply-network Giant Component\nIf we’re interested in exploring some of the Twitter interactions taking place between users during the 21:30 - 21:45 window of the first debate, the network can be further distilled by looking at the actor network and including only reply edges. This will reveal a number of reply-conversations, but we can select for the giant component to find the largest one. The igraph library can be used to perform a number of common network operations such as removing self-loops, isolates and finding the giant component.\n\nlibrary(igraph)\n\n# convert vosonsml actor network to igraph object\n> g_actor <- net_actor %>% Graph()\nCreating igraph network graph...Done.\n\n# remove edges that are not replies\n# remove self-loops and isolates\ng_actor_reply <- g_actor %>% delete_edges(E(g_actor)[E(g_actor)$edge_type != \"reply\"])\ng_actor_reply <- g_actor_reply %>% simplify(remove.multiple = FALSE)\ng_actor_reply <- g_actor_reply %>%\n  delete_vertices(V(g_actor_reply)[which(degree(g_actor_reply) == 0)])\n\n# find the giant component\ncomps <- clusters(g_actor_reply, mode = \"weak\")\nlargest_cluster_id <- which.max(comps$csize)\nnode_ids <- V(g_actor_reply)[comps$membership == largest_cluster_id]\ng_actor_reply_gc <- induced_subgraph(g_actor_reply, node_ids)\n\nThe giant component in the reply-network has 1743 nodes and 1982 edges. We can use igraphs degree function to further explore who the most prominent actors are in the network by in-degree.\n\n> V(g_actor_reply_gc)$screen_name[order(degree(g_actor_reply_gc, mode = \"in\"), decreasing = TRUE)]\n[1] \"realDonaldTrump\"    \"JoeBiden\"           \"GOPLeader\"         \n[4] \"KamalaHarris\"       \"ProjectLincoln\"     \"Alyssa_Milano\"\n\nTo visualise the conversation occurring during this 15 minutes of the debate, we first removed the Twitter accounts for the two candidates (\"realDonaldTrump\", \"JoeBiden\", \"GOPLeader\"), and then constructed a new giant component from the reply network, which now has 1345 nodes and 1484 edges.\nThe visualization of this network (using Gephi) with node size proportional to in-degree is below.\nFigure 4: Reply-network giant component with candidates removed. Figure created by Robert Ackland using Gephi.This graph provides some insight into the largest reply network at our chosen point in time, revealing the Twitter actors receiving the most reply attention and their associations.\n\n\n\n",
    "preview": "posts/2021-06-03-us-presidential-debates-2020-twitter-collection/debate_preview.png",
    "last_modified": "2023-02-13T21:59:09+11:00",
    "input_file": {},
    "preview_width": 1024,
    "preview_height": 1024
  },
  {
    "path": "posts/2021-03-23-twitter-conversation-networks/",
    "title": "Twitter Conversation Networks",
    "description": "Getting started with the voson.tcn package.",
    "author": [
      {
        "name": "Bryan Gertzel",
        "url": {}
      }
    ],
    "date": "2021-03-23",
    "categories": [
      "rstats",
      "twitter",
      "conversations",
      "voson.tcn",
      "networks"
    ],
    "contents": "\n\nContents\nTwitter Developer\nAccess\nInstallation\nAuthentication\nCollection\nNetwork Creation\nActivity Network\nActor Network\n\nPlot Graphs\nActivity Network\nActor Network\n\n\n The VOSON Lab has recently published to GitHub a new\nopen source R package called voson.tcn.\nThe package uses the Early-Access Twitter API v2, to collect tweets\nbelonging to specified threaded conversations and generate networks. The\nTwitter API v2 provides a new tweet identifier: the conversation\nID, that is common to all tweets that are part of a conversation,\nand can be searched for using the API search endpoints. Identifiers and\nassociated metadata for referenced tweets can also be collected in the\nsame search for conversation tweets, allowing us to construct twitter\nnetworks with tweet and user metadata whilst minimising API\nrequests.\nTwitter Developer Access\nThe voson.tcn package requires developer app\nauthentication keys or tokens to access the Twitter API v2. These can be\neither the Access token & secret of an app or its\nBearer token.\nTo obtain these credentials and use the early-access API you will\nneed to have or apply for a Twitter\nDeveloper Account, as well as have activated the new Developer\nPortal. Once approved you will then need to create a development project,\nwhich is the new management container for apps, and either create a new\napp or\nassociate one of your existing apps with it.\nThere are currently two project types available that correspond to\nTwitter’s developer product\ntracks, a standard and academic type.\nAcademic projects are only available to researchers who have\ncompleted and have had their application\nfor the academic research track approved for non-commercial research\npurposes. Standard projects are for more general use, including\nhobby and educational purposes. The project type features differ in\ntheir API access and caps; standard projects can only access\nthe 7-day recent search endpoint whereas an academic project\ncan access the full-archive\nsearch endpoint for historical tweets. There are also rate-limits\nand monthly tweet caps for API v2 search endpoints. At the time of\nwriting, the caps are 500k and 10 million tweets that can be retrieved\nper month for the standard and academic track projects\nrespectively.\nPlease note that there are also terms\nof use and restricted use cases that should be considered before\napplying for access or using the Twitter API.\nInstallation\nThe voson.tcn R package is in development and currently\nonly available on GitHub. It can be\ninstalled as follows:\n\n\n# use the remotes package to install the latest dev version of voson.tcn from github\nlibrary(remotes)\ninstall_github(\"vosonlab/voson.tcn\")\n\n# Downloading GitHub repo vosonlab/voson.tcn@HEAD\n# √  checking for file\n# -  preparing 'voson.tcn':\n# √  checking DESCRIPTION meta-information ... \n# -  checking for LF line-endings in source and make files and shell scripts\n# -  checking for empty or unneeded directories\n# -  building 'voson.tcn_0.1.6.9000.tar.gz'\n#    \n# * installing *source* package 'voson.tcn' ...\n# ...\n# * DONE (voson.tcn)\n# Making 'packages.html' ... done\n\n\n\nAuthentication\nThe voson.tcn package only supports app based\nauthentication using OAuth2.0 tokens which are also known\nas bearer\ntokens. We will likely support user based tokens in the future,\nhowever at this stage they do not offer any advantages as they have\nlower rate-limits and we are not using any private metadata of which\nthey permit access (such as user-visible only metrics).\nThe token can be created using either your apps\naccess token & secret (also known as consumer keys) or\nits bearer token. It is recommended that this token is\nsaved for future use; there is no need to perform this step more than\nonce as the token will not change unless it is invalidated or you\nregenerate keys on the developer portal.\n\n\nlibrary(voson.tcn)\n\n# retrieves a bearer token from the API using the apps consumer keys\ntoken <- tcn_token(consumer_key = \"xxxxxxxx\",\n                   consumer_secret = \"xxxxxxxx\")\n\n# alternatively if you have a bearer token already you can assign it directly\ntoken <- list(bearer = \"xxxxxxxxxxxx\")\n\n# if you save the token to file this step only needs to be done once\nsaveRDS(token, \"~/.tcn_token\")\n\n\n\nCollection\nCollecting conversation tweets requires the tweet ID or URL of a\ntweet that belongs to each threaded conversation that you are interested\nin. These are passed to the voson.tcn collect function\ntcn_threads as a vector or list. Conversation IDs will be\ntracked by this function to avoid duplication and, if tweet IDs are\nfound to belong to a conversation that has already been collected on,\nthen that conversation will be skipped.\nIn the following example, we are collecting the tweets for a threaded\nconversation belonging to a public lockdown announcement following a\nCOVID-19 outbreak in Brisbane, Queensland, Australia, that took place on\nMarch, 29, 2021. The tweet URL or ID (number following the status in the\nURL) can be passed directly to the collection function.\nFigure 1: Public announcement tweet\nregarding a COVID-19 lockdown of Brisbane, from the Queensland Premier.\nEmbedded tweet sourced from Twitter.\n\n# read token from file\ntoken <- readRDS(\"~/.tcn_token\")\n\n# collect the conversation thread tweets for supplied ids           \ntweets <- tcn_threads(\"https://twitter.com/AnnastaciaMP/status/1376311897624956929\", token)\n\n\n\nWhen completed, a list of named dataframes will be returned, with\ntweets containing all of the tweets and their metadata, and\nusers containing all of the referenced users in the tweets\nand their metadata. In our example, 286 tweets were collected with 180\nassociated users public metadata.\nNote that the collection of a threaded tweet conversation is a\nsnapshot of the state of the conversation at a point in time. Metrics\nand networks produced from our data will not completely match subsequent\ncollections of the same conversation, as it will have likely\ncumulatively expanded over time.\n\n\n# collected tweets\nprint(tweets$tweets, n = 3)\n# # A tibble: 286 x 14\n#   in_reply_to_user~ conversation_id  source  author_id  tweet_id  ref_tweet_type\n#   <chr>             <chr>            <chr>   <chr>      <chr>     <chr>         \n# 1 15999~            137631189762495~ Twitte~ 134852208~ 13763373~ replied_to    \n# 2 1142316897985163~ 137631189762495~ Twitte~ 126908387~ 13763373~ replied_to    \n# 3 25683~            137631189762495~ Twitte~ 137503906~ 13763371~ replied_to    \n# # ... with 283 more rows, and 8 more variables: ref_tweet_id <chr>, text <chr>,\n# #   created_at <chr>, includes <chr>, public_metrics.retweet_count <int>,\n# #   public_metrics.reply_count <int>, public_metrics.like_count <int>,\n# #   public_metrics.quote_count <int>\n\n# users metadata\nprint(tweets$users, n = 3)\n# # A tibble: 180 x 12\n#   profile.username profile.created_~ profile.profile_~ user_id profile.descript~\n#   <chr>            <chr>             <chr>             <chr>   <chr>            \n# 1 MSMW~            2013-03-30T06:48~ https://pbs.twim~ 131592~ \"Fact checking i~\n# 2 bpro~            2012-12-04T02:07~ https://pbs.twim~ 987844~ \"Only way to get~\n# 3 scre~            2009-10-22T22:56~ https://pbs.twim~ 844463~ \"I'm a  creative~\n# # ... with 177 more rows, and 7 more variables: profile.name <chr>,\n# #   profile.verified <lgl>, profile.location <chr>,\n# #   profile.public_metrics.followers_count <int>,\n# #   profile.public_metrics.following_count <int>,\n# #   profile.public_metrics.tweet_count <int>,\n# #   profile.public_metrics.listed_count <int>\n\n\n\nIf interested in text analysis, the tweet text can be found in the\ntext column of the tweets dataframe and user\nprofile descriptions in profile.description of the\nusers dataframe.\nPublic metrics for tweets and users are\nfound in dataframe columns prefixed, with public_metrics\nand profile.public_metrics respectively.\n\n\nlibrary(dplyr)\n\nnames(select(tweets$tweets, starts_with(\"public_metrics\")))\n# [1] \"public_metrics.retweet_count\" \"public_metrics.reply_count\"\n# [3] \"public_metrics.like_count\" \"public_metrics.quote_count\"\n\nnames(select(tweets$users, starts_with(\"profile.public_metrics\")))\n# [1] \"profile.public_metrics.followers_count\"\n# [2] \"profile.public_metrics.following_count\"\n# [3] \"profile.public_metrics.tweet_count\"\n# [4] \"profile.public_metrics.listed_count\"\n\n\n\nNetwork Creation\nThere are two types of networks that can be generated using\nvoson.tcn: activity and actor\nnetwork. These differ by the type of node and resulting structure of the\nnetworks.\nActivity Network\nAn activity network is a representation of the\nconversation as seen on Twitter: nodes are tweets and the edges are how\nthey are related. Tweets (or nodes) are identified by their unique\nidentifier Tweet ID (formerly Status ID). In a\nTwitter threaded conversation, there are only two types of connections\nor edges between tweets and these are replied_to and\nquoted.\nReplies are made when a user chooses the reply option and\npublishes a tweet response to the tweet they are replying to. Quotes are\na little different in that the user has included a link to or\nquoted another tweet in the body of their tweet. In Twitter\nconversation networks, it is common to quote a tweet as part of\na reply tweet, generating in the activity network a\nreplied_to and quoted edge from the same\nnode.\n\n\n# generate an activity network\nactivity_net <- tcn_network(tweets, \"activity\")\n\n# number of nodes\nnrow(activity_net$nodes)\n# [1] 279\n\n# number of edges\nprint(activity_net$edges, n = 3)\n# # A tibble: 281 x 3\n#   from                to                  type      \n#   <chr>               <chr>               <chr>     \n# 1 1376337359126495232 1376328523518898176 replied_to\n# 2 1376337350163267584 1376325216658317315 replied_to\n# 3 1376337128016113665 1376311897624956929 replied_to\n# # ... with 278 more rows\n\nunique(activity_net$edges$type)\n# [1] \"replied_to\" \"quoted\"\n\n\n\nActor Network\nAn actor network represents the interactions between\nTwitter users in the conversation: nodes are the users and edges are\ntheir connections. As in the activity network, edges are\neither a reply or a quote but edges represent\nthe classification of a tweet connecting users rather than the activity.\nUsers (or nodes) are identified by their unique Twitter\nUser ID. In the actor network, interactions\nbetween users are more apparent and can be measured by the frequency\n(and direction) of edges between them.\n\n\n# generate an actor network\nactor_net <- tcn_network(tweets, \"actor\")\n\n# number of nodes or actors\nnrow(actor_net$nodes)\n# [1] 180\n\nprint(actor_net$edges, n = 3)\n# # A tibble: 286 x 6\n#   from      to        type  tweet_id     created_at    text                     \n#   <chr>     <chr>     <chr> <chr>        <chr>         <chr>                    \n# 1 13485220~ 15999128~ reply 13763373591~ 2021-03-29T0~ \"@Ther~ @Scott~\n# 2 12690838~ 11423168~ reply 13763373501~ 2021-03-29T0~ \"@Luke~ @Annas~\n# 3 13750390~ 25683344~ reply 13763371280~ 2021-03-29T0~ \"@AnnastaciaMP You do un~\n# # ... with 283 more rows\n\nunique(actor_net$edges$type)\n# [1] \"reply\" \"quote\" \"tweet\"\n\n\n\nNote that in the actor network there is an additional\nedge type: tweet, which is assigned to a self-loop edge\ncreated for the thread’s initial tweet. This is a technique used to\nretain the initial tweet’s metadata as edge attributes comparable to\nother edges in the network.\nThe initial conversation tweet would not usually be included in the\nedge list, as the initial conversation tweet is not directed at another\nuser, and hence no edge to attach metadata.For example, this allows the\ntext of the initial tweet to be included in any actor network tweet text\nanalysis. It would not usually be included in the edge list as the\ninitial conversation tweet is not directed at another user, and hence no\nedge to attach metadata is naturally found in this type of network.\nPlot Graphs\nActivity Network\nVisualisation of the activity network produced with\nigraph.\n\n\nlibrary(igraph)\nlibrary(RColorBrewer)\n\ng <- graph_from_data_frame(activity_net$edges, vertices = activity_net$nodes)\n\n\n\n\n\nShow code\n\n# change likes to log scale\nlike_count <- V(g)$public_metrics.like_count\nlike_count[is.na(like_count)] <- 0\nln_like_count <- log(like_count)\nln_like_count[!is.finite(ln_like_count)] <- 0\n\n# set node size based on likes, min size 4\nsize <- ln_like_count * 2\nV(g)$size <- ifelse(size > 0, size + 8, 4)\n\n# set node label if number of likes >= 2\nV(g)$label <- ifelse(like_count >= 2, like_count, NA)\nV(g)$label.color <- \"black\"\n\n# set node colors based on number of retweets low to high is yellow to green\n# set tweets with no retweets to grey\nrt_count <- V(g)$public_metrics.retweet_count\nrt_count[is.na(rt_count)] <- 0\ncols <- colorRampPalette(c(\"yellow1\", \"green3\"))\ncols <- cols(max(rt_count) + 1)\nV(g)$color <- cols[rt_count + 1]\nV(g)$color[which(rt_count < 1)] <- \"lightgrey\"\n\n# set edge color to orange if tweet quoted another tweet\nE(g)$color <- ifelse(E(g)$type == \"quoted\", \"orange\", \"grey\")\n\n\n\n\n\n# plot the graph using fruchterman reingold layout\nset.seed(200)\ntkplot(g,\n       canvas.width = 1024, canvas.height = 1024,\n       layout = layout_with_fr(g),\n       edge.arrow.size = 0.5,\n       edge.width = 2)\n\n\n\nFigure 2: Conversation activity network -\nNode size and label represent number of tweet likes, color scale is\nindicating low to high number of retweets (yellow to green). Orange\ncoloured edges are quoting linked tweet.voson.tcn collects tweets that are all linked to each\nother via the conversation ID. This means that in a network\ngenerated from this data, such as the activity network, all of the nodes\n(tweets) should be connected in a single component per\nconversation ID. If multiple conversation IDs\nwere collected on then, it is also possible to have one component\nbecause of quote edges. These edges joining conversations occur\nwhen a tweet in one conversation has quoted a tweet in another that you\nhave collected.\nIn the example activity network above, there are two components even\nthough only one conversation ID was collected on. Multiple\ncomponents are usually due to a missing conversation tweet not able to\nbe retrieved from the API and producing a broken reply chain. This can\noften occur if, for example, a tweet has been deleted, or the tweet or\nuser flagged or suspended in some way restricting public\navailability.\nActor Network\nVisualisation of the actor network produced with\nigraph.\n\n\nlibrary(dplyr)\nlibrary(magrittr)\nlibrary(stringr)\n\nregex_ic <- function(x) regex(x, ignore_case = TRUE)\n\n# best effort set the node colour attribute based on presence of city, state,\n# or country in the actors profile location field\n# value assigned from first match\nnodes <- actor_net$nodes %>%\n  mutate(color = case_when(\n    str_detect(profile.location, regex_ic(\"brisbane|bris\")) ~ \"orange\",\n    str_detect(profile.location, regex_ic(\"queensland|qld\")) ~ \"gold\",\n    str_detect(profile.location, regex_ic(\"australia|oz\")) ~ \"yellow\",\n    TRUE ~ \"lightgrey\"))\n\ng2 <- graph_from_data_frame(actor_net$edges, vertices = nodes)\n\n\n\n\n\nShow code\n\n# the following code de-clutters the actor network by removing some nodes\n# that are not part of conversation chains and are stand-alone replies to\n# the initial thread tweet\n\n# get the author of the initial thread tweet using the conversation id\nconversation_ids <- c(\"1376311897624956929\")\nthread_authors <- activity_net$nodes %>%\n  filter(tweet_id %in% conversation_ids) %>% select(user_id)\n\n# remove actors replying to the initial tweet that have a degree of 1\nthread_spokes <- unlist(\n  incident_edges(g2, V(g2)[which(V(g2)$name %in% thread_authors$user_id)],\n                 \"in\"))\nspokes_tail_nodes <- V(g2)[tail_of(g2, thread_spokes)]$name\ng2 <- delete_vertices(g2, degree(g2) == 1 & V(g2)$name %in% spokes_tail_nodes)\n\n# convert the graph to undirected\n# simplify the graph and collapse edges into an edge weight value\nE(g2)$weight <- 1\ng2 <- as.undirected(simplify(g2, edge.attr.comb = list(weight = \"sum\")))\ng2 <- delete_vertices(g2, degree(g2) == 0)\n\n# use edge weight for graph edge width\nE(g2)$width <- ifelse(E(g2)$weight > 1, E(g2)$weight + 1, 1)\n\n# use the actors followers count for node size \nfollowers_count <- log(V(g2)$profile.public_metrics.followers_count)\nfollowers_count[!is.finite(followers_count)] <- 0\nsize <- followers_count * 3\nV(g2)$size <- ifelse(size < 6, 6, size)\nV(g2)$label <- ifelse(followers_count > 0,\n                      V(g2)$profile.public_metrics.followers_count, NA)\n\n\n\n\n\n# plot the graph using automatically chosen layout\nset.seed(201)\ntkplot(g2,\n       canvas.width = 1024, canvas.height = 1024,\n       layout = layout_nicely(g2),\n       vertex.label.cex = 0.8,\n       vertex.label.color = \"black\")\n\n\n\nFigure 3: Conversation actor network -\nNode size and label represent users follower counts. Node color\nindicates user self-reported location. Edge width represents number of\ncollapsed edges.\n\n\n",
    "preview": "posts/2021-03-23-twitter-conversation-networks/activity_network.png",
    "last_modified": "2022-08-04T14:14:49+10:00",
    "input_file": {},
    "preview_width": 1024,
    "preview_height": 1025
  },
  {
    "path": "posts/2021-03-15-hyperlink-networks-with-vosonsml/",
    "title": "Hyperlink Networks with vosonSML",
    "description": "An introduction to creating hyperlink networks with vosonSML.",
    "author": [
      {
        "name": "Bryan Gertzel",
        "url": {}
      }
    ],
    "date": "2021-03-15",
    "categories": [
      "rstats",
      "hyperlinks",
      "vosonSML",
      "networks"
    ],
    "contents": "\n\nContents\nIntroduction\nInstallation\n\nHyperlink Collection\nSetting Up\nPerforming the\nCollection\n\nNetwork Creation\nNetworks\nPlot a Graph\n\n\nThe VOSON software for\nhyperlink collection and analysis was an early research output of the\nVOSON Lab (Ackland 2010).\nIt addressed a need for tools that could help study online social\nnetworks, even before the rise of social media, and assisted researchers\ngain insights into important phenomena such as networks around issue\nspheres and online social movements [see (Ackland and O’Neil 2011) and (Ackland 2013)]. After many years and many\niterations since its inception in 2004, the VOSON Lab is happy to\nreintroduce the canonical VOSON hyperlink collection software as part of\nour R open-source toolkit for social media collection:\nvosonSML.1\nThis simple guide will demonstrate how to use the new features of the\nvosonSML package to perform a hyperlink collection and\ngenerate networks for analysis.\nIntroduction\nThe vosonSML hyperlink collection and network creation\nworks similarly to the 3-step process we use with other social media\nsources: the Authenticate, Collect and\nCreate verb functions. The Authenticate\nfunction is first called with the parameter “web” to identify and set up\nthe context for subsequent operations, but it does not require any\nfurther credentials in this implementation. vosonSML uses\nstandard web crawling and text-based page scraping techniques to\ndiscover hyperlinks and, as such, there is no need to access any\nrestricted data API’s as we commonly do with social media.\nInstallation\nThe new hyperlink collection and network features are currently\navailable in the development version of vosonSML on GitHub, and\nare to soon be released on CRAN. The development version can be\ninstalled as follows:\n\n\n# use the remotes package to install the latest dev version of vosonSML from github\nlibrary(remotes)\ninstall_github(\"vosonlab/vosonsml\")\n\n# Downloading GitHub repo vosonlab/vosonsml@HEAD\n# √  checking for file\n# -  preparing 'vosonSML':\n# √  checking DESCRIPTION meta-information ... \n# -  checking for LF line-endings in source and make files and shell scripts\n# -  checking for empty or unneeded directories\n# -  building 'vosonSML_0.30.00.9000.tar.gz'\n#    \n# * installing *source* package 'vosonSML' ...\n# ...\n# * DONE (vosonSML)\n# Making 'packages.html' ... done\n\n\n\nHyperlink Collection\nSetting Up\nThe web sites or pages to collect hyperlinks from are specified and\ninput to the Collect function in a dataframe. As there are\npage specific options that can be used, this format helps us to organise\nand set the request parameters. The URL’s set in the dataframe for the\npage column are called ‘seed pages’ and are the starting\npoints for web crawling. Although not explicitly indicated in the URL’s,\nthe seed pages are actually the landing pages or “index” pages of the\nweb sites and a page name can be specified if known or desired.\n\n\n# set sites as seed pages and set each for external crawl with a max depth\npages <- data.frame(page = c(\"http://vosonlab.net\",\n                             \"https://www.oii.ox.ac.uk\",\n                             \"https://sonic.northwestern.edu\"),\n                    type = c(\"ext\", \"ext\", \"ext\"),\n                    max_depth = c(2, 2, 2))\n\n\n\nThe example above shows seed pages with some additional per-seed\nparameters that are used to control the web crawling. The\ntype parameter can be set to a value of either\nint, ext or all, which correspond\nto following only internal, external or following all hyperlinks found\non a seeded web page and subsequent pages discovered from that\nparticular seed. How a hyperlink is classified is determined by the seed\ndomain name, for example, if the seed page is\nhttps://vosonlab.net a type of ext will follow\nhyperlinks from that page that do not have a domain name of\n“vosonlab.net”. A type of int will follow only hyperlinks\nthat match a domain of “vosonlab.net”, and a type of all\nwill follow all hyperlinks found irrespective of their domain. The final\nparameter max_depth refers to how many levels of pages to\nfollow from the seed page. In the diagram below, the green dots are\npages scraped by the web crawler and the blue dots links are the\nhyperlinks collected from them for a max depth of 1,2 and 3.\nFigure 1: Scope of hyperlinks collected\nusing the max_depth parameterAs can be seen, a max depth of 1 directs the crawler to scrape and\ncollect hyperlinks from only seed pages, a max depth of 2 to follow\nhyperlinks found on the seed pages and collect hyperlinks from those\npages as well, and so on radiating outwards. The number of pages and\nhyperlinks can rise very rapidly so it is best to keep this number as\nlow as possible. If a greater reach in collection sites is desired, this\ncould perhaps more efficiently be achieved by revising and adding more\nseed pages in the first instance. In the example code the\ntype has been set to “ext” (external) for all three seed\nsites, so as to limit “mapping” of the internal seed web sites and focus\non their outward facing connections. Depth of crawl was set to 2.\nIt should be noted that all hyperlinks found are collected from\nscraped pages and used to generate networks. The type and\nmax_depth parameters only apply to the web crawling and\nscraping activity.\nPerforming the Collection\nThe hyperlink data can now be collected using the\nCollect function with the pages parameter.\nThis produces a dataframe that contains the hyperlink URL’s found, pages\nthey were found on and other metadata that can be used to help construct\nnetworks.\n\n\nlibrary(magrittr)\nlibrary(dplyr)\nlibrary(vosonSML)\n\n# set up as a web collection and collect the hyperlink data using the\n# previously defined seed pages\nhyperlinks <- Authenticate(\"web\") %>% Collect(pages)\n\n# Collecting web page hyperlinks...\n# *** initial call to get urls - http://vosonlab.net\n# * new domain: http://vosonlab.net \n# + http://vosonlab.net (10 secs)\n# *** end initial call\n# *** set depth: 2\n# *** loop call to get urls - nrow: 6 depth: 2 max_depth: 2\n# * new domain: http://rsss.anu.edu.au \n# + http://rsss.anu.edu.au (0.96 secs)\n# ...\n\n# dataframe structure\nglimpse(hyperlinks)\n# Rows: 1,163\n# Columns: 9\n# $ url       <chr> \"http://rsss.anu.edu.au\", \"http://rsss.cass.anu.edu.au\", \"ht~\n# $ n         <int> 1, 1, 4, 1, 1, 2, 2, 2, 2, 2, 2, 1, 1, 1, 4, 1, 1, 1, 1, 1, ~\n# $ page_err  <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, ~\n# $ page      <chr> \"http://vosonlab.net\", \"http://vosonlab.net\", \"http://vosonl~\n# $ depth     <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ~\n# $ max_depth <dbl> 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ~\n# $ parse     <df[,6]> <data.frame[26 x 6]>\n# $ seed      <chr> \"http://vosonlab.net\", \"http://vosonlab.net\", \"http://vos~\n# $ type      <chr> \"ext\", \"ext\", \"ext\", \"ext\", \"ext\", \"ext\", \"ext\", \"ext\", \"ext~\n\n# number of pages scraped for hyperlinks\nnrow(hyperlinks %>% distinct(page))\n# [1] 38\n\n# number of hyperlinks collected\nnrow(hyperlinks)\n# [1] 1163\n\n\n\nA total of 1,163 hyperlinks were collected from 38 pages followed\nfrom our 3 seed pages. Using this data, it is now possible to generate\nhyperlink networks.\nNetwork Creation\nNetworks\nAs with other vosonSML social media, there are two\nstandard types of networks we can create. An activity\nnetwork that produces a more structural representation of the network\nwhere nodes are web pages and edges are the hyperlink references between\nthem, and an actor network that instead groups pages into\nentities based on their domain names.\n\n\n# generate a hyperlink activity network\nactivity_net <- Create(hyperlinks, \"activity\")\n\n# generate a hyperlink actor network\nactor_net <- Create(hyperlinks, \"actor\")\n# Generating web actor network...\n# Done.\n\n\n\nThe output of the network creation is a named list of two dataframes,\none for the nodes and the other for the edges\nor edge list data. The example below shows the actor_net.\nNote that the edges of the actor network are also\naggregated into a weight value and that actors can link to themselves\nforming self-loops.\n\n\nprint(as_tibble(actor_net$nodes))\n# # A tibble: 185 x 2\n#   id                              link_id\n#   <chr>                             <int>\n# 1 accounts.google.com                   1\n# 2 alumni.kellogg.northwestern.edu       2\n# 3 anu.edu.au                            3\n# # ... with 182 more rows\n\nprint(as_tibble(actor_net$edges))\n# # A tibble: 226 x 3\n#   from            to              weight\n#   <chr>           <chr>            <int>\n# 1 rsss.anu.edu.au anu.edu.au           2\n# 2 rsss.anu.edu.au rsss.anu.edu.au     36\n# 3 rsss.anu.edu.au soundcloud.com       1\n# # ... with 223 more rows\n\n\n\nPlot a Graph\nNow that the network has been generated, we can create a graph and\nplot it. The Graph function creates an igraph\nformat object that can be directly plotted or adjusted for presentation\nusing igraph plotting parameters.\n\n\nlibrary(igraph)\nlibrary(stringr)\n\nactor_net <- Create(hyperlinks, \"actor\")\n\n# identify the seed pages and set a node attribute\nseed_pages <- pages %>%\n  mutate(page = str_remove(page, \"^http[s]?://\"), seed = TRUE)\nactor_net$nodes <- actor_net$nodes %>%\n  left_join(seed_pages, by = c(\"id\" = \"page\"))\n\n# create an igraph from the network\ng <- actor_net %>% Graph()\n\n# set node colours\nV(g)$color <- ifelse(degree(g, mode = \"in\") > 1, \"yellow\", \"grey\")\nV(g)$color[which(V(g)$seed == TRUE)] <- \"dodgerblue3\"\n\n# set label colours\nV(g)$label.color <- \"black\"\nV(g)$label.color[which(V(g)$seed == TRUE)] <- \"dodgerblue4\"\n\n# set labels for seed sites and nodes with an in-degree > 1\nV(g)$label <- ifelse((degree(g, mode = \"in\") > 1 | V(g)$seed), V(g)$name, NA)\n\n# simplify and plot the graph\nset.seed(200)\ntkplot(simplify(g),\n       canvas.width = 1024, canvas.height = 1024,\n       layout = layout_with_dh(g),\n       vertex.size = 3 + (degree(g, mode = \"in\")*2),\n       vertex.label.cex = 1 + log(degree(g, mode = \"in\")),\n       edge.arrow.size = 0.4,\n       edge.width = 1 + log(E(g)$weight))\n\n\n\nFigure 2: Hyperlink network of\nactorsWe now have a simple graph of the actor hyperlink network. Our seed\nactors are indicated by blue nodes and sites with an in-degree greater\nthan one indicated in yellow. Node size and label size reflect most\nlinked to nodes or highest in-degree. Perhaps unsurprisingly, social\nmedia sites and the institutions at which the seed pages are located\nfeature most prominently in the network, and the graph plot provides us\na view of the actors online presence and connections.\nThere is much more network visualisation and analysis that could be\nperformed on the vosonSML hyperlink networks and we will be\nworking to add more features such as text analysis and network\nrefinements in our near future releases. In the meantime, we hope you\nhave found this practical introduction to our new tool useful and look\nforward to your feedback!\n\n\n\nAckland, R. 2010. “WWW Hyperlink Networks.” Edited by D. L.\nHansen and B. Shneiderman and M. A. Smith. Morgan-Kaufmann.\n\n\n———. 2013. Web Social Science: Concepts, Data and Tools for Social\nScientists in the Digital Age. SAGE Publications.\n\n\nAckland, R., and M. O’Neil. 2011. “Online Collective Identity: The\nCase of the Environmental Movement.” Social Networks 33\n(3): 177–90. https://doi.org/10.1016/j.socnet.2011.03.001.\n\n\nReferences compiled by Francisca\nBorquez↩︎\n",
    "preview": "posts/2021-03-15-hyperlink-networks-with-vosonsml/hyperlink_network.png",
    "last_modified": "2022-08-04T14:14:49+10:00",
    "input_file": {},
    "preview_width": 1024,
    "preview_height": 1025
  },
  {
    "path": "posts/2021-02-11-twitter-vosonsml-from-rtweet/",
    "title": "Creating Twitter Networks with vosonSML using rtweet Data",
    "description": "Simple guide to collecting data with rtweet and generating networks with vosonSML.",
    "author": [
      {
        "name": "Bryan Gertzel",
        "url": {}
      }
    ],
    "date": "2021-02-11",
    "categories": [
      "rstats",
      "twitter",
      "vosonSML",
      "rtweet",
      "networks"
    ],
    "contents": "\n\nContents\nIntroduction\nAPI Authentication\n\nTwitter Data\nCollection with rtweet\nSearch Collection\nSave the Data\n\nCreating Networks with\nvosonSML\nRead the Data\nPrepare the Data\nCreate the Network\n\n\nIntroduction\nSocial media platforms are a rich resource for Social Network data.\nTwitter is a highly popular public platform for social commentary that,\nlike most social media supporting third-party applications, allow\nsoftware to access and retrieve it’s data via Application Programming\nInterfaces or API’s.\nBecause of its popularity with individuals and communities around the\nworld, the ready availability of its data, and low barrier for entry,\nTwitter has become of great interest as a data source for online\nempirical research.\nThere have been many pieces of software developed across\nprogramming languages and environments to access the Twitter\nAPI. Within the R ecosystem the most comprehensive and well\nsupported of Twitter packages is rtweet\ndeveloped by Michael Kearney and part of the rOpenSci initiative. The\nrtweet package provides R functions to both authenticate\nand collect timelines, tweets and other metadata using Twitter’s v1.1\nstandard and premium API’s.\nThe VOSON Lab\ndevelops and maintains the open source R packages vosonSML\nand VOSONDash. These were created to integrate online data\ncollection, network generation and analysis into a consistent and easy\nto use work flow across many popular web and social media platforms. For\nTwitter, the vosonSML package provides an interface to\nrtweet’s collection features through which tweets can be searched for\nand retrieved, and then uses this data to produce networks. There may be\ncases however, such as in the collection of streaming data or analysis\nof previously collected twitter data where you haven’t used vosonSML’s\ncollection function but instead simply wish to produce\nvosonSML generated networks from your rtweet\ndata. Because vosonSML uses rtweet this is\neasily achievable and with minimal R coding.\nAPI Authentication\nAccessing the Twitter API to collect tweets requires authentication\nvia a Twitter app. There are generally two ways this can be achieved,\nyou can apply for a Twitter Developer account and create your own app\n(and access keys) or you can authorize another persons app to access the\nAPI on your behalf (using their keys). The latter still requires your\nown Twitter user account but you do not need to go through the Developer\napplication or app creation process. The vosonSML package\nrequires users to create their own app and use their own keys but the\nrtweet package supports both methods, and you can collect\ntweets after a simple one-time web authorization step of their embedded\nrstats2twitter app.\nTwitter Data Collection\nwith rtweet\nThe following simple example will demonstrate how to use the\nrtweet package to collect some tweet data using built-in\nauthentication via the rtweet app.\nSearch Collection\nA fairly standard tweet collection usually involves using the Twitter\nSearch API endpoint to search for past tweets that meet a\ncertain criteria. This can be done with rtweet and the\nsearch_tweets function with the criteria set by passing\nadditional parameters. In our example we will direct the API to search\nand return 100 tweets (n = 100) containing the hashtag\n#auspol and excluding any retweets\n(include_rts = FALSE). By default only the most recent\ntweets within the last 7 days will be returned by the API.\n\n\nlibrary(rtweet)\n\n# recent tweet search collection\nauspol_tweets <- search_tweets(\"#auspol\", n = 100, include_rts = FALSE)\n\n#> Requesting token on behalf of user...\n#> Waiting for authentication in browser...\n#> Press Esc/Ctrl + C to abort\n#> Authentication complete.\n\n\n\nThe first time rtweet collection functions are run they\nwill open a Twitter web page on your default web browser asking\npermission to authorize rstats2twitter.\n\n\n\nFigure 1: rstats2twitter app authorization\n\n\n\nIf API authentication and search succeeds then the\nsearch_tweets function will return a data frame of tweet\ndata. The data frame will have up to 100 rows, one for each tweet\ncollected and 90 columns for associated tweet metadata:\n\n\nlibrary(tibble)\n\n# print the first 2 rows\nprint(auspol_tweets, n = 2)\n# # A tibble: 100 x 90\n#   user_id  status_id  created_at          screen_name text      source\n#   <chr>    <chr>      <dttm>              <chr>       <chr>     <chr> \n# 1 27007685 136400068~ 2021-02-22 23:54:39 ronth~      \"@janeen~ Twitt~\n# 2 1359301~ 136400067~ 2021-02-22 23:54:37 Injur~      \"When th~ Twitt~\n\n\n\n\n\nShow additional columns\n\n# # ... with 98 more rows, and 84 more variables:\n# #   display_text_width <dbl>, reply_to_status_id <chr>,\n# #   reply_to_user_id <chr>, reply_to_screen_name <chr>,\n# #   is_quote <lgl>, is_retweet <lgl>, favorite_count <int>,\n# #   retweet_count <int>, quote_count <int>, reply_count <int>,\n# #   hashtags <list>, symbols <list>, urls_url <list>,\n# #   urls_t.co <list>, urls_expanded_url <list>, media_url <list>,\n# #   media_t.co <list>, media_expanded_url <list>, media_type <list>,\n# #   ext_media_url <list>, ext_media_t.co <list>,\n# #   ext_media_expanded_url <list>, ext_media_type <chr>,\n# #   mentions_user_id <list>, mentions_screen_name <list>, lang <chr>,\n# #   quoted_status_id <chr>, quoted_text <chr>,\n# #   quoted_created_at <dttm>, quoted_source <chr>,\n# #   quoted_favorite_count <int>, quoted_retweet_count <int>,\n# #   quoted_user_id <chr>, quoted_screen_name <chr>,\n# #   quoted_name <chr>, quoted_followers_count <int>,\n# #   quoted_friends_count <int>, quoted_statuses_count <int>,\n# #   quoted_location <chr>, quoted_description <chr>,\n# #   quoted_verified <lgl>, retweet_status_id <chr>,\n# #   retweet_text <chr>, retweet_created_at <dttm>,\n# #   retweet_source <chr>, retweet_favorite_count <int>,\n# #   retweet_retweet_count <int>, retweet_user_id <chr>,\n# #   retweet_screen_name <chr>, retweet_name <chr>,\n# #   retweet_followers_count <int>, retweet_friends_count <int>,\n# #   retweet_statuses_count <int>, retweet_location <chr>,\n# #   retweet_description <chr>, retweet_verified <lgl>,\n# #   place_url <chr>, place_name <chr>, place_full_name <chr>,\n# #   place_type <chr>, country <chr>, country_code <chr>,\n# #   geo_coords <list>, coords_coords <list>, bbox_coords <list>,\n# #   status_url <chr>, name <chr>, location <chr>, description <chr>,\n# #   url <chr>, protected <lgl>, followers_count <int>,\n# #   friends_count <int>, listed_count <int>, statuses_count <int>,\n# #   favourites_count <int>, account_created_at <dttm>,\n# #   verified <lgl>, profile_url <chr>, profile_expanded_url <chr>,\n# #   account_lang <lgl>, profile_banner_url <chr>,\n# #   profile_background_url <chr>, profile_image_url <chr>\n\n\n\nThis contains all of the data necessary for vosonSML to\nconstruct Twitter networks.\nSave the Data\nThere are a few methods of saving data depending on where and how it\nwill be used. Two common methods are to use a text-based file format\nsuch as a CSV, or\nalternatively if the data will be used within R we can save\nthe dataframe object to a binary compressed\nRDS (R data object) file using saveRDS\ninstead. Conveniently, the rtweet package has a method to\nsave Twitter data to file in CSV format with the write_as_csv\nfunction that takes care of Twitter nested data and conversion issues,\nand saving an RDS file is also very easy as follows.\n\n\n# save data using rtweet write csv\nwrite_as_csv(auspol_tweets, \"auspol_tweets.csv\")\n\n# save data to file as an R data object\nsaveRDS(auspol_tweets, \"auspol_tweets.rds\")\n\n\n\nCreating Networks with\nvosonSML\nRead the Data\nIf the data was saved to file with the rtweet function\nwrite_as_csv it can be read again using\nread_twitter_csv or readRDS if from an\nRDS file.\n\n\nauspol_tweets <- read_twitter_csv(\"auspol_tweets.csv\")\n\nauspol_tweets <- readRDS(\"auspol_tweets.rds\")\n\n\n\nPrepare the Data\nFor vosonSML to recognize the previously collected data\nas a Twitter data source and be able to internally route it to the\nappropriate network functions a minor change needs to be made to the\ndata frame first. This involves adding two attributes\ndatasource and twitter to the class list of\nthe auspol_tweets data frame object as follows:\n\n\n# original class list\nclass(auspol_tweets)\n\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n# add to the class list\nclass(auspol_tweets) <- append(c(\"datasource\", \"twitter\"), class(auspol_tweets))\n\n# modified class list\nclass(auspol_tweets)\n\n\n[1] \"datasource\" \"twitter\"    \"tbl_df\"     \"tbl\"        \"data.frame\"\n\nThe order of classes is important and for the data frame to be\ncompatible with dplyr - a\nvery common data manipulation package in R, and subsequently usable in\nthe tidyverse and\nvosonSML, then the new attributes need to be added to the\nbeginning of the list.\nFor versions of vosonSML more recent than\n0.29.13 this can now all be managed by using the\nImportData function. This method is preferable as it is\neasier, works for both files and data frames, and will support any\nfuture updates to vosonSML without breaking your code.\n\n\nlibrary(vosonSML)\n\n# use the import data function\nauspol_tweets <- ImportData(auspol_tweets, \"twitter\")\n\n\n\nPlease note that modifying data frame attributes or importing data is\nonly required for rtweet data and not a necessary step for\nTwitter data collected using the vosonSML Twitter\nCollect function.\nObject classes in R are a more advanced topic and not required\nknowledge to use vosonSML but if you would like to learn\nmore a good introduction can be found in the Object-oriented programming\nchapter of Advanced R by Hadley Wickham.\nCreate the Network\nThe tweet data can now be used to create the nodes and edges network\ndata, and a graph by using the vosonSML Create\nand Graph functions:\n\n\n# create the network data\nauspol_actor_network <- Create(auspol_tweets, \"actor\")\n\n\nGenerating twitter actor network...\n-------------------------\ncollected tweets | 100\ntweet mention    | 26\ntweet            | 57\nreply mention    | 15\nreply            | 25\nquote mention    | 7 \nquote            | 18\nnodes            | 149\nedges            | 148\n-------------------------\nDone.\n\n\n\n# create an igraph\nauspol_actor_graph <- Graph(auspol_actor_network)\n\n\nCreating igraph network graph...Done.\n\nThat’s all there is to it, and now the resulting igraph\nnetwork can be plotted.\n\n\nlibrary(igraph)\n\n# set plot margins\npar(mar = c(0, 0, 0, 0))\n\n# auspol actor network with fruchterman-reingold layout\nplot(auspol_actor_graph, layout = layout_with_fr(auspol_actor_graph),\n     vertex.label = NA, vertex.size = 6, edge.arrow.size = 0.4)\n\n\n\n\nFigure 2: Actor network graph for collected #auspol tweets\n\n\n\nFor further information about rtweet, its features and\nhow to use it to collect twitter data please refer to the package site and\nintroductory rtweet\nvignette. For creating different types of networks such as the\nactivity, 2-mode and semantic\ntypes with vosonSML see the package documentation and\nintroductory vosonSML\nvignette.\n\n\n\n",
    "preview": "posts/2021-02-11-twitter-vosonsml-from-rtweet/rtweet_logo_preview.png",
    "last_modified": "2022-08-04T14:14:49+10:00",
    "input_file": {},
    "preview_width": 432,
    "preview_height": 499
  },
  {
    "path": "posts/2021-02-05_welcome/",
    "title": "Welcome to the VOSON Lab Code Blog",
    "description": "The code blog is a space to share tools, methods, tips, examples and code. A place to collect data, construct and analyze online networks.",
    "author": [
      {
        "name": "Robert Ackland",
        "url": "http://vosonlab.net/"
      }
    ],
    "date": "2021-02-04",
    "categories": [
      "rstats",
      "SNA",
      "Computational Social Science"
    ],
    "contents": "\nWelcome to the VOSON Lab Code Blog! We have created this space to share methods, tips, examples and code. It’s also a place where we will demonstrate constructing and analyzing networks from various API and other online data sources.\nMost of our posts will cover techniques around the tools we have developed at the Lab: vosonSML, VOSONDash and voson.tcn, which are available on both CRAN and GitHub. But we also plan to use this space to cover other complementary R packages and open-source software, such as fantastic R packages within the tidyverse, RStudio’s shiny for web apps, and visualization tools such as igraph and Gephi.\nVOSON Lab R Packages - Hex stickersVOSON Lab Open Source Tools\nvosonSML is a R package for social media data collection (currently twitter, youtube, and reddit), hyperlink collection and network generation. VOSONDash is a Shiny app that integrates tools for visualizing and manipulating network graphs, performing network and text analysis, as well as an interface for collecting data with vosonSML.\nMore information on these packages, their development and code can be found on our vosonSML, VOSONDash and voson.tcn github pages.\nWe also have some other guides for using the packages. Check the vosonSML Vignette and the VOSON Dash Userguide for some practical examples and feature reference.\nWe hope you find this content useful!\nThe VOSON Lab team.\nVirtual Observatory for the Study of Online Networks VOSON Lab, School of Sociology, The Australian National University.\n\n\n\n",
    "preview": "posts/2021-02-05_welcome/square-cards.png",
    "last_modified": "2023-02-10T13:54:31+11:00",
    "input_file": {},
    "preview_width": 800,
    "preview_height": 640
  }
]

[
  {
    "path": "posts/2023-01-20-hyperlink-networks/",
    "title": "Hyperlink networks",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "Robert Ackland",
        "url": {}
      },
      {
        "name": "Francisca Borquez",
        "url": {}
      },
      {
        "name": "Sidiq Madya",
        "url": {}
      }
    ],
    "date": "2023-01-20",
    "categories": [
      "rstats",
      "hyperlink networks",
      "vosonsml"
    ],
    "contents": "\nFirst step - hyperlink network collection\nLet’s first test out hyperlink collection.\nWe are going to use VOSON to collect hyperlinks.\nFirst load the libraries.\n\n\nlibrary(magrittr)\nlibrary(vosonSML)\nlibrary(igraph)\nlibrary(dplyr)\nlibrary(knitr)\nlibrary(stringr)\n\n\nWe create a dataframe containing the pages to be crawled. We are reading a csv file with pages for 20 organisations. These organisations are samples of non-state or non-government entities (for-profit and non-for-profit) which actively engage in ‘data sovereignty’ debates as part of their concern in contemporary data politics. These organisations include NGO, research think thank, media, companies, industry associations, and movements or initiatives from communities. These different group of organisations deal with various issues and values when promoting their agenda ranging from security, privacy, innovation, entrepreneurship, to human rights and social justice.\nBeing involved in the emerging issues of global data politics, these organisations are based or headquartered in different countries across the globe include the US, UK, Canada, Germany, The Netherlands, Belgium, and Denmark which represent the Global North, and South Africa, India and Hong Kong representing the Global South. The websites are being used by these organisations to participate in the emerging debates on data politics. Their participation in the debates are becoming more intense in the midst of the ongoing process of massive ‘digitisation’ and ‘datafication’ in societies. We have chosen 1 or 2 pages from each website and are using these as “seed pages” where the crawler will start.\n\n\n#For more information on collecting hyperlink networks using vosonSML, see:\n#https://vosonlab.github.io/posts/2021-03-15-hyperlink-networks-with-vosonsml/\n\n# seed pages\n# int: collect all hyperlinks but only follow links that have same domain as seed page (internal)\n# ext: collect all hyperlinks but only follow links that have different domain as seed page (external)\n# all: collect and follow all hyperlinks\n# max_depth: how many levels of hyperlinks to follow from seed page\n#pages <- data.frame(page = c(\"http://vosonlab.net\",\n#                             \"https://rsss.cass.anu.edu.au\",\n#                             \"https://www.ansna.org.au\"),\n#                    type = c(\"int\", \"ext\", \"all\"),\n#                    max_depth = c(1, 1, 1))\n\npages <- read.csv(\"seed_sites_20.csv\")\nkable(head(pages))\n\npage\ntype\nmax_depth\ndomain\ncountry\nhttps://womeninlocalization.com/partners/\nint\n1\nwomeninlocalization.com\nUS\nhttps://womeninlocalization.com/data-localization-laws-around-the-world/\nint\n1\nwomeninlocalization.com\nUS\nhttps://iwgia.org/en/network.html\nint\n1\niwgia.org\nDenmark\nhttps://www.iwgia.org/en/indigenous-data-sovereignty/4699-iw-2022-indigenous-data-sovereignty.html?filter_tag[0]=37\nint\n1\niwgia.org\nDenmark\nhttps://indigenousdatalab.org/networks/\nint\n1\nindigenousdatalab.org\nUS\nhttps://indigenousdatalab.org/projects-working/\nint\n1\nindigenousdatalab.org\nUS\n\n#remove pages that caused error with crawler\n#Rob to check this again..error might no longer be present with new version of vosonSML\npages <- pages %>% filter(!grepl(\"ispa.org.za\", pages$page))\n\n\nNote that for the crawler all we need is a dataframe with three columns: page, type, max_depth (explain these), but of course it might be useful to include other meta data in this file that is used in analysis later.\nNow run the crawl.\n\n\n#Remember to set `verbose=TRUE` to see the crawler working\ncrawlDF <- Authenticate(\"web\") %>% Collect(pages, verbose=TRUE)\ncrawlDF\n\n#We will save this dataframe, for use later\n#saveRDS(crawlDF, \"crawlDF.rds\")\nsaveRDS(crawlDF, \"crawlDF_20_sites_depth1.rds\")\n\n\nCreating hyperlink networks\nLet’s now create networks from the crawl data.\n\n\ncrawlDF <- readRDS(\"crawlDF_20_sites_depth1.rds\")\n\n# explore dataframe structure\nglimpse(crawlDF)\n\nRows: 2,826\nColumns: 9\n$ url       <chr> \"http://goap-global.com\", \"http://www.reddit.com/s…\n$ n         <int> 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1,…\n$ page_err  <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ page      <chr> \"https://womeninlocalization.com/partners\", \"https…\n$ depth     <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ max_depth <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ parse     <df[,6]> <data.frame[23 x 6]>\n$ seed      <chr> \"https://womeninlocalization.com/partners\", \"ht…\n$ type      <chr> \"int\", \"int\", \"int\", \"int\", \"int\", \"int\", \"int\", \"…\n\n# create activity network: nodes are pages hyperlinks were collected from\nnet_activity <- crawlDF %>% Create(\"activity\")\ng_activity <- net_activity %>% Graph(writeToFile = TRUE)\nplot(g_activity, layout=layout_with_fr(g_activity), vertex.label=\"\", vertex.size=3, edge.width=1, edge.arrow.size=0.5)\n\n\n# create actor network: nodes are site domains of pages hyperlinks were collected from\nnet_actor <- crawlDF %>% Create(\"actor\")\ng_actor <- net_actor %>% Graph(writeToFile = TRUE)\ng_actor\n\nIGRAPH 283e79f DN-- 497 2826 -- \n+ attr: type (g/c), name (v/c)\n+ edges from 283e79f (vertex names):\n [1] womeninlocalization.com->goap-global.com           \n [2] womeninlocalization.com->www.reddit.com            \n [3] womeninlocalization.com->csa-research.com          \n [4] womeninlocalization.com->euatc.org                 \n [5] womeninlocalization.com->locworld.com              \n [6] womeninlocalization.com->slator.com                \n [7] womeninlocalization.com->translatedinargentina.com \n [8] womeninlocalization.com->translation-conference.com\n+ ... omitted several edges\n\nplot(g_actor, layout=layout_with_fr(g_actor), vertex.label=\"\", vertex.size=3, edge.width=1, edge.arrow.size=0.5)\n\n\n\nFor the rest of this exercise we will use the actor network. The actor network has 497 nodes and 2826 edges. We will now look at three approaches for processing hyperlink network data: pruning, preserving and pagegrouping [REF to Rob’s earlier work, VOSON software].\nPruning\nPruning refers to removing nodes that are considered not relevant to the analysis. It is highly likely that the web crawler will pick up pages that are not relevant to the study, and so we use pruning to identify and remove these irrelevant pages.\nSeed sites only\n\n\n# identify the seed pages and set a node attribute\n# we are also removing http tag and trailing forward slash\nseed_pages <- pages %>%\n  mutate(page = str_remove(page, \"^http[s]?://\"), seed = TRUE)\n# also remove trailing \"/\"\nseed_pages <- seed_pages %>%\n  mutate(page = str_remove(page, \"/$\"))\n\n#Because default vosonsml behaviour when creating actor network is to \n#not \"preserve\" subdirectories, this means that seed site:\n#womeninlocalization.com/partners \n#is in the network as:\n#womeninlocalization.com\n#As a HACK for time being, we will ignore the preservation issue\n#So adjust the URL in seed_pages accordingly, so we can get identify all seeds in the network\n\n#The following will return just the domain name\na <- str_match(seed_pages$page, \"(.+?)/\")\nseed_pages$page <- ifelse(grepl(\"/\", seed_pages$page), a[,2], seed_pages$page)\n\nseed_pages <- seed_pages %>% distinct(page, .keep_all=TRUE)\n\nnrow(seed_pages)\n\n[1] 21\n\nnet_actor$nodes <- net_actor$nodes %>%\n  left_join(seed_pages, by = c(\"id\" = \"page\"))\n\n\n# create an igraph object from the network\ng <- net_actor %>% Graph()\n\n#simplify the network - remove loops and multiple edges\nE(g)$weight <- 1\ng <- simplify(g)\n\n#OK now have all 11 seeds in the network\ntable(V(g)$seed)\n\n\nTRUE \n  21 \n\n#TRUE \n#11 \n\n#create igraph graph of just the seed sites\ng_seed <- induced.subgraph(g, which(V(g)$seed==TRUE))\nplot(g_seed, layout=layout_with_fr(g_seed), vertex.label=\"\", vertex.size=3, edge.width=1, edge.arrow.size=0.5)\n\n\nV(g)$color <- ifelse(V(g)$seed==TRUE, \"red\", \"blue\")\nplot(g, layout=layout_with_fr(g), vertex.label=\"\", vertex.size=3, edge.width=E(g)$weight, edge.arrow.size=0.5)\n\n\n\nSeed sites plus “important” other sites\n\n\ng_seedImp <- induced.subgraph(g, which(V(g)$seed==TRUE | (degree(g)>=2)))\nplot(g_seedImp, layout=layout_with_fr(g_seedImp), vertex.size=3, edge.width=1, edge.arrow.size=0.5)\n\n\n\nPruning\nThe above “seeds plus important” network has sites that are not relevant to the study. let’s prune them.\n\n\n\n",
    "preview": "posts/2023-01-20-hyperlink-networks/hyperlink-networks_files/figure-html5/unnamed-chunk-4-1.png",
    "last_modified": "2023-02-10T16:47:25+11:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2022-06-05-egocentric-networks-from-twitter-timelines/",
    "title": "Egocentric Networks from Twitter timelines",
    "description": "Demonstration of how to use rtweet and vosonSML to construct an ego net from Twitter users timelines.",
    "author": [
      {
        "name": "Bryan Gertzel",
        "url": {}
      }
    ],
    "date": "2022-08-22",
    "categories": [
      "rstats",
      "twitter",
      "networks",
      "egocentric",
      "rtweet",
      "vosonsml",
      "timeline"
    ],
    "contents": "\n\nContents\nIntroduction\nCollect the ego timeline\nCollect timelines of\nalters\nCreate an actor network\nOptionally add\nuser metadata as node attributes\nCreate an ego subgraph\nVisualise with an\ninteractive visNetwork plot\n\n\nUpdated for rtweet v1.0.2 and\nvosonSML\nv0.32.7.This article does not require Twitter API keys, but does\nrequire a twitter account and for the user to authorise the rtweet\nrstats2twitter app when prompted.\n\nIntroduction\nEgocentric networks or ego nets are networks that focus on a\nparticular actor (the ego) and map out their connections to other\nactors. In an ego net other actors are referred to as alters, and by\ncollecting the outward expanding connections of alters, ego nets of\nvarying degrees can be constructed (see Hogan,\n2011, p. 168). Some literature and software refer to these as\nneighborhood networks, with varying orders instead of degrees. For\nexample in a friendship network, a neighborhood network of the first\norder (1.0 degree) contains just the friends of the ego, whereas a\nnetwork of the second order (sometimes second step) also contains\n“friends of friends” (a 2.0 degree ego net).\nBy collecting the tweets in a Twitter users timeline, and the\ntimelines of users referenced, we can create a 1.0, 2.0 or 1.5 degree\nnetwork for the ego. A 1.5 degree network is similar to the 1.0 degree,\nexcept it also contains relationships or ties between the alters, or\n“between friends” of the ego from the previous friendship network\nexample.\nIt should be noted that by using user timelines that this is not\nnecessarily a friendship network, but instead a network of twitter users\nwho are associated through tweet activity. This kind of ego net can lead\nto insights beyond simply declared friendships (obtained from Twitter’s\nfriend/follower metadata) as the network structure is the result of\nusers interactions on the platform over a period of time.\nThis post will demonstrate how to construct an ego networks from a\ntwitter timelines using the rtweet package to collect\ntweets and vosonSML to create an actor network.\nCollect the ego timeline\nThe first step is to collect the ego’s timelime. In this post we will\nbe using the VOSON Lab @vosonlab twitter account, and\ncollecting the Twitter timeline using rtweet. The Twitter\nAPI restricts the number of timeline tweets that can be collected to the\nmost\nrecent 3,200 tweets, but we can set this to a lesser value e.g most\nrecent 100 tweets, and also use the same parameter for alters timelines\nfor the purposes of this demonstration.\n\n\nlibrary(dplyr)\nlibrary(DT)\nlibrary(rtweet)\nlibrary(vosonSML)\n\n# get twitter user timeline\nego_tweets <- get_timeline(c(\"vosonlab\"), n = 100, token = NULL)\n\n\n\n\n# convert rtweet data into vosonSML format\nego_tweets <- ego_tweets |> ImportRtweet()\n\n# create actor network from timeline tweets\nego_net <- ego_tweets |> Create(\"actor\", verbose = TRUE)\n\nGenerating twitter actor network...\n-------------------------\ncollected tweets | 100\ntweet mention    | 62\ntweet            | 25\nretweet          | 66\nquote mention    | 13\nquote            | 9 \nnodes            | 41\nedges            | 175\n-------------------------\nDone.\n\nA result of 41 nodes indicates that there are\n40 alters in the network with ties to the ego.\n\n\n\nFigure 1: vosonlab 1.0 degree actor network\n\n\n\nCollect timelines of alters\nFrom the previous step we created an actor network represented as\nnodes and edges dataframes from the ego’s tweet timeline. We can now use\nthis to extract all of the user ids of the alters in the network.\nNote that we have not specified the degree of the ego net at this\nstage, however by virtue of the twitter data (timeline tweets) having\nall been created by the ego user, we can assume all of the alters\n(referenced users) are connected to the ego in this network.\n\n\n# get ego user id\nego_user_id <- ego_net$nodes |>\n  filter(screen_name == \"vosonlab\") |> pull(user_id)\n\n# get list of alter user ids from network\nalter_user_ids <- ego_net$nodes |>\n  filter(user_id != ego_user_id) |> distinct(user_id) |> pull()\n\n\nUsing the alters user ids the timeline tweets can be collected and\nimported into vosonSML as follows:\n\n\n# get 100 most recent tweets from all of the alters timelines\n# and convert to vosonSML format\nalters_tweets <- alter_user_ids |>\n  get_timeline(n = 100, retryonratelimit = TRUE) |>\n  ImportRtweet()\n\n# Error: Number of tweet observations does not match number of users. 3526 != 99\n\n\n\nPlease note there seems to be an inconsistency in timeline results for\nthis version of rtweet and the following workaround can be used instead:\n\n\n\n# workaround for rtweet timeline users issue\nget_alters_timelines <- function(x) {\n  ImportRtweet(get_timeline(user = x, n = 100, retryonratelimit = TRUE))\n}\n\n# collects timelines individually and place into a list\nrequire(purrr)\nalters_tweets <- map(alter_user_ids, get_alters_timelines)\n\n\nAlternatively, if you have your own API access the\nvosonSML Collect function can also be used\nwith the endpoint = \"timeline\" parameter:\n\n\n# requires a previously saved vosonSML twitter auth object\nauth_twitter <- readRDS(\"~/.vsml_auth_tw\")\n\nalters_tweets2 <- auth_twitter |>\n  Collect(\n    endpoint = \"timeline\",\n    users = alter_user_ids,\n    numTweets = 100,\n    verbose = TRUE\n  )\n\n# Collecting timeline tweets for users...\n# Requested 4000 tweets of 150000 in this search rate limit.\n# Rate limit reset: 2022-08-22 06:21:30\n# \n# tweet        | status_id           | created            \n# --------------------------------------------------------\n# Latest Obs   | 1560130378366562304 | 2022-08-18 05:04:02\n# Earliest Obs | 1544727926645596162 | 2022-07-06 17:00:12\n# Collected 3525 tweets.\n# Done.\n\n\nCreate an actor network\nNow that all of the tweets from the alters timelines have also been\ncollected, the data can be merged and a single actor network created.\nThis actor network can be considered a 2.0 degree\nnetwork, as it contains not only the associations or “friends” from the\nego’s timeline, but also the associations or “friends” of the alters\nfrom their timelines.\n\n\n# combine all of the tweets from ego and alters timelines using vosonSML merge\ntweets <- do.call(Merge, alters_tweets)\ntweets <- Merge(ego_tweets, tweets)\n\n# create actor network from combined timeline tweets\nactor_net <- tweets |> Create(\"actor\", verbose = TRUE)\n\nGenerating twitter actor network...\n-------------------------\ncollected tweets | 3626\ntweet mention    | 1030\ntweet            | 1160\nretweet          | 1657\nreply mention    | 735\nreply            | 584\nquote mention    | 187\nquote            | 232\nnodes            | 1818\nedges            | 5585\n-------------------------\nDone.\n\nHere we can see an actor network of 1818 nodes and\n5585 edges, substantially larger than our initial actor\nnetwork.\n\n\n\nFigure 2: vosonlab 2.0 degree actor network\n\n\n\nOptionally add\nuser metadata as node attributes\nAt this point we can optionally add some user metadata to our network\nas node attributes. This allows us to change visual properties of the\nnetwork graph based on actor attributes. For example, we could map the\nnode size to number of followers a twitter user may have.\nPlease note this step requires a vosonSML\ntwitter auth object if you want to use the look up feature for\ncomplete users’ metadata.\n\n\n# this step requires a previously saved vosonSMML twitter auth object\nauth_twitter <- readRDS(\"~/.vsml_auth_tw\")\n\n# add user profile metadata\nactor_net_meta <- actor_net |>\n  AddUserData(tweets, lookupUsers = TRUE, twitterAuth = auth_twitter)\n\n\nHere is a sample of the actor metadata available and an example of\nhow it can be presented and explored using a data table:\n\n\n# node attributes\nnames(actor_net_meta$nodes)\n\n [1] \"user_id\"                 \"screen_name\"            \n [3] \"u.user_id\"               \"u.name\"                 \n [5] \"u.screen_name\"           \"u.location\"             \n [7] \"u.description\"           \"u.url\"                  \n [9] \"u.protected\"             \"u.followers_count\"      \n[11] \"u.friends_count\"         \"u.listed_count\"         \n[13] \"u.created_at\"            \"u.favourites_count\"     \n[15] \"u.verified\"              \"u.statuses_count\"       \n[17] \"u.profile_banner_url\"    \"u.default_profile\"      \n[19] \"u.default_profile_image\" \"u.withheld_in_countries\"\n[21] \"u.derived\"               \"u.withheld_scope\"       \n[23] \"u.utc_offset\"            \"u.time_zone\"            \n[25] \"u.geo_enabled\"           \"u.lang\"                 \n[27] \"u.has_extended_profile\" \n\n# explore actors metadata\nactors_table <- actor_net_meta$nodes |>\n  filter(user_id %in% c(ego_user_id, alter_user_ids)) |>\n  mutate(u.screen_name = paste0(\"@\", screen_name)) |>\n  select(name = u.screen_name,\n         display = u.name,\n         locationu = u.location,\n         description = u.description,\n         followers = u.followers_count,\n         tweets = u.statuses_count) |>\n  slice_head(n = 5)\n\nlibrary(reactable)\n\nreactable(actors_table, bordered = TRUE, striped = TRUE, resizable = TRUE,\n          wrap = FALSE, searchable = TRUE, paginationType = \"simple\")\n\n\n\nCreate an ego subgraph\nA 1.5 degree network can be useful to reveal the associations between\nan ego’s alters. This can be achieved by creating a subgraph of the 2.0\nego network that retains only the previously identified alters (see\nbelow igraph::induced_subgraph). As we know every alter is\nconnected to the ego so it is also often useful to visualise ego\nnetworks without the ego as it is then easier to observe clustering.\n\n\nlibrary(igraph)\n\n# use the vosonSML to convert the network dataframes into an igraph object\ng <- actor_net_meta |> Graph()\n\n# create a subgraph with ego removed\ng2 <- induced_subgraph(g, c(alter_user_ids))\n\ng2\n\nIGRAPH 953ba4f DN-- 40 1922 -- \n+ attr: type (g/c), name (v/c), screen_name (v/c), u.user_id\n| (v/c), u.name (v/c), u.screen_name (v/c), u.location (v/c),\n| u.description (v/c), u.url (v/c), u.protected (v/l),\n| u.followers_count (v/n), u.friends_count (v/n),\n| u.listed_count (v/n), u.created_at (v/c), u.favourites_count\n| (v/n), u.verified (v/l), u.statuses_count (v/n),\n| u.profile_banner_url (v/c), u.default_profile (v/l),\n| u.default_profile_image (v/l), u.withheld_in_countries\n| (v/x), u.derived (v/c), u.withheld_scope (v/l), u.utc_offset\n| (v/l), u.time_zone (v/l), u.geo_enabled (v/l), u.lang (v/l),\n| u.has_extended_profile (v/l), status_id (e/c), created_at\n| (e/c), edge_type (e/c)\n+ edges from 953ba4f (vertex names):\n\nAs we saw in our initial actor network constructed from only the\nego’s timeline we now have 40 nodes again, matching the\nnumber of alters. This actor network has many more edges however, as\n1922 ties or relations between the alters were captured\nfrom the collection of the alters timelines.\nVisualise with an\ninteractive visNetwork plot\nUsing the igraph and visNetwork package we\ncan create a simplified and undirected ego network graph of alters.\nCommunity detection can be performed and visualised using the\nigraph walktrap clustering algorithm and\nFruchterman-Reingold force-directed layout. We can further map some\nvisual properties of nodes to attributes - with node size corresponding\nto the node degree, edge width to combined weight, and color to\nclustering community group.\n\n\nlibrary(visNetwork)\n\n# combine and weight the edges between nodes\nE(g2)$weight <- 1\ng2 <- igraph::simplify(g2, edge.attr.comb = list(weight = \"sum\"))\ng2 <- as.undirected(g2)\n\n# perform some community detection using a random walk algorithm\nc <- walktrap.community(g2)\nV(g2)$group <- membership(c)\n\n# map visual properties of graph to attributes\nE(g2)$width <- ifelse(E(g2)$weight > 1, log(E(g2)$weight) + 1, 1.1)\n\nV(g2)$size <- degree(g2) + 5\nV(g2)$label <- paste0(\"@\", V(g2)$u.screen_name)\n\nvisIgraph(g2, idToLabel = FALSE) |>\n  visIgraphLayout(layout = \"layout_with_fr\") |>\n  visOptions(\n    nodesIdSelection = TRUE,\n    highlightNearest = TRUE\n  )\n\n\n\n\nFigure 3: vosonlab 1.5 degree actor network\n\n\n\nThe final result is an ego net with some clear associations. Colours\nand placement of nodes found to represent some interesting domains and\ncommunity relationships between the Twitter @vosonlab\naccount and its timeline network alters. Isolates represent more distant\nconnections with no detected community ties.\n\n\n\nHogan, B. (2011). Chapter 11 - visualizing and interpreting facebook\nnetworks [Book Section]. In D. L. Hansen, B. Shneiderman & M. A.\nSmith (Eds.), Analyzing social media networks with NodeXL (pp.\n165–179). Morgan Kaufmann. https://doi.org/10.1016/B978-0-12-382229-1.00011-4\n\n\n\n\n",
    "preview": "posts/2022-06-05-egocentric-networks-from-twitter-timelines/egocentric_500x500.png",
    "last_modified": "2022-08-25T15:58:00+10:00",
    "input_file": {},
    "preview_width": 500,
    "preview_height": 500
  },
  {
    "path": "posts/2021-12-06-collecting-youtube-comments-with-vosondash/",
    "title": "Collecting YouTube comments with VOSONDash",
    "description": "Collect YouTube comments and create networks for analysis with VOSONDash",
    "author": [
      {
        "name": "Francisca Borquez",
        "url": {}
      }
    ],
    "date": "2021-12-06",
    "categories": [
      "rstats",
      "SNA",
      "VOSONDash",
      "networks",
      "YouTube",
      "visualisation"
    ],
    "contents": "\nThis guide provides a practical demonstration for collecting comments from YouTube videos and constructing networks, using the VOSON Lab’s interactive R/Shiny app VOSONDash.\nThe GitHub page provides instructions to install and run VOSONDash. More information on features can be accessed in the VOSONDash Userguide(Borquez et al. 2020).\nCollecting YouTube data\nAs for Twitter, YouTube collection requires API keys, which are provided via the Google API console. Similarly, we enter the key in the API Keys window, YouTube tab in VOSONDash and the token can be saved to disk for future use.\nIn this example, we are collecting comments from a YouTube video titled Update on reinfection caused by Omicron variant, which was uploaded by the World Health Organization (WHO) on 5th December 2021 and had attracted 182 comments at the time of data collection (17 December 2021).\nYouTube networks\nVOSONDash (via vosonSML) provides two types of YouTube networks:\nActivity networks – nodes are either comments or videos (videos represent a starting comment), and edges are interactions between comments. In this example, there are 119 nodes and 118 edges. The most central node is the initial post, which receives 100 comments.\nActor networks – nodes are users who have commented on videos and the videos themselves are included in the network as special nodes. Edges are the interactions between users in the comments. We can distinguish two types of edges “top-level comment,” which is a reply to the initial post (video), and “reply to a reply,” when users mention the username of the person they are replying to. In this example, there are 81 nodes and 119 edges. The most central node is the initial post, which receives 110 comments.\nFigure 1: VOSONDash – YouTube collection, Activity and Actor networksThe Blog post Analysing online networks with VOSONDash provides more detail into features for network analysis, network visualisation and text analysis.\n\n\n\nBorquez, F., B. Gertzel, X. Cai, and R. Ackland. 2020. VOSON Dashboard Userguide. Canberra, Australia: VOSON Lab, Australian National University. https://vosonlab.github.io/VOSONDashDocs/.\n\n\n\n\n",
    "preview": "posts/2021-12-06-collecting-youtube-comments-with-vosondash/youtube.png",
    "last_modified": "2022-08-04T14:14:49+10:00",
    "input_file": {},
    "preview_width": 1283,
    "preview_height": 635
  },
  {
    "path": "posts/2021-11-25-collecting-twitter-data-with-vosondash/",
    "title": "Collecting Twitter data with VOSONDash",
    "description": "A short guide to collecting Twitter data and constructing networks for analysis with VOSONDash.",
    "author": [
      {
        "name": "Francisca Borquez",
        "url": {}
      }
    ],
    "date": "2021-11-25",
    "categories": [
      "rstats",
      "SNA",
      "VOSONDash",
      "networks",
      "twitter",
      "visualisation"
    ],
    "contents": "\nThis guide provides a practical demonstration for collecting Twitter data and constructing networks, using VOSON Lab’s interactive R/Shiny app VOSONDash.\nThe GitHub page provides instructions to install and run VOSONDash. More information on features can be accessed in the VOSONDash Userguide(Borquez et al. 2020).\nCollecting Twitter data\nTwitter collection require authentication with OAuth1.0 API keys, provided via Twitter Developer account. Simply, enter the fields in the API Keys window in VOSONDash. The token can be saved to disk for future use.\nIn this example, data were collected on 6 December 2021 and include 200 recent tweets with the hashtags #auspol and #COVID-19 (Fig. 1).\nTwitter networks\nVOSONDash – via vosonSML– provides four types of Twitter networks for analysis:\nActivity networks – where nodes represent tweets and edge types are: replies, retweets and quoted retweets. In this example, there are 225 nodes (excluding isolates) and 200 edges (including multiple edges and loops).\nActor networks – where nodes represent users who have tweeted, or else are mentioned or replied to in tweets. Edges represent interactions between Twitter users, and an edge attribute indicates whether the interaction is a mention, reply, retweet, quoted retweet or self-loop. In this example, there are 212 nodes and 213 edges (including multiple edges and loops).\nFigure 1: VOSONDash – Twitter collection, Activity and Actor networksTwo-mode networks – where nodes are actors (Twitter users) and hashtags, and there is an edge from user i to hashtag j if user i authored a tweet containing hashtag j. In this example, we have removed the hashtags we used in our collection #auspol and #COVID-19. The resulting network has 242 nodes and 213 edges. When clicking on Label attribute, we can observe hashtags and handles used in those tweets.\nFigure 2: VOSONDash – Two-mode networkSemantic networks – where nodes represent entities extracted from the tweet text: common words, hashtags and usernames. Edges reflect co-occurrence of terms. In this example, we have removed the terms #auspol and #COVID-19, and set the parameters to include 5% most frequent words and 50% most frequent hashtags. The resulting network has 55 nodes (excluding isolates), and 127 edges. Label attribute option displays terms in network visualisation.\nFigure 3: VOSONDash – Semantic networkTo learn more about VOSONDash network and text analysis features, see our previous post Analysing online networks with VOSONDash.\n\n\n\nBorquez, F., B. Gertzel, X. Cai, and R. Ackland. 2020. VOSON Dashboard Userguide. Canberra, Australia: VOSON Lab, Australian National University. https://vosonlab.github.io/VOSONDashDocs/.\n\n\n\n\n",
    "preview": "posts/2021-11-25-collecting-twitter-data-with-vosondash/VOSONDash-t.png",
    "last_modified": "2021-12-16T23:45:37+11:00",
    "input_file": {},
    "preview_width": 1366,
    "preview_height": 768
  },
  {
    "path": "posts/2021-10-01-testing-bot-status-of-twitter-users/",
    "title": "Testing the bot status of users in Twitter networks collected via vosonSML",
    "description": "We use vosonSML to collect Twitter data and then use Botometer to test the bot status of a subset of Twitter users.",
    "author": [
      {
        "name": "Robert Ackland",
        "url": {
          "https://orcid.org/0000-0002-0008-1766": {}
        }
      },
      {
        "name": "Francisca Borquez",
        "url": {}
      }
    ],
    "date": "2021-10-08",
    "categories": [
      "rstats",
      "python",
      "SNA",
      "vosonSML",
      "networks",
      "Botometer",
      "bot detection"
    ],
    "contents": "\nIntroduction\nAs social media platforms like Twitter become important spaces for information diffusion, discussion and opinion formation, serious concerns have been raised about the role of malicious socialbots in interfering, manipulating and influencing communication and public opinion Badawy, Ferrara, and Lerman (2018). Their detection and the understanding of consequent dynamics of behaviour are relevant to researchers and it is central to the research collaboration the VOSON Lab is involved in (see Cimiano et al. 2020).\nIn this post we will use vosonSML to collect data via the Twitter API and construct a network represented as an igraph graph object in R. Then, we will identify a subset of users and test their bot status using Botometer1 (in python) and include the bot scores as node attributes in the network graph in R.\nCollecting the Twitter network using vosonSML in R\nThe vosonSML vignette (Ackland, Gertzel, and Borquez 2020) provides comprehensive instructions on how to use vosonSML. In this post, we are going to focus on the essential steps for Twitter collection and network generation via vosonSML. The first step involves loading the vosonSML package into the R session, and use the Web Auth approach to create a Twitter API access token:\n\n\nlibrary(magrittr)\nlibrary(vosonSML)\n\ntwitterAuth <-\n   Authenticate(\n      \"twitter\",\n      appName = \"An App\",\n      apiKey = \"xxxxxxxxxxxx\",\n      apiSecret = \"xxxxxxxxxxxx\")\n\n#Optionally, save the access token to disk:\nsaveRDS(twitterAuth, file = \"twitter_auth.rds\")\n\n#The following loads into the current session a previously-created access token:\ntwitterAuth <- readRDS(\"twitter_auth.rds\")\n\n\n\nThen, we collect 100 tweets that contain the Australian politics hashtag #auspol. Data is saved in .rds dataframe format.\n\n\ntwitterData <- twitterAuth %>%\n   Collect(\n      searchTerm = \"#auspol\",\n      numTweets = 100,\n      includeRetweets = FALSE,\n      retryOnRateLimit = TRUE,\n      writeToFile = TRUE)\n\n\n\nTo read the Twitter dataframe from disk, the ImportData() function modifies the class values for the object before it is used with vosonSML:\n\n\ntwitterData <- ImportData(\"2021-09-30_182359-TwitterData.rds\", \"twitter\")\n\n\n\nAnd now we use the vosonSML functions Create(\"actor\") to create an Actor network and Graph() to create an igraph graph object g.\nThe Create(\"actor\") function generates a named list containing two dataframes named nodes and edges. In this Actor network nodes are users who have either tweeted using the search term #auspol, or else are mentioned or replied to in tweets featuring the search terms. Edges represent interactions between Twitter users, and an edge attribute indicates whether the interaction is a mention, reply, retweet, quoted retweet or self-loop.\n\n\nactorNetwork <- twitterData %>% Create(\"actor\", vertbose=TRUE)\ng <- actorNetwork %>% Graph()\ng\n\n\n\nThe output in the console loos like this:\n\n> actorNetwork <- twitterData %>% Create(\"actor\", vertbose=TRUE)\nGenerating twitter actor network...\n-------------------------\ncollected tweets | 100\ntweet mention    | 119\ntweet            | 62\nreply mention    | 27\nreply            | 21\nquote mention    | 5 \nquote            | 17\nnodes            | 226\nedges            | 251\n-------------------------\nDone.\n`> g <- actorNetwork %>% Graph()\nCreating igraph network graph...Done.\n\nThe graph object g prints as follows:\n\n> g\nGRAPH 527682d DN-- 226 251 -- \n+ attr: type (g/c), name (v/c), screen_name (v/c), status_id (e/c), created_at (e/c),\n| edge_type (e/c)\n+ edges from 527682d (vertex names):\n [1] 1362279599191691264->1116612139          43447495           ->43447495           \n [3] 518488471          ->518488471           353381552          ->353381552          \n [5] 576131356          ->576131356           28305154           ->3288075858         \n [7] 1310795233651601408->3079563404          3842652433         ->3842652433         \n [9] 3219321554         ->3219321554          1327357902424666112->29387813           \n[11] 1327357902424666112->1548253015          1327357902424666112->1327357902424666112\n[13] 37891446           ->37891446            1296548400         ->1296548400         \n+ ... omitted several edges\n\nWe now use igraph to manipulate the network. The simplify(g) function removes multiple edges and loops from the network. Then, we proceed to identify a subset of Twitter users (5), based on indegree, which we will later use in our bot status analysis.\n\n\nlibrary(igraph)\n\n#remove multiple and loop edges\ng <- simplify(g)\n\nV(g)$screen_name[order(degree(g, mode=\"in\"), decreasing=TRUE)][1:5]\n\n\n\nGiven this network was created using tweets that contain the #auspol hashtag, it is not surprising that the top 5 Twitter users based on indegree are four politicians and a political commentator:\n\n[1] \"ScottMorrisonMP\" \"DanielAndrewsMP\" \"JoshFrydenberg\"  \"GladysB\"         \"bruce_haigh\"```  \n\nSince we are going to access the Botometer API via Python, first we need to print the Twitter handles we want to check (5) with Botometer to a .csv file.\n\n\nwrite.csv(data.frame(user=V(g)$screen_name[order(degree(g, mode=\"in\"), decreasing=TRUE)][1:5]), \"top-5_auspol.csv\", row.names=FALSE)\n\n\n\nFinding the bot scores using Botometer in python\nGetting started with Botometer\nWe are now going use the Botometer API to find the bot scores for the 5 Twitter accounts. We will use the python client Botometer-python provided by the Botometer team.\nTo use the Botometer API you need to be able to authenticate using Twitter developer app API keys (same keys you use for Dev Auth approach to authenticating for Twitter collection via vosonSML). You also need a free RapidAPI (previously Mashape) account with the Botometer Pro API enabled (the Basic plan is free).\nBelow is a test of the Botometer python client v4 , using code from the Indiana University Network Science Institute GitHub page.\nTo access Botometer, enter the following code in a python shell or script. The second step involves checking the bot status of a single Twitter account:\n\nimport botometer\n\nrapidapi_key = \"xx\"\ntwitter_app_auth = {\n                    'consumer_key': \"xx\",\n                    'consumer_secret': \"xx\",\n                    'access_token': \"xx\",\n                    'access_token_secret': \"xx\"\n                   }\n\nbom = botometer.Botometer(wait_on_ratelimit=True,\n                          rapidapi_key=rapidapi_key,\n                          **twitter_app_auth)\n\n# Check a single account by screen name\nresult = bom.check_account('@clayadavis')\nprint(result)\n\nThe result of our test prints as follows:\n\n{\n    \"cap\": {\n        \"english\": 0.4197222421546159,\n        \"universal\": 0.6608500314332488\n    },\n    \"display_scores\": {\n        \"english\": {\n            \"astroturf\": 0.2,\n            \"fake_follower\": 1.2,\n            \"financial\": 0.0,\n            \"other\": 0.3,\n            \"overall\": 0.4,\n            \"self_declared\": 0.2,\n            \"spammer\": 0.0\n        },\n        \"universal\": {\n            \"astroturf\": 0.2,\n            \"fake_follower\": 0.9,\n            \"financial\": 0.0,\n            \"other\": 0.3,\n            \"overall\": 0.8,\n            \"self_declared\": 0.0,\n            \"spammer\": 0.1\n        }\n    },\n    \"raw_scores\": {\n        \"english\": {\n            \"astroturf\": 0.04,\n            \"fake_follower\": 0.23,\n            \"financial\": 0.0,\n            \"other\": 0.06,\n            \"overall\": 0.08,\n            \"self_declared\": 0.05,\n            \"spammer\": 0.01\n        },\n        \"universal\": {\n            \"astroturf\": 0.04,\n            \"fake_follower\": 0.18,\n            \"financial\": 0.0,\n            \"other\": 0.06,\n            \"overall\": 0.17,\n            \"self_declared\": 0.0,\n            \"spammer\": 0.02\n        }\n    },\n    \"user\": {\n        \"majority_lang\": \"en\",\n        \"user_data\": {\n            \"id_str\": \"11330\",\n            \"screen_name\": \"test_screen_name\"\n        }\n    }\n}\n\nThe descriptions of elements in the response e.g. users, raw scores, etc., are specified in the GitHub page.\nAnalysing bot status with Botometer in phyton\nThis step involves reading in the .csv file with the 5 Twitter handles into python and run them through the botometer API:\n\nimport pandas as pd\nusers = pd.read_csv(\"top-5_auspol.csv\")\nprint(users)\n\n\ncat(\"user\\n 0  ScottMorrisonMP\\n 1  DanielAndrewsMP\\n 2   JoshFrydenberg\\n 3          GladysB\\n 4  bruce_haigh\\n\")\n\nNow, we collect the botscores and print the Complete Automation Probability (CAP) to the csv file.\n\nresults_dict = {}     #use this to save all botometer results to file\ncap = []              #use this for writing botometer CAP to csv\nfor i in users.user:\n   #print(i)\n   result = bom.check_account('@'+i)\n   #print(result)\n   cap.append([i, result['cap']['english']])\n   results_dict[i] = result\n\n#write CAP score to csv\ndf = pd.DataFrame(cap, columns=[\"user\", \"cap\"])\n#print(df)\ndf.to_csv(\"top-5_auspol_cap.csv\")\n\n#write results dictionary to file\nimport json\njson.dump(results_dict, open(\"top-5_auspol_botometer_results.txt\",'w'))\n#can be read back in with\n#d2 = json.load(open(\"top-5_auspol_botometer_results.txt\"))\n\nThe Botometer API provides scores as Complete Automation Probability (CAP), defined as the probability, according to Botometer models, that an account with a certain score or greater is a bot. More information on how to interpret the scores is available here.\nBot scores as node attributes in the graph in R\nFinally, we can read the botometer scores back into R and include them as a node attribute in the graph.\n\n\ndf2 <- read.csv(\"top-5_auspol_cap.csv\")\ndf2\n\n\n\nThe dataframe looks like this:\n\n> df2 <- read.csv(\"top-5_auspol_cap.csv\")\n> df2\n  X            user       cap\n1 0 ScottMorrisonMP 0.7384783\n2 1 DanielAndrewsMP 0.7966467\n3 2  JoshFrydenberg 0.4756770\n4 3         GladysB 0.7966369\n5 4     bruce_haigh 0.7874002\n\nTo add the bot scores as node attributes, we create a new node attribute “cap” and copy in the scores from the csv file.\n\n\nV(g)$cap <- NA\nV(g)$cap[match(df2$user,V(g)$screen_name)] <- df2$cap\nV(g)$screen_name[!is.na(V(g)$cap)]\n\n\n\nThe console output presenting nodes with the node attribute we just created is as follows:\n\n[1] \"bruce_haigh\"     \"DanielAndrewsMP\" \"JoshFrydenberg\"  \"ScottMorrisonMP\" \"GladysB\" \n\nTo inspect the values, we run the following code:\n\n\nV(g)$cap[!is.na(V(g)$cap)]\n\n\n\n\n[1] 0.7874002 0.7966467 0.4756770 0.7384783 0.7966369\n\n\n\n\nAckland, R., B. Gertzel, and F. Borquez. 2020. Introduction to vosonSML. Canberra, Australia: VOSON Lab, Australian National University. https://cran.r-project.org/web/packages/vosonSML/vignettes/Intro-to-vosonSML.html.\n\n\nBadawy, A., E. Ferrara, and K. Lerman. 2018. “Analyzing the Digital Traces of Political Manipulation: The 2016 Russian Interference Twitter Campaign.” In 2018 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM).\n\n\nCimiano, P., F. Muhle, O. Putz, B. Schiffhauer, E. Esposito, R. Ackland, U. Seelmeyer, and T. Veale. 2020. “Bots Building Bridges (3b): Theoretical, Empirical, and Technological Foundations for Systems That Monitor and Support Political Deliberation Online.” Volkswagen Foundation Grant in AI and the Society of the Future stream (2021-2025).\n\n\nRizoiu, M-A., T. Graham, R. Zhang, Y. Zhang, R. Ackland, and L. Xie. 2018. “#DebateNight: The Role and Influence of Socialbots on Twitter During the 1st U.S. Presidential Debate.” In International AAAI Conference on Web and Social Media.\n\n\nBotometer is a joint project of the Observatory on Social Media (OSoMe) and the Network Science Institute (IUNI) at Indiana University, USA↩︎\n",
    "preview": "posts/2021-10-01-testing-bot-status-of-twitter-users/bot.png",
    "last_modified": "2021-12-16T23:58:11+11:00",
    "input_file": {},
    "preview_width": 1260,
    "preview_height": 600
  },
  {
    "path": "posts/2021-08-05-exploring-issues-in-reddit-using-voson-dash/",
    "title": "Exploring issues in Reddit using VOSON Dash",
    "description": "An easy guide to explore issues in Redddit and construct networks for analysis using VOSONDash.",
    "author": [
      {
        "name": "Francisca Borquez",
        "url": {}
      }
    ],
    "date": "2021-09-23",
    "categories": [
      "rstats",
      "SNA",
      "VOSONDash",
      "networks",
      "text analysis",
      "Reddit"
    ],
    "contents": "\nThe following guide provides a practical demonstration for collecting Reddit data and constructing networks, using VOSON Lab’s interactive R/Shiny app VOSONDash. Reddit – a social news aggregation, content rating, and discussion website – provides the opportunity for researchers to access a wide range of themed-based online discussion data, and to understand the dynamics of these conversations.\nSNA approach to studying online networks\nVOSONDash (and vosonSML) method for network construction is based on Ackland and Zhu (2015) approach, whereby edges in Reddit networks represent implicitly directed ties, i.e. reflecting exchange of opinion between users rather than an explicit social relationship. Conversations threads can be analysed as networks and VOSONDash provides two approaches to constructing Reddit networks:\nActor networks – where nodes represent users who have posted original posts and comments, and edges are the comment interactions between users.\nActivity networks – where nodes are comments or initial thread posts and edges represent either replies to the original post, or replies to comments.\nMethodology\nIn this example, we will collect data from a Reddit post relating to the COVID-19 lockdown in Sydney, Australia, and proceed to use VOSONDash features to demonstrate the data outputs and a quick overview of analysis tools.\nThe post titled Sydney Lockdown extended until the end of September was created on 20 August 2021, and by the time of data collection (23 September 2021) it had attracted 557 comments.\nThe GitHub page provides instructions to install and run VOSONDash. More information on features can be accessed in the VOSONDash Userguide(Borquez et al. 2020).\nCollecting Reddit data\nReddit collection does not require API authentication. Simply go to the Reddit view, enter the URL, and click on Collect Threads. The output of the collection is presented in the right pane (Figure 1). In this example, 494 comments were collected. At this stage, the data can be saved as .rds dataframe.\nFigure 1: VOSONDash – Reddit collectionCreating Reddit Activity networks with VOSONDash\nActivity networks represent the three-like structure of conversations, with nodes being comments or the initial post, and edges being replies to comments or replies to initial post. In this example, we selected the Add text option, so the .graphml file contains text data.\nThe console displays the output of the activity network (Figure 2). The Activity network has 495 nodes (including the initial post), and 494 edges (comments). The network can be saved ad .graphml, if you prefer to use a different tool for analysis.\nCreating Reddit Actor networks with VOSONDash\nIn a similar workflow, we can use the data we just collected to create Actor networks, to observe Reddit users interactions. As mentioned earlier, in Actor networks, nodes are users who have commented, or who have created initial thread posts, and edges represent either replies to the original post, or replies to comments. Again, the Add text option was selected, for the .graphml file to contain text data.\nThe console displays the output of the activity network once the network is created. The Activity network has 302 nodes, and 495 edges.\nFigure 2: Reddit Activity and Actor networksVOSONDash provides three approaches to analyse networks: Network graph, Network metrics (SNA), and Text analysis. These tools are presented in more detail in the post Analysing networks with VOSONDash.\nWe hope this guide has been useful and easy to follow. In the next post, we will cover Twitter data collection with VOSONDash.\n\n\n\nAckland, R., and J. Zhu. 2015. “Social Network Analysis.” In Innovations in Digital Research Methods, edited by P. Halfpenny and R. Procter. London: SAGE Publications.\n\n\nBorquez, F., B. Gertzel, X. Cai, and R. Ackland. 2020. VOSON Dashboard Userguide. Canberra, Australia: VOSON Lab, Australian National University. https://vosonlab.github.io/VOSONDashDocs/.\n\n\n\n\n",
    "preview": "posts/2021-08-05-exploring-issues-in-reddit-using-voson-dash/preview.png",
    "last_modified": "2021-12-16T23:46:43+11:00",
    "input_file": {},
    "preview_width": 1266,
    "preview_height": 700
  },
  {
    "path": "posts/2021-08-06-analysing-online-networks-with-vosondash/",
    "title": "Analysing online networks with VOSONDash",
    "description": "A quick introduction to VOSONDash network and text analysis features",
    "author": [
      {
        "name": "Francisca Borquez",
        "url": {}
      }
    ],
    "date": "2021-09-09",
    "categories": [
      "rstats",
      "SNA",
      "VOSONDash",
      "networks",
      "text analysis",
      "visualisation"
    ],
    "contents": "\nThis post introduces VOSONDash network analysis tools, which include network visualisation, network metrics, and text analysis. Users can analyse different networks including those collected with VOSONDash (Twitter, YouTube and Reddit), or import graphml files collected elsewhere.\nAnalysing online networks with VOSONDash is the first of a series of posts where we will cover VOSONDash features. Data collection with VOSONDash is covered in the following posts:\nTwitter – Collecting Twitter data with VOSONDash\nReddit – Exploring issues in Reddit using VOSON Dash\nYouTube – Collecting YouTube comments with VOSONDash\nAbout VOSONDash\nVOSONDash is an output of computational social methods research, designed to be a “Swiss Army knife” for studying online networks. The R/Shiny dashboard tool enables online data collection, and network and text analysis (including visualisation) within the same environment. VOSONDash builds on a number of R packages, in particular vosonSML for data collection and network generation, and igraph for network analysis. The package provides a graphical user interface which does not require users to have R programming skills and it is available on CRAN and GitHub. Bryan Gertzel is the lead developer and maintainer of VOSONDash.\nStarting VOSONDash\nThe GitHub page provides instructions to install VOSONDash via R or Rstudio. Once the package is installed, run VOSONDash from the RStudio console entering the following code; VOSONDash will open in a web browser.\n\n\nlibrary(VOSONDash)\nrunVOSONDash()\n\n\n\nNetwork data\nTo ease replication, in this example we will use the EnviroActivistsWebsite_2006 demo dataset which is provided in the package. The dataset is a hyperlink network collected with VOSON in 2006, as part of a research piece (Ackland and O’Neil 2011). The network has 161 nodes (websites representing environmental organisations) and 1,444 edges representing hyperlinks between these organisations. In this dataset, text data is stored as node attribute and categorical values are assigned depending on type of environmental organisations (Bios, Globals, and Toxics).\nNetwork analysis using VOSONDash\nThere are three main approaches to analysing online networks with VOSONDash: Network graph, Network metrics (SNA), and Text analysis. More information on features can be accessed in the VOSONDash Userguide (Borquez et al. 2020).\nNetwork graph\nIn Network graph provides two options to explore networks: network visualisation via igraph and visNetwork; and tabulations for nodes and edges. The Network graph pane provides the following options for manipulating the network:\nLabels – to display or not labels.\nGraph Filters – to display or not multiple edges, loops, and isolates.\nLayout – to select graph layout and spread.\nNode Size – to select node size by metric e.g. indegree and define size (multiplier).\nCategorical filter – option available when data contains pre-set categorical values. New collections do not have that option.\nComponent filter – to display weak or strong components and define component range.\nNeighbourhood select – to create subnetworks. It uses ego network terminology of order, where order 1 include ties between the alters.\nFigure 1: VOSONDash network visualisationNetwork metrics\nVia the Network metrics pane, we can observe basic SNA metrics, including network level and node level metrics (e.g. centralisation). Network metrics reflect the applied filters for the visualisation; in this example we removed isolates (3 nodes), so network size is 158 and the Component distribution is 1 (one connected component). Degree distribution is only available for undirected networks; Indegree distribution and Outdegree distribution charts are available for directed networks, like this example. Accordingly, in this network, there are 15 nodes receiving one hyperlink, and three nodes receiving 35 hyperlinks. While 19 nodes link out to only one other site, there are two organisations in this network that link out to 50 sites.\nAssortativity metrics (Homogeneity and Homophily indexes, including mixing matrix and population share) are presented for networks with categorical node attributes. In this example, we have selected the categorical attribute Type. The mixing matrix table presents links across the three types of organisations Globals, Bios and Toxics. The Bios and Globals sub-movements show a strong tendency towards linking to their own type. Population shares, Homogeneity indexes and Homophily indexes are presented by type. Controlling for group size, Globals are the group more biased towards its own type, where 53% of their ties to other Global organisations can be explained by homophily.\nFigure 2: SNA and Assortativity metricsText analysis\nFor a network with text data stored as either node or edge attribute, it is possible to conduct basic text analysis with VOSONDash. Text corpus can be pre-processed using Filters to:\nremove common English words that are not relevant to the analysis, such as “and,” “the,” and “but” using Remove Standard Stopwords,\ncreate own stopword list in User-Defined Stopwords,\nreduce words to their stems with Apply word stemming,\nremove URLs, numbers, or punctuation, based on user’s specifications.\nWord lenght, if need to specify number of characters.\nAdvanced options provide HTML Decode and iconv UTF8, specially useful for social media as text often contains encoded characters.\nFor Twitter networks, two other options become available: Remove Twitter hashtags and Remove Twitter Usernames.\nThere are three methods available to visualise text:\nWord frequency bar charts, where further parameters can be applied such as to define the number of results displayed, and frequency to define Minimum frequency, for the text to appear.\nWord clouds where users can adjust Minimum frequency (how many times a word needs to have been used in order for it to feature in the visualisation); Maximum words to control for the number of words appearing in the graph; percentage of vertical words can be set for legibility; and random colours can be assigned to the visualisation. Comparison clouds are only available for datasets with categorical data, like this example where colour represents the node attribute type (Bios, Globals or Toxics).\nThe Sentiment analysis function uses the Syuzhet package and classifies words based on the NRC Emotion Lexicon, which is a list of English words and their associations with eight basic emotions (anger, fear, anticipation, trust, surprise, sadness, joy, and disgust) and two sentiments (negative and positive).\nFigure 3: Text analysisWe hope this guide is useful and easy to follow.\n\n\n\nAckland, R., and M. O’Neil. 2011. “Online Collective Identity: TheCase of the Environmental Movement.” Social Networks 33: 177–90. https://doi.org/https://doi.org/10.1016/j.socnet.2011.03.001.\n\n\nBorquez, F., B. Gertzel, X. Cai, and R. Ackland. 2020. VOSON Dashboard Userguide. Canberra, Australia: VOSON Lab, Australian National University. https://vosonlab.github.io/VOSONDashDocs/.\n\n\n\n\n",
    "preview": "posts/2021-08-06-analysing-online-networks-with-vosondash/Actor_net.png",
    "last_modified": "2022-08-04T14:14:49+10:00",
    "input_file": {},
    "preview_width": 1282,
    "preview_height": 670
  },
  {
    "path": "posts/2021-06-03-us-presidential-debates-2020-twitter-collection/",
    "title": "#DebateNight 2020: Hashtag Twitter Collection of the US Presidential Debates",
    "description": "Methodology for the bulk collection of tweets containing key hashtags for the US Presidential Debates and generation of Networks for Analysis.",
    "author": [
      {
        "name": "Bryan Gertzel",
        "url": {}
      }
    ],
    "date": "2021-06-03",
    "categories": [
      "rstats",
      "twitter",
      "hashtags",
      "networks",
      "election",
      "debate"
    ],
    "contents": "\n\nContents\nCollection Strategy\nHashtags\nTimezones\n\nStreaming Collection\nSearch Collection\nFirst\nPresidential Debate Preliminary Results\nData Summary\nData Tweet Activity\n\nNetwork Analysis\nMerge Collected Data\nCreate Networks\nReply-network Giant\nComponent\n\n\nThe VOSON Lab undertook a number of Twitter collections on selected\nhashtags during the 2020 US Presidential debates and townhalls. The\nTwitter streaming API endpoint was used for a sampled real-time\ncollection of tweets during the debates, and the recent search API was\nused post-debate to collect all tweets containing hashtags that occurred\nover the debate telecast periods. The two approaches differ in that the\nstreaming API endpoint allows access to a “roughly\n1% random sample of publicly available Tweets in real-time” using\nprovided hashtags or terms as a filter to monitor events. The search API\nendpoint uses terms in a search query to match all tweets containing\nhashtags for a recent historical period in time. The streaming\ncollections, a filtered sample, produced much smaller datasets and was\nuseful for a timeley review of the Twitter activity during the debates\nas well as for identifying tweets to act as bounds for search\ncollections. The retrospective search collections were much larger and\nproduced more comprehensive datasets.\nThe R packages rtweet and vosonSML, the\nlatter of which wraps the functionality of rtweet for its\nTwitter capability, were used in a semi-automated way to collect data\nwith both streaming and search API’s. These packages use the standard Twitter API\nv1.1 endpoints.\nTwo US Presidential debates took place between Donald Trump and Joe\nBiden on September 29th and October 22nd, with one debate scheduled for\nOctober 15th cancelled due to COVID-19 concerns. One VP debate between\nKamala Harris and Mike Pence took place on October 7th. This article\nwill focus on the datasets collected for the first debate, widely\nreported as “pure\nchaos” and “90\nminutes of insult and interjection” by commentators, to demonstrate\nour collection methodology and some simple network analysis.\nFigure 1: Next day headlines, tweet from\nhttps://twitter.com/oliverdarcy. Embedded tweet sourced\nfrom Twitter.Collection Strategy\nThe US Presidential debates were all scheduled to run for 1.5 hours\nbetween 9pm and 10.30pm Eastern Daylight Time (EDT). To capture Twitter\ndiscussion surrounding the debate a four hour streaming window was\nchosen, with the collection starting at 8.30pm and concluding at 12.30am\nEDT (0.5 hours before and 2 hours after the debate). Streaming\ncollection was performed by multiple team members, each with slightly\nstaggered start times. This is because streaming collections were\ndivided into 15 minute segments, and offsetting allowed tweets to be\ncollected during connections, disconnections and segment changeover\nintervals. The tweets could then be merged and any duplicates removed in\npost-processing.\nBecause of the very likely large volume of tweets collected using the\nsearch API endpoint the collection window was reduced to 2 hours,\nstarting 15 minutes before and concluding 15 minutes after the debates.\nTwitter searches are historical and page backwards in time, results are\nreturned from most recent tweet as the first observation to the earliest\ntweet matching search criteria as last observation collected. There are\nalso limits to how many tweets can be collected as defined by the API\nrate-limit. Using a bearer\nauthentication token to perform searches allows for 45,000 tweets to\nbe retrieved every 15 minutes.\nAs tweet status ID’s are sequential numbers, with new tweets\nincrementing the identifier, they can be used to set bounds for\nsearches. Simply, the first tweet in the collected data from a search\n(earliest tweet in data) can be used as a starting point for subsequent\nsearches as we move backwards in time. This means that given two tweets,\none at the beginning and the other at the end of an event - the debate,\nwe can systematically and programatically collect all tweets in-between\nworking backwards from the end of event tweet. To identify which tweets\nto use as bounds we performed timestamp search of collected streaming\ntweets. We identified the highest status ID tweet matching our end time\nfor the collection window 10.45pm EDT (search start) and the lowest ID\nmatching our debate start time 8.45pm EDT (search end).\nIn practice, 45,000 tweets were collected with a pause for the 15\nminute rate-limit to reset, then a further 45,000 were collected with a\npause, and so on until the tweet that we identified as marking the\nbeginning of the debate window was collected. Because this process could\ntake many hours, it was important to perform the search collections\nwithin 7 days of the debates.\nHashtags\nA set of hashtags were selected in order to capture tweets related to\nthe presidential debates. The following 12 were used for streaming and\nsearch collections for all debates, with 4 addditonal hashtags for the\nvice presidential debate and 3 for the townhalls.\nTable 1: Debate hashtags\n\nHashtags\nHashtags for all events\n#PresidentialDebate, #PresidentialDebates, #Election2020,\n#Debates2020, #Debates, #DebateNight, #Biden, #Biden2020,\n#BidenHarris2020, #Trump, #Trump2020, #TrumpPence2020\nAdditional vice presidential debate\n#VPDebate, #VPDebate2020, #VPDebates, #VPDebates2020\nAdditional televised townhalls\n#BidenTownHall, #TrumpTownHall, #TownHall\nTimezones\nThe first US Presidential debate took place in Cleveland, Ohio at 9pm\nEastern Daylight Time (EDT) on the 29th September, 2020. Tweet\ntimestamps are all in Universal Coordinated Time (UTC), meaning times in\nthe data need to be offset by -4 to find the debate time in EDT. As\ncollection took place in Canberra, Australia or Australian Eastern\nStandard Time (AEST), the local system timestamps produced by scripts\nfor logging are in AEST. The first debate time and timezone conversions\ncan be seen in the table below.\nTable 2: First debate timezone reference\nTimezone\nStart time\nEnd time\nEDT\n2020-09-29 21:00\n2020-09-29 22:30\nUTC\n2020-09-30 01:00\n2020-09-30 02:30\nAEST\n2020-09-30 11:00\n2020-09-30 12:30\nStreaming Collection\nFor the streaming collection a directory was created to for easier\npost-processing. Streaming data was collected and written to file in\nJSON format using timestamp formatted file names. The streaming\ncollection period was set to 4 hours and divided into segments or\nfiles.\n\n\nShow code\n\nwd <- getwd()\n\n# check paths and create directories if they do not exist\ndata_path <- paste0(wd, \"/data\")\nif (!dir.exists(data_path)) { dir.create(data_path, showWarnings = FALSE) }\n\nstreams_path <- paste0(data_path, \"/pres-debate-streams\")\nif (!dir.exists(streams_path)) { dir.create(streams_path, showWarnings = FALSE) }\n\n# helper functions to write to log file and to create date time based file names\nlog <- function(msg, fn) { cat(msg, file = fn, append = TRUE, sep = \"\\n\") }\nfname <- function(path, ts) { paste0(path, \"/stream-\", gsub(\"[^[:digit:]_]\", \"\", ts)) }\n\n# set stream filter hashtags - comma seperated\nstream_filter <- paste0(\"#PresidentialDebate,#PresidentialDebates,#Election2020,\",\n                       \"#Debates2020,#Debates,#DebateNight,\",\n                       \"#Biden,#Biden2020,#BidenHarris2020,\",\n                       \"#Trump,#Trump2020,#TrumpPence2020\")\n\n# set the time period to collect tweets in seconds\nstream_period <- 4 * 60 * 60 # 4 hours or 14400 seconds\n\n# break up streaming collection into segments\nnum_segs <- 16 # each segment is 15 minutes\nseg_period <- ceiling(stream_period / num_segs)\n\n\n\nThe streaming collection is performed by the rtweet\nfunction stream_tweets which in our operation uses the\nquery or filter parameter q, a timeout period\nwhich is the length of time to collect streaming tweets, and an output\nJSON file_name. The collection is wrapped in a loop which\nis for the number of 15 minute segments in the collection period. Each\niteration sets up a new timestamped data file and log file.\n\n\nShow code\n\nlibrary(rtweet)\n\n# load rtweet auth token\ntoken <- readRDS(\"~/.rtweet_oauth1a\")\n\n# collect streaming tweets with a new file every 15 minutes\nfor (i in 1:num_segs) {\n\n  # create log file and JSON data file\n  timestamp <- Sys.time()\n  log_file <- paste0(fname(streams_path, timestamp), \".txt\")\n  json_file <- paste0(fname(streams_path, timestamp), \".json\")\n  \n  log(paste0(\"timestamp: \", timestamp, \"\\ntimeout: \",\n             seg_period, \" secs\\nfilter: \", stream_filter),\n    log_file)\n  \n  # collect streaming tweets and write to JSON file\n  tryCatch({\n    rtweet::stream_tweets(\n      token = token,\n      q = stream_filter,\n      timeout = seg_period,\n      file_name = json_file,\n      parse = FALSE\n    )\n  }, error = function(e) {\n    cat(paste0(e, \"\\n\"))\n    log(paste0(\"error: \", e), log_file)\n  })\n  \n  log(paste0(\"completed: \", Sys.time()), log_file)\n}\n\n\n\nLog entries for each 15 minute iteration confirm the collection\nperiod and each file matches a JSON data file (note timestamps are in\nlocal time which was AEST).\n# data/pres-debate-streams/stream-20200930102859.txt \n\ntimestamp: 2020-09-30 10:28:59\ntimeout: 900 secs\nquery: #PresidentialDebate,#PresidentialDebates,#Election2020,\n#Debates2020,#Debates,#DebateNight,#Biden,#Biden2020,#BidenHarris2020,\n#Trump,#Trump2020,#TrumpPence2020\ncompleted: 2020-09-30 10:43:59\nFor the first streaming collection iteration an\n180MB JSON file was written with 111,244\nlines. Each line contains the JSON for a single tweet, meaning\nthe same number of tweets were collected.\n\n> first_json_file <- \"./data/pres-debate-streams/stream-20200930102859.json\"\n> file.size(first_json_file)\n[1] 188575977 # 180MB\n\n> length(readLines(first_json_file))\n[1] 111244\n\n16 JSON data files were written, corresponding to\nthe number of 15 minute segments specified. These were then individually\nprocessed and converted to dataframes, which were then merged into a\nsingle complete streaming collection dataframe for the first debate.\nSearch Collection\nAs with the streaming collection directories were created to store\ncollected data and search parameters set. The search query containing\nhashtags uses the Twitter OR search operator unlike the\nstreaming filter which was comma seperated. A maximum number of tweets\nfor each collection iteration as well as a maximum number of iterations\nare set. The number of tweets is required for the search request and is\nset to the maximum rate-limit value for a bearer token. A maximum number\nof iterations is set as a precaution to prevent infinite collection\nshould a problem arise. The two tweets found from the streaming\ncollection and used as search bounds are also set. The search will start\nat the latest id and continue until the\nearliest id is found, or the maximum iterations has been\nreached.\n\n\nShow code\n\nwd <- getwd()\n\n# check paths and create directories if they do not exist\ndata_path <- paste0(wd, \"/data\")\nif (!dir.exists(data_path)) { dir.create(data_path, showWarnings = FALSE) }\n\nsearches_path <- paste0(data_path, \"/pres-debate-searches\")\nif (!dir.exists(searches_path)) { dir.create(searches_path, showWarnings = FALSE) }\n\n# set search query hashtags - separated with OR search operator\nq <- paste0(\"#PresidentialDebate OR #PresidentialDebates \",\n            \"OR #Election2020 OR \",\n            \"#Debates2020 OR #Debates OR #DebateNight OR \",\n            \"#Biden OR #Biden2020 OR #BidenHarris2020 OR \",\n            \"#Trump OR #Trump2020 OR #TrumpPence2020\")\n\ntype <- \"recent\"\nnum_tweets <- 45000\nmax_iter <- 40\n\n# pres debate 1 search\nlatest_id <- \"1311121700394807296\"    # start tweet\nearliest_id <- \"1311104723978579968\"  # end tweet\n\n\n\nThe search collection is performed by the vosonSML\nfunction Collect. The process is more involved than the\nstreaming collection in that the reset time for the rate-limit is\ncalculated each collection iteration and the script sleeps for that\nperiod of time before continuing. Tracking of search progress is also\nlogged to the console in this approach but was redirected to a log\nfile.\n\n\nShow code\n\nlibrary(vosonSML)\n\nauth <- readRDS(\"~/.vsml_oauth2\")\n\ncat(\"large twitter search\\n\")\ncat(paste0(\"type: \", type, \"\\n\"))\ncat(paste0(\"tweets per iter: \", num_tweets, \"\\n\"))\ncat(paste0(\"max iter: \", max_iter, \" (\", (max_iter * num_tweets), \" tweets)\\n\\n\"))\n\ni <- 1\nwhile (i <= max_iter) {\n  cat(paste0(\"iteration \", i, \" of \", max_iter, \"\\n\"))\n  cat(paste0(\"time: \", Sys.time(), \"\\n\"))\n  cat(paste0(\"set max_id: \", latest_id, \"\\n\"))\n  req_time <- as.numeric(Sys.time())\n  reset_time <- req_time + (15 * 60) + 10 # add 10 sec buffer\n  \n  code_wd <- getwd()\n  setwd(searches_path)\n  \n  data <- tryCatch({\n    auth %>%\n      Collect(searchTerm = q,\n              searchType = type,\n              numTweets = num_tweets,\n              max_id = latest_id,\n              verbose = TRUE,\n              includeRetweets = TRUE,\n              retryOnRateLimit = TRUE,\n              writeToFile = TRUE)\n  }, error = function(e) {\n    cat(paste0(e, \"\\n\"))\n    NULL\n  })\n  \n  setwd(code_wd)\n  \n  if (!is.null(data) && nrow(data)) {\n    data_first_obvs_id <- data$status_id[1]\n    data_last_obvs_id <- data$status_id[nrow(data)]\n    cat(paste0(\"data nrows = \", nrow(data), \"\\n\",\n               \"first row status id = \", data_first_obvs_id, \"\\n\",\n               \"last row status id = \", data_last_obvs_id, \"\\n\"))\n    \n    # set latest id to lowest status id in data for NEXT iteration\n    # this is typically the last observation\n    latest_id <- data_last_obvs_id\n    \n    # if our target id is passed then stop\n    if (earliest_id >= latest_id) {\n      cat(\"earliest id reached\\n\")\n      break\n    }\n    now_time <- as.numeric(Sys.time())\n    if (i < max_iter) {\n      sleep_time <- reset_time - now_time\n      if (sleep_time > 0) {\n        cat(\"sleeping \", sleep_time, \" secs\\n\")\n        Sys.sleep(sleep_time)  \n      }      \n    }\n  } else {\n    cat(\"no data\\n\")\n    break\n  }\n  i <- i + 1\n}\n\ncat(paste0(\"completed: \", Sys.time(), \"\\n\"))\n\n\n\nThe first search collection iteration collected a full 45,000\ntweets and wrote an R dataframe object to an\nRDS file. The vosonSML output also indicates\nthe minimum and maximum tweet status ID in the data and\ntheir timestamp (UTC) to assist with tracking the collection progress,\nit shows that the first 45,000 tweets were all created within an\napproximate 2.25 min period. It took 5.5 mins for the\nfirst collection to complete, and it slept for over 9.5 mins while the\nrate-limit reset before iteration 2. Timestamps other than the tweet\ncreation time are in local time AEST.\nlarge twitter search\ntype: recent\ntweets per iter: 45000\nmax iter: 40 (1800000 tweets)\n\niteration 1 of 40\ntime: 2020-10-03 08:50:31\nset max_id: 1311134926167834628\nCollecting tweets for search query...\nSearch term: #PresidentialDebate OR #PresidentialDebates OR\n#Election2020 OR #Debates2020 OR #Debates OR #DebateNight OR\n#Biden OR #Biden2020 OR #BidenHarris2020 OR #Trump OR #Trump2020\nOR #TrumpPence2020\nRequested 45000 tweets of 45000 in this search rate limit.\nRate limit reset: 2020-10-03 09:05:32\nDownloading [=========================================] 100%\n\ntweet  | status_id           | created             | screen_name   \n-------------------------------------------------------------------\nMin ID | 1311134367985565697 | 2020-09-30 02:42:47 | @pxxxxx_xx   \nMax ID | 1311134926167834628 | 2020-09-30 02:45:00 | @ixxxxxxxxxxx\nCollected 45000 tweets.\nRDS file written: ./data/pres-debate-searches/2020-10-03_085550-TwitterData.rds\nDone.\nElapsed time: 0 hrs 5 mins 22 secs (321.79)\ndata nrows = 45000\nfirst row status id = 1311134926167834628\nlast row status id = 1311134367985565697\nsleeping  588.2078  secs\n54 RDS data files containing Twitter collection\ndataframes were written and the search took approximately 17.5\nhours. These files were then merged into a single complete\nsearch collection dataframe for the first debate.\nFirst\nPresidential Debate Preliminary Results\nData Summary\nData was collected by multiple team members, presented are the\nun-merged results from a single members streaming and search collections\nfor the first presidential debate.\nTable 2: Collection summary\nTwitter API endpoint\nStart time (EDT)\nEnd time (EDT)\nPeriod (hours)\nObservations (unique tweets)\nStreaming\n2020-09-29 20:30\n2020-09-30 00:30\n4.00\n449,102\nSearch\n2020-09-29 20:45\n2020-09-29 22:45\n2.00\n2,387,587\nData Tweet Activity\nTime series plots for the streaming and search collections were\ncreated to indicate tweet activity over time. Observations are grouped\nby tweet type and into 5 minute bins. Perhaps unsurprisingly, retweet\nactivity appears to became more prevalent as the first debate\nprogressed. At around 10.20pm in the search collection\njust over 100,000 retweets were collected.\nFigure 2: Streaming collection time\nseries plotFigure 3: Search collection time series\nplotNetwork Analysis\nUsing vosonSML the Twitter data for both streaming and\nsearch collections is able to be converted into networks in the same\nway. The following code will demonstrate the merging of search\ncollection data, and creation of an activity and\nactor network for the first debate.\nMerge Collected Data\n\n\nlibrary(dplyr)\n\n# combine the search data files and remove duplicate tweets\n\n# get all of the files to combine\nfiles <- list.files(path = \"./data/pres-debate-searches/\",\n                    pattern = \".+TwitterData\\\\.rds$\", full.names = TRUE)\n\n# merge dataframes\ncomb_data <- bind_rows(lapply(files, function(x) { readRDS(x) }))\ndata <- comb_data %>% arrange(desc(status_id))\n\n# find and remove any duplicates\ndupes <- data %>% filter(status_id %in% data$status_id[duplicated(data$status_id)])\nif (nrow(dupes)) { data <- data %>% distinct(status_id, .keep_all = TRUE) }\n\nsaveRDS(data, \"./data/pres_debate1_search.rds\") # save combined data\n\n\n\nCreate Networks\nBecause the combined search data for the first debate is quite large,\nit can be useful to reduce it to a much smaller window of time for\ndemonstration and network visualization purposes. The following code\nwill extract 15 minutes of tweet data from between 21:30 - 21:45\nEDT.\n\ndata <- readRDS(\"./data/pres_debate1_search.rds\") # load the previously saved data\n\n# filter out tweets with creation timestamps outside of the 15 min window\ndata <- data %>% filter(created_at >= as.POSIXct(\"2020-09-30 01:30:00\", tz = \"UTC\") &\n                        created_at <= as.POSIXct(\"2020-09-30 01:45:00\", tz = \"UTC\"))\n\n> nrow(data)\n[1] 349937\n> min(data$created_at)\n[1] \"2020-09-30 01:30:00 UTC\"\n> max(data$created_at)\n[1] \"2020-09-30 01:45:00 UTC\"\n\nThe data is now comprised of 349,937 unique tweets\nthat all were created during our specified window. We can now create our\nnetworks using vosonSML.\n\nlibrary(vosonSML)\n\n# use the vosonsml create function to create networks\n\n# activity network\n> net_activity <- data %>% Create(\"activity2\")\nGenerating twitter activity network...\n-------------------------\ncollected tweets | 349937\ntweet            | 125626\nretweet          | 212842\nreply            | 7316\nquote            | 4210\nnodes            | 366029\nedges            | 349994\n-------------------------\nDone.\n\n# actor network\n> net_actor <- data %>% Create(\"actor2\")\nGenerating twitter actor network...\n-------------------------\ncollected tweets | 349937\ntweet mention    | 15021\ntweet            | 125626\nretweet          | 212842\nreply mention    | 2202 \nreply            | 7316 \nquote mention    | 734  \nquote            | 4210 \nnodes            | 202333\nedges            | 367951\n-------------------------\nDone.\n\nThe activity network has 366,029 nodes\nor unique tweets, and the actor network\nhas 202,333 nodes or unique actors for\nour 15 minute window.\nReply-network Giant\nComponent\nIf we’re interested in exploring some of the Twitter interactions\ntaking place between users during the 21:30 - 21:45 window of the first\ndebate, the network can be further distilled by looking at the actor\nnetwork and including only reply edges. This will reveal a number of\nreply-conversations, but we can select for the giant component to find\nthe largest one. The igraph library can be used to perform\na number of common network operations such as removing self-loops,\nisolates and finding the giant component.\n\nlibrary(igraph)\n\n# convert vosonsml actor network to igraph object\n> g_actor <- net_actor %>% Graph()\nCreating igraph network graph...Done.\n\n# remove edges that are not replies\n# remove self-loops and isolates\ng_actor_reply <- g_actor %>% delete_edges(E(g_actor)[E(g_actor)$edge_type != \"reply\"])\ng_actor_reply <- g_actor_reply %>% simplify(remove.multiple = FALSE)\ng_actor_reply <- g_actor_reply %>%\n  delete_vertices(V(g_actor_reply)[which(degree(g_actor_reply) == 0)])\n\n# find the giant component\ncomps <- clusters(g_actor_reply, mode = \"weak\")\nlargest_cluster_id <- which.max(comps$csize)\nnode_ids <- V(g_actor_reply)[comps$membership == largest_cluster_id]\ng_actor_reply_gc <- induced_subgraph(g_actor_reply, node_ids)\n\nThe giant component in the reply-network has 1743\nnodes and 1982 edges. We can use\nigraphs degree function to further explore who\nthe most prominent actors are in the network by in-degree.\n\n> V(g_actor_reply_gc)$screen_name[order(degree(g_actor_reply_gc, mode = \"in\"), decreasing = TRUE)]\n[1] \"realDonaldTrump\"    \"JoeBiden\"           \"GOPLeader\"         \n[4] \"KamalaHarris\"       \"ProjectLincoln\"     \"Alyssa_Milano\"\n\nTo visualise the conversation occurring during this 15 minutes of the\ndebate, we first removed the Twitter accounts for the two candidates\n(\"realDonaldTrump\", \"JoeBiden\",\n\"GOPLeader\"), and then constructed a new giant component\nfrom the reply network, which now has 1345 nodes and\n1484 edges.\nThe visualization of this network (using Gephi) with\nnode size proportional to in-degree is below.\nFigure 4: Reply-network giant component\nwith candidates removed. Figure created by Robert Ackland using\nGephi.This graph provides some insight into the largest reply network at\nour chosen point in time, revealing the Twitter actors receiving the\nmost reply attention and their associations.\n\n\n\n",
    "preview": "posts/2021-06-03-us-presidential-debates-2020-twitter-collection/debate_preview.png",
    "last_modified": "2022-08-04T14:14:49+10:00",
    "input_file": {},
    "preview_width": 1024,
    "preview_height": 1024
  },
  {
    "path": "posts/2021-03-23-twitter-conversation-networks/",
    "title": "Twitter Conversation Networks",
    "description": "Getting started with the voson.tcn package.",
    "author": [
      {
        "name": "Bryan Gertzel",
        "url": {}
      }
    ],
    "date": "2021-03-23",
    "categories": [
      "rstats",
      "twitter",
      "conversations",
      "voson.tcn",
      "networks"
    ],
    "contents": "\n\nContents\nTwitter Developer\nAccess\nInstallation\nAuthentication\nCollection\nNetwork Creation\nActivity Network\nActor Network\n\nPlot Graphs\nActivity Network\nActor Network\n\n\n The VOSON Lab has recently published to GitHub a new\nopen source R package called voson.tcn.\nThe package uses the Early-Access Twitter API v2, to collect tweets\nbelonging to specified threaded conversations and generate networks. The\nTwitter API v2 provides a new tweet identifier: the conversation\nID, that is common to all tweets that are part of a conversation,\nand can be searched for using the API search endpoints. Identifiers and\nassociated metadata for referenced tweets can also be collected in the\nsame search for conversation tweets, allowing us to construct twitter\nnetworks with tweet and user metadata whilst minimising API\nrequests.\nTwitter Developer Access\nThe voson.tcn package requires developer app\nauthentication keys or tokens to access the Twitter API v2. These can be\neither the Access token & secret of an app or its\nBearer token.\nTo obtain these credentials and use the early-access API you will\nneed to have or apply for a Twitter\nDeveloper Account, as well as have activated the new Developer\nPortal. Once approved you will then need to create a development project,\nwhich is the new management container for apps, and either create a new\napp or\nassociate one of your existing apps with it.\nThere are currently two project types available that correspond to\nTwitter’s developer product\ntracks, a standard and academic type.\nAcademic projects are only available to researchers who have\ncompleted and have had their application\nfor the academic research track approved for non-commercial research\npurposes. Standard projects are for more general use, including\nhobby and educational purposes. The project type features differ in\ntheir API access and caps; standard projects can only access\nthe 7-day recent search endpoint whereas an academic project\ncan access the full-archive\nsearch endpoint for historical tweets. There are also rate-limits\nand monthly tweet caps for API v2 search endpoints. At the time of\nwriting, the caps are 500k and 10 million tweets that can be retrieved\nper month for the standard and academic track projects\nrespectively.\nPlease note that there are also terms\nof use and restricted use cases that should be considered before\napplying for access or using the Twitter API.\nInstallation\nThe voson.tcn R package is in development and currently\nonly available on GitHub. It can be\ninstalled as follows:\n\n\n# use the remotes package to install the latest dev version of voson.tcn from github\nlibrary(remotes)\ninstall_github(\"vosonlab/voson.tcn\")\n\n# Downloading GitHub repo vosonlab/voson.tcn@HEAD\n# √  checking for file\n# -  preparing 'voson.tcn':\n# √  checking DESCRIPTION meta-information ... \n# -  checking for LF line-endings in source and make files and shell scripts\n# -  checking for empty or unneeded directories\n# -  building 'voson.tcn_0.1.6.9000.tar.gz'\n#    \n# * installing *source* package 'voson.tcn' ...\n# ...\n# * DONE (voson.tcn)\n# Making 'packages.html' ... done\n\n\n\nAuthentication\nThe voson.tcn package only supports app based\nauthentication using OAuth2.0 tokens which are also known\nas bearer\ntokens. We will likely support user based tokens in the future,\nhowever at this stage they do not offer any advantages as they have\nlower rate-limits and we are not using any private metadata of which\nthey permit access (such as user-visible only metrics).\nThe token can be created using either your apps\naccess token & secret (also known as consumer keys) or\nits bearer token. It is recommended that this token is\nsaved for future use; there is no need to perform this step more than\nonce as the token will not change unless it is invalidated or you\nregenerate keys on the developer portal.\n\n\nlibrary(voson.tcn)\n\n# retrieves a bearer token from the API using the apps consumer keys\ntoken <- tcn_token(consumer_key = \"xxxxxxxx\",\n                   consumer_secret = \"xxxxxxxx\")\n\n# alternatively if you have a bearer token already you can assign it directly\ntoken <- list(bearer = \"xxxxxxxxxxxx\")\n\n# if you save the token to file this step only needs to be done once\nsaveRDS(token, \"~/.tcn_token\")\n\n\n\nCollection\nCollecting conversation tweets requires the tweet ID or URL of a\ntweet that belongs to each threaded conversation that you are interested\nin. These are passed to the voson.tcn collect function\ntcn_threads as a vector or list. Conversation IDs will be\ntracked by this function to avoid duplication and, if tweet IDs are\nfound to belong to a conversation that has already been collected on,\nthen that conversation will be skipped.\nIn the following example, we are collecting the tweets for a threaded\nconversation belonging to a public lockdown announcement following a\nCOVID-19 outbreak in Brisbane, Queensland, Australia, that took place on\nMarch, 29, 2021. The tweet URL or ID (number following the status in the\nURL) can be passed directly to the collection function.\nFigure 1: Public announcement tweet\nregarding a COVID-19 lockdown of Brisbane, from the Queensland Premier.\nEmbedded tweet sourced from Twitter.\n\n# read token from file\ntoken <- readRDS(\"~/.tcn_token\")\n\n# collect the conversation thread tweets for supplied ids           \ntweets <- tcn_threads(\"https://twitter.com/AnnastaciaMP/status/1376311897624956929\", token)\n\n\n\nWhen completed, a list of named dataframes will be returned, with\ntweets containing all of the tweets and their metadata, and\nusers containing all of the referenced users in the tweets\nand their metadata. In our example, 286 tweets were collected with 180\nassociated users public metadata.\nNote that the collection of a threaded tweet conversation is a\nsnapshot of the state of the conversation at a point in time. Metrics\nand networks produced from our data will not completely match subsequent\ncollections of the same conversation, as it will have likely\ncumulatively expanded over time.\n\n\n# collected tweets\nprint(tweets$tweets, n = 3)\n# # A tibble: 286 x 14\n#   in_reply_to_user~ conversation_id  source  author_id  tweet_id  ref_tweet_type\n#   <chr>             <chr>            <chr>   <chr>      <chr>     <chr>         \n# 1 15999~            137631189762495~ Twitte~ 134852208~ 13763373~ replied_to    \n# 2 1142316897985163~ 137631189762495~ Twitte~ 126908387~ 13763373~ replied_to    \n# 3 25683~            137631189762495~ Twitte~ 137503906~ 13763371~ replied_to    \n# # ... with 283 more rows, and 8 more variables: ref_tweet_id <chr>, text <chr>,\n# #   created_at <chr>, includes <chr>, public_metrics.retweet_count <int>,\n# #   public_metrics.reply_count <int>, public_metrics.like_count <int>,\n# #   public_metrics.quote_count <int>\n\n# users metadata\nprint(tweets$users, n = 3)\n# # A tibble: 180 x 12\n#   profile.username profile.created_~ profile.profile_~ user_id profile.descript~\n#   <chr>            <chr>             <chr>             <chr>   <chr>            \n# 1 MSMW~            2013-03-30T06:48~ https://pbs.twim~ 131592~ \"Fact checking i~\n# 2 bpro~            2012-12-04T02:07~ https://pbs.twim~ 987844~ \"Only way to get~\n# 3 scre~            2009-10-22T22:56~ https://pbs.twim~ 844463~ \"I'm a  creative~\n# # ... with 177 more rows, and 7 more variables: profile.name <chr>,\n# #   profile.verified <lgl>, profile.location <chr>,\n# #   profile.public_metrics.followers_count <int>,\n# #   profile.public_metrics.following_count <int>,\n# #   profile.public_metrics.tweet_count <int>,\n# #   profile.public_metrics.listed_count <int>\n\n\n\nIf interested in text analysis, the tweet text can be found in the\ntext column of the tweets dataframe and user\nprofile descriptions in profile.description of the\nusers dataframe.\nPublic metrics for tweets and users are\nfound in dataframe columns prefixed, with public_metrics\nand profile.public_metrics respectively.\n\n\nlibrary(dplyr)\n\nnames(select(tweets$tweets, starts_with(\"public_metrics\")))\n# [1] \"public_metrics.retweet_count\" \"public_metrics.reply_count\"\n# [3] \"public_metrics.like_count\" \"public_metrics.quote_count\"\n\nnames(select(tweets$users, starts_with(\"profile.public_metrics\")))\n# [1] \"profile.public_metrics.followers_count\"\n# [2] \"profile.public_metrics.following_count\"\n# [3] \"profile.public_metrics.tweet_count\"\n# [4] \"profile.public_metrics.listed_count\"\n\n\n\nNetwork Creation\nThere are two types of networks that can be generated using\nvoson.tcn: activity and actor\nnetwork. These differ by the type of node and resulting structure of the\nnetworks.\nActivity Network\nAn activity network is a representation of the\nconversation as seen on Twitter: nodes are tweets and the edges are how\nthey are related. Tweets (or nodes) are identified by their unique\nidentifier Tweet ID (formerly Status ID). In a\nTwitter threaded conversation, there are only two types of connections\nor edges between tweets and these are replied_to and\nquoted.\nReplies are made when a user chooses the reply option and\npublishes a tweet response to the tweet they are replying to. Quotes are\na little different in that the user has included a link to or\nquoted another tweet in the body of their tweet. In Twitter\nconversation networks, it is common to quote a tweet as part of\na reply tweet, generating in the activity network a\nreplied_to and quoted edge from the same\nnode.\n\n\n# generate an activity network\nactivity_net <- tcn_network(tweets, \"activity\")\n\n# number of nodes\nnrow(activity_net$nodes)\n# [1] 279\n\n# number of edges\nprint(activity_net$edges, n = 3)\n# # A tibble: 281 x 3\n#   from                to                  type      \n#   <chr>               <chr>               <chr>     \n# 1 1376337359126495232 1376328523518898176 replied_to\n# 2 1376337350163267584 1376325216658317315 replied_to\n# 3 1376337128016113665 1376311897624956929 replied_to\n# # ... with 278 more rows\n\nunique(activity_net$edges$type)\n# [1] \"replied_to\" \"quoted\"\n\n\n\nActor Network\nAn actor network represents the interactions between\nTwitter users in the conversation: nodes are the users and edges are\ntheir connections. As in the activity network, edges are\neither a reply or a quote but edges represent\nthe classification of a tweet connecting users rather than the activity.\nUsers (or nodes) are identified by their unique Twitter\nUser ID. In the actor network, interactions\nbetween users are more apparent and can be measured by the frequency\n(and direction) of edges between them.\n\n\n# generate an actor network\nactor_net <- tcn_network(tweets, \"actor\")\n\n# number of nodes or actors\nnrow(actor_net$nodes)\n# [1] 180\n\nprint(actor_net$edges, n = 3)\n# # A tibble: 286 x 6\n#   from      to        type  tweet_id     created_at    text                     \n#   <chr>     <chr>     <chr> <chr>        <chr>         <chr>                    \n# 1 13485220~ 15999128~ reply 13763373591~ 2021-03-29T0~ \"@Ther~ @Scott~\n# 2 12690838~ 11423168~ reply 13763373501~ 2021-03-29T0~ \"@Luke~ @Annas~\n# 3 13750390~ 25683344~ reply 13763371280~ 2021-03-29T0~ \"@AnnastaciaMP You do un~\n# # ... with 283 more rows\n\nunique(actor_net$edges$type)\n# [1] \"reply\" \"quote\" \"tweet\"\n\n\n\nNote that in the actor network there is an additional\nedge type: tweet, which is assigned to a self-loop edge\ncreated for the thread’s initial tweet. This is a technique used to\nretain the initial tweet’s metadata as edge attributes comparable to\nother edges in the network.\nThe initial conversation tweet would not usually be included in the\nedge list, as the initial conversation tweet is not directed at another\nuser, and hence no edge to attach metadata.For example, this allows the\ntext of the initial tweet to be included in any actor network tweet text\nanalysis. It would not usually be included in the edge list as the\ninitial conversation tweet is not directed at another user, and hence no\nedge to attach metadata is naturally found in this type of network.\nPlot Graphs\nActivity Network\nVisualisation of the activity network produced with\nigraph.\n\n\nlibrary(igraph)\nlibrary(RColorBrewer)\n\ng <- graph_from_data_frame(activity_net$edges, vertices = activity_net$nodes)\n\n\n\n\n\nShow code\n\n# change likes to log scale\nlike_count <- V(g)$public_metrics.like_count\nlike_count[is.na(like_count)] <- 0\nln_like_count <- log(like_count)\nln_like_count[!is.finite(ln_like_count)] <- 0\n\n# set node size based on likes, min size 4\nsize <- ln_like_count * 2\nV(g)$size <- ifelse(size > 0, size + 8, 4)\n\n# set node label if number of likes >= 2\nV(g)$label <- ifelse(like_count >= 2, like_count, NA)\nV(g)$label.color <- \"black\"\n\n# set node colors based on number of retweets low to high is yellow to green\n# set tweets with no retweets to grey\nrt_count <- V(g)$public_metrics.retweet_count\nrt_count[is.na(rt_count)] <- 0\ncols <- colorRampPalette(c(\"yellow1\", \"green3\"))\ncols <- cols(max(rt_count) + 1)\nV(g)$color <- cols[rt_count + 1]\nV(g)$color[which(rt_count < 1)] <- \"lightgrey\"\n\n# set edge color to orange if tweet quoted another tweet\nE(g)$color <- ifelse(E(g)$type == \"quoted\", \"orange\", \"grey\")\n\n\n\n\n\n# plot the graph using fruchterman reingold layout\nset.seed(200)\ntkplot(g,\n       canvas.width = 1024, canvas.height = 1024,\n       layout = layout_with_fr(g),\n       edge.arrow.size = 0.5,\n       edge.width = 2)\n\n\n\nFigure 2: Conversation activity network -\nNode size and label represent number of tweet likes, color scale is\nindicating low to high number of retweets (yellow to green). Orange\ncoloured edges are quoting linked tweet.voson.tcn collects tweets that are all linked to each\nother via the conversation ID. This means that in a network\ngenerated from this data, such as the activity network, all of the nodes\n(tweets) should be connected in a single component per\nconversation ID. If multiple conversation IDs\nwere collected on then, it is also possible to have one component\nbecause of quote edges. These edges joining conversations occur\nwhen a tweet in one conversation has quoted a tweet in another that you\nhave collected.\nIn the example activity network above, there are two components even\nthough only one conversation ID was collected on. Multiple\ncomponents are usually due to a missing conversation tweet not able to\nbe retrieved from the API and producing a broken reply chain. This can\noften occur if, for example, a tweet has been deleted, or the tweet or\nuser flagged or suspended in some way restricting public\navailability.\nActor Network\nVisualisation of the actor network produced with\nigraph.\n\n\nlibrary(dplyr)\nlibrary(magrittr)\nlibrary(stringr)\n\nregex_ic <- function(x) regex(x, ignore_case = TRUE)\n\n# best effort set the node colour attribute based on presence of city, state,\n# or country in the actors profile location field\n# value assigned from first match\nnodes <- actor_net$nodes %>%\n  mutate(color = case_when(\n    str_detect(profile.location, regex_ic(\"brisbane|bris\")) ~ \"orange\",\n    str_detect(profile.location, regex_ic(\"queensland|qld\")) ~ \"gold\",\n    str_detect(profile.location, regex_ic(\"australia|oz\")) ~ \"yellow\",\n    TRUE ~ \"lightgrey\"))\n\ng2 <- graph_from_data_frame(actor_net$edges, vertices = nodes)\n\n\n\n\n\nShow code\n\n# the following code de-clutters the actor network by removing some nodes\n# that are not part of conversation chains and are stand-alone replies to\n# the initial thread tweet\n\n# get the author of the initial thread tweet using the conversation id\nconversation_ids <- c(\"1376311897624956929\")\nthread_authors <- activity_net$nodes %>%\n  filter(tweet_id %in% conversation_ids) %>% select(user_id)\n\n# remove actors replying to the initial tweet that have a degree of 1\nthread_spokes <- unlist(\n  incident_edges(g2, V(g2)[which(V(g2)$name %in% thread_authors$user_id)],\n                 \"in\"))\nspokes_tail_nodes <- V(g2)[tail_of(g2, thread_spokes)]$name\ng2 <- delete_vertices(g2, degree(g2) == 1 & V(g2)$name %in% spokes_tail_nodes)\n\n# convert the graph to undirected\n# simplify the graph and collapse edges into an edge weight value\nE(g2)$weight <- 1\ng2 <- as.undirected(simplify(g2, edge.attr.comb = list(weight = \"sum\")))\ng2 <- delete_vertices(g2, degree(g2) == 0)\n\n# use edge weight for graph edge width\nE(g2)$width <- ifelse(E(g2)$weight > 1, E(g2)$weight + 1, 1)\n\n# use the actors followers count for node size \nfollowers_count <- log(V(g2)$profile.public_metrics.followers_count)\nfollowers_count[!is.finite(followers_count)] <- 0\nsize <- followers_count * 3\nV(g2)$size <- ifelse(size < 6, 6, size)\nV(g2)$label <- ifelse(followers_count > 0,\n                      V(g2)$profile.public_metrics.followers_count, NA)\n\n\n\n\n\n# plot the graph using automatically chosen layout\nset.seed(201)\ntkplot(g2,\n       canvas.width = 1024, canvas.height = 1024,\n       layout = layout_nicely(g2),\n       vertex.label.cex = 0.8,\n       vertex.label.color = \"black\")\n\n\n\nFigure 3: Conversation actor network -\nNode size and label represent users follower counts. Node color\nindicates user self-reported location. Edge width represents number of\ncollapsed edges.\n\n\n",
    "preview": "posts/2021-03-23-twitter-conversation-networks/activity_network.png",
    "last_modified": "2022-08-04T14:14:49+10:00",
    "input_file": {},
    "preview_width": 1024,
    "preview_height": 1025
  },
  {
    "path": "posts/2021-03-15-hyperlink-networks-with-vosonsml/",
    "title": "Hyperlink Networks with vosonSML",
    "description": "An introduction to creating hyperlink networks with vosonSML.",
    "author": [
      {
        "name": "Bryan Gertzel",
        "url": {}
      }
    ],
    "date": "2021-03-15",
    "categories": [
      "rstats",
      "hyperlinks",
      "vosonSML",
      "networks"
    ],
    "contents": "\n\nContents\nIntroduction\nInstallation\n\nHyperlink Collection\nSetting Up\nPerforming the\nCollection\n\nNetwork Creation\nNetworks\nPlot a Graph\n\n\nThe VOSON software for\nhyperlink collection and analysis was an early research output of the\nVOSON Lab (Ackland 2010).\nIt addressed a need for tools that could help study online social\nnetworks, even before the rise of social media, and assisted researchers\ngain insights into important phenomena such as networks around issue\nspheres and online social movements [see (Ackland and O’Neil 2011) and (Ackland 2013)]. After many years and many\niterations since its inception in 2004, the VOSON Lab is happy to\nreintroduce the canonical VOSON hyperlink collection software as part of\nour R open-source toolkit for social media collection:\nvosonSML.1\nThis simple guide will demonstrate how to use the new features of the\nvosonSML package to perform a hyperlink collection and\ngenerate networks for analysis.\nIntroduction\nThe vosonSML hyperlink collection and network creation\nworks similarly to the 3-step process we use with other social media\nsources: the Authenticate, Collect and\nCreate verb functions. The Authenticate\nfunction is first called with the parameter “web” to identify and set up\nthe context for subsequent operations, but it does not require any\nfurther credentials in this implementation. vosonSML uses\nstandard web crawling and text-based page scraping techniques to\ndiscover hyperlinks and, as such, there is no need to access any\nrestricted data API’s as we commonly do with social media.\nInstallation\nThe new hyperlink collection and network features are currently\navailable in the development version of vosonSML on GitHub, and\nare to soon be released on CRAN. The development version can be\ninstalled as follows:\n\n\n# use the remotes package to install the latest dev version of vosonSML from github\nlibrary(remotes)\ninstall_github(\"vosonlab/vosonsml\")\n\n# Downloading GitHub repo vosonlab/vosonsml@HEAD\n# √  checking for file\n# -  preparing 'vosonSML':\n# √  checking DESCRIPTION meta-information ... \n# -  checking for LF line-endings in source and make files and shell scripts\n# -  checking for empty or unneeded directories\n# -  building 'vosonSML_0.30.00.9000.tar.gz'\n#    \n# * installing *source* package 'vosonSML' ...\n# ...\n# * DONE (vosonSML)\n# Making 'packages.html' ... done\n\n\n\nHyperlink Collection\nSetting Up\nThe web sites or pages to collect hyperlinks from are specified and\ninput to the Collect function in a dataframe. As there are\npage specific options that can be used, this format helps us to organise\nand set the request parameters. The URL’s set in the dataframe for the\npage column are called ‘seed pages’ and are the starting\npoints for web crawling. Although not explicitly indicated in the URL’s,\nthe seed pages are actually the landing pages or “index” pages of the\nweb sites and a page name can be specified if known or desired.\n\n\n# set sites as seed pages and set each for external crawl with a max depth\npages <- data.frame(page = c(\"http://vosonlab.net\",\n                             \"https://www.oii.ox.ac.uk\",\n                             \"https://sonic.northwestern.edu\"),\n                    type = c(\"ext\", \"ext\", \"ext\"),\n                    max_depth = c(2, 2, 2))\n\n\n\nThe example above shows seed pages with some additional per-seed\nparameters that are used to control the web crawling. The\ntype parameter can be set to a value of either\nint, ext or all, which correspond\nto following only internal, external or following all hyperlinks found\non a seeded web page and subsequent pages discovered from that\nparticular seed. How a hyperlink is classified is determined by the seed\ndomain name, for example, if the seed page is\nhttps://vosonlab.net a type of ext will follow\nhyperlinks from that page that do not have a domain name of\n“vosonlab.net”. A type of int will follow only hyperlinks\nthat match a domain of “vosonlab.net”, and a type of all\nwill follow all hyperlinks found irrespective of their domain. The final\nparameter max_depth refers to how many levels of pages to\nfollow from the seed page. In the diagram below, the green dots are\npages scraped by the web crawler and the blue dots links are the\nhyperlinks collected from them for a max depth of 1,2 and 3.\nFigure 1: Scope of hyperlinks collected\nusing the max_depth parameterAs can be seen, a max depth of 1 directs the crawler to scrape and\ncollect hyperlinks from only seed pages, a max depth of 2 to follow\nhyperlinks found on the seed pages and collect hyperlinks from those\npages as well, and so on radiating outwards. The number of pages and\nhyperlinks can rise very rapidly so it is best to keep this number as\nlow as possible. If a greater reach in collection sites is desired, this\ncould perhaps more efficiently be achieved by revising and adding more\nseed pages in the first instance. In the example code the\ntype has been set to “ext” (external) for all three seed\nsites, so as to limit “mapping” of the internal seed web sites and focus\non their outward facing connections. Depth of crawl was set to 2.\nIt should be noted that all hyperlinks found are collected from\nscraped pages and used to generate networks. The type and\nmax_depth parameters only apply to the web crawling and\nscraping activity.\nPerforming the Collection\nThe hyperlink data can now be collected using the\nCollect function with the pages parameter.\nThis produces a dataframe that contains the hyperlink URL’s found, pages\nthey were found on and other metadata that can be used to help construct\nnetworks.\n\n\nlibrary(magrittr)\nlibrary(dplyr)\nlibrary(vosonSML)\n\n# set up as a web collection and collect the hyperlink data using the\n# previously defined seed pages\nhyperlinks <- Authenticate(\"web\") %>% Collect(pages)\n\n# Collecting web page hyperlinks...\n# *** initial call to get urls - http://vosonlab.net\n# * new domain: http://vosonlab.net \n# + http://vosonlab.net (10 secs)\n# *** end initial call\n# *** set depth: 2\n# *** loop call to get urls - nrow: 6 depth: 2 max_depth: 2\n# * new domain: http://rsss.anu.edu.au \n# + http://rsss.anu.edu.au (0.96 secs)\n# ...\n\n# dataframe structure\nglimpse(hyperlinks)\n# Rows: 1,163\n# Columns: 9\n# $ url       <chr> \"http://rsss.anu.edu.au\", \"http://rsss.cass.anu.edu.au\", \"ht~\n# $ n         <int> 1, 1, 4, 1, 1, 2, 2, 2, 2, 2, 2, 1, 1, 1, 4, 1, 1, 1, 1, 1, ~\n# $ page_err  <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, ~\n# $ page      <chr> \"http://vosonlab.net\", \"http://vosonlab.net\", \"http://vosonl~\n# $ depth     <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ~\n# $ max_depth <dbl> 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ~\n# $ parse     <df[,6]> <data.frame[26 x 6]>\n# $ seed      <chr> \"http://vosonlab.net\", \"http://vosonlab.net\", \"http://vos~\n# $ type      <chr> \"ext\", \"ext\", \"ext\", \"ext\", \"ext\", \"ext\", \"ext\", \"ext\", \"ext~\n\n# number of pages scraped for hyperlinks\nnrow(hyperlinks %>% distinct(page))\n# [1] 38\n\n# number of hyperlinks collected\nnrow(hyperlinks)\n# [1] 1163\n\n\n\nA total of 1,163 hyperlinks were collected from 38 pages followed\nfrom our 3 seed pages. Using this data, it is now possible to generate\nhyperlink networks.\nNetwork Creation\nNetworks\nAs with other vosonSML social media, there are two\nstandard types of networks we can create. An activity\nnetwork that produces a more structural representation of the network\nwhere nodes are web pages and edges are the hyperlink references between\nthem, and an actor network that instead groups pages into\nentities based on their domain names.\n\n\n# generate a hyperlink activity network\nactivity_net <- Create(hyperlinks, \"activity\")\n\n# generate a hyperlink actor network\nactor_net <- Create(hyperlinks, \"actor\")\n# Generating web actor network...\n# Done.\n\n\n\nThe output of the network creation is a named list of two dataframes,\none for the nodes and the other for the edges\nor edge list data. The example below shows the actor_net.\nNote that the edges of the actor network are also\naggregated into a weight value and that actors can link to themselves\nforming self-loops.\n\n\nprint(as_tibble(actor_net$nodes))\n# # A tibble: 185 x 2\n#   id                              link_id\n#   <chr>                             <int>\n# 1 accounts.google.com                   1\n# 2 alumni.kellogg.northwestern.edu       2\n# 3 anu.edu.au                            3\n# # ... with 182 more rows\n\nprint(as_tibble(actor_net$edges))\n# # A tibble: 226 x 3\n#   from            to              weight\n#   <chr>           <chr>            <int>\n# 1 rsss.anu.edu.au anu.edu.au           2\n# 2 rsss.anu.edu.au rsss.anu.edu.au     36\n# 3 rsss.anu.edu.au soundcloud.com       1\n# # ... with 223 more rows\n\n\n\nPlot a Graph\nNow that the network has been generated, we can create a graph and\nplot it. The Graph function creates an igraph\nformat object that can be directly plotted or adjusted for presentation\nusing igraph plotting parameters.\n\n\nlibrary(igraph)\nlibrary(stringr)\n\nactor_net <- Create(hyperlinks, \"actor\")\n\n# identify the seed pages and set a node attribute\nseed_pages <- pages %>%\n  mutate(page = str_remove(page, \"^http[s]?://\"), seed = TRUE)\nactor_net$nodes <- actor_net$nodes %>%\n  left_join(seed_pages, by = c(\"id\" = \"page\"))\n\n# create an igraph from the network\ng <- actor_net %>% Graph()\n\n# set node colours\nV(g)$color <- ifelse(degree(g, mode = \"in\") > 1, \"yellow\", \"grey\")\nV(g)$color[which(V(g)$seed == TRUE)] <- \"dodgerblue3\"\n\n# set label colours\nV(g)$label.color <- \"black\"\nV(g)$label.color[which(V(g)$seed == TRUE)] <- \"dodgerblue4\"\n\n# set labels for seed sites and nodes with an in-degree > 1\nV(g)$label <- ifelse((degree(g, mode = \"in\") > 1 | V(g)$seed), V(g)$name, NA)\n\n# simplify and plot the graph\nset.seed(200)\ntkplot(simplify(g),\n       canvas.width = 1024, canvas.height = 1024,\n       layout = layout_with_dh(g),\n       vertex.size = 3 + (degree(g, mode = \"in\")*2),\n       vertex.label.cex = 1 + log(degree(g, mode = \"in\")),\n       edge.arrow.size = 0.4,\n       edge.width = 1 + log(E(g)$weight))\n\n\n\nFigure 2: Hyperlink network of\nactorsWe now have a simple graph of the actor hyperlink network. Our seed\nactors are indicated by blue nodes and sites with an in-degree greater\nthan one indicated in yellow. Node size and label size reflect most\nlinked to nodes or highest in-degree. Perhaps unsurprisingly, social\nmedia sites and the institutions at which the seed pages are located\nfeature most prominently in the network, and the graph plot provides us\na view of the actors online presence and connections.\nThere is much more network visualisation and analysis that could be\nperformed on the vosonSML hyperlink networks and we will be\nworking to add more features such as text analysis and network\nrefinements in our near future releases. In the meantime, we hope you\nhave found this practical introduction to our new tool useful and look\nforward to your feedback!\n\n\n\nAckland, R. 2010. “WWW Hyperlink Networks.” Edited by D. L.\nHansen and B. Shneiderman and M. A. Smith. Morgan-Kaufmann.\n\n\n———. 2013. Web Social Science: Concepts, Data and Tools for Social\nScientists in the Digital Age. SAGE Publications.\n\n\nAckland, R., and M. O’Neil. 2011. “Online Collective Identity: The\nCase of the Environmental Movement.” Social Networks 33\n(3): 177–90. https://doi.org/10.1016/j.socnet.2011.03.001.\n\n\nReferences compiled by Francisca\nBorquez↩︎\n",
    "preview": "posts/2021-03-15-hyperlink-networks-with-vosonsml/hyperlink_network.png",
    "last_modified": "2022-08-04T14:14:49+10:00",
    "input_file": {},
    "preview_width": 1024,
    "preview_height": 1025
  },
  {
    "path": "posts/2021-02-11-twitter-vosonsml-from-rtweet/",
    "title": "Creating Twitter Networks with vosonSML using rtweet Data",
    "description": "Simple guide to collecting data with rtweet and generating networks with vosonSML.",
    "author": [
      {
        "name": "Bryan Gertzel",
        "url": {}
      }
    ],
    "date": "2021-02-11",
    "categories": [
      "rstats",
      "twitter",
      "vosonSML",
      "rtweet",
      "networks"
    ],
    "contents": "\n\nContents\nIntroduction\nAPI Authentication\n\nTwitter Data\nCollection with rtweet\nSearch Collection\nSave the Data\n\nCreating Networks with\nvosonSML\nRead the Data\nPrepare the Data\nCreate the Network\n\n\nIntroduction\nSocial media platforms are a rich resource for Social Network data.\nTwitter is a highly popular public platform for social commentary that,\nlike most social media supporting third-party applications, allow\nsoftware to access and retrieve it’s data via Application Programming\nInterfaces or API’s.\nBecause of its popularity with individuals and communities around the\nworld, the ready availability of its data, and low barrier for entry,\nTwitter has become of great interest as a data source for online\nempirical research.\nThere have been many pieces of software developed across\nprogramming languages and environments to access the Twitter\nAPI. Within the R ecosystem the most comprehensive and well\nsupported of Twitter packages is rtweet\ndeveloped by Michael Kearney and part of the rOpenSci initiative. The\nrtweet package provides R functions to both authenticate\nand collect timelines, tweets and other metadata using Twitter’s v1.1\nstandard and premium API’s.\nThe VOSON Lab\ndevelops and maintains the open source R packages vosonSML\nand VOSONDash. These were created to integrate online data\ncollection, network generation and analysis into a consistent and easy\nto use work flow across many popular web and social media platforms. For\nTwitter, the vosonSML package provides an interface to\nrtweet’s collection features through which tweets can be searched for\nand retrieved, and then uses this data to produce networks. There may be\ncases however, such as in the collection of streaming data or analysis\nof previously collected twitter data where you haven’t used vosonSML’s\ncollection function but instead simply wish to produce\nvosonSML generated networks from your rtweet\ndata. Because vosonSML uses rtweet this is\neasily achievable and with minimal R coding.\nAPI Authentication\nAccessing the Twitter API to collect tweets requires authentication\nvia a Twitter app. There are generally two ways this can be achieved,\nyou can apply for a Twitter Developer account and create your own app\n(and access keys) or you can authorize another persons app to access the\nAPI on your behalf (using their keys). The latter still requires your\nown Twitter user account but you do not need to go through the Developer\napplication or app creation process. The vosonSML package\nrequires users to create their own app and use their own keys but the\nrtweet package supports both methods, and you can collect\ntweets after a simple one-time web authorization step of their embedded\nrstats2twitter app.\nTwitter Data Collection\nwith rtweet\nThe following simple example will demonstrate how to use the\nrtweet package to collect some tweet data using built-in\nauthentication via the rtweet app.\nSearch Collection\nA fairly standard tweet collection usually involves using the Twitter\nSearch API endpoint to search for past tweets that meet a\ncertain criteria. This can be done with rtweet and the\nsearch_tweets function with the criteria set by passing\nadditional parameters. In our example we will direct the API to search\nand return 100 tweets (n = 100) containing the hashtag\n#auspol and excluding any retweets\n(include_rts = FALSE). By default only the most recent\ntweets within the last 7 days will be returned by the API.\n\n\nlibrary(rtweet)\n\n# recent tweet search collection\nauspol_tweets <- search_tweets(\"#auspol\", n = 100, include_rts = FALSE)\n\n#> Requesting token on behalf of user...\n#> Waiting for authentication in browser...\n#> Press Esc/Ctrl + C to abort\n#> Authentication complete.\n\n\n\nThe first time rtweet collection functions are run they\nwill open a Twitter web page on your default web browser asking\npermission to authorize rstats2twitter.\n\n\n\nFigure 1: rstats2twitter app authorization\n\n\n\nIf API authentication and search succeeds then the\nsearch_tweets function will return a data frame of tweet\ndata. The data frame will have up to 100 rows, one for each tweet\ncollected and 90 columns for associated tweet metadata:\n\n\nlibrary(tibble)\n\n# print the first 2 rows\nprint(auspol_tweets, n = 2)\n# # A tibble: 100 x 90\n#   user_id  status_id  created_at          screen_name text      source\n#   <chr>    <chr>      <dttm>              <chr>       <chr>     <chr> \n# 1 27007685 136400068~ 2021-02-22 23:54:39 ronth~      \"@janeen~ Twitt~\n# 2 1359301~ 136400067~ 2021-02-22 23:54:37 Injur~      \"When th~ Twitt~\n\n\n\n\n\nShow additional columns\n\n# # ... with 98 more rows, and 84 more variables:\n# #   display_text_width <dbl>, reply_to_status_id <chr>,\n# #   reply_to_user_id <chr>, reply_to_screen_name <chr>,\n# #   is_quote <lgl>, is_retweet <lgl>, favorite_count <int>,\n# #   retweet_count <int>, quote_count <int>, reply_count <int>,\n# #   hashtags <list>, symbols <list>, urls_url <list>,\n# #   urls_t.co <list>, urls_expanded_url <list>, media_url <list>,\n# #   media_t.co <list>, media_expanded_url <list>, media_type <list>,\n# #   ext_media_url <list>, ext_media_t.co <list>,\n# #   ext_media_expanded_url <list>, ext_media_type <chr>,\n# #   mentions_user_id <list>, mentions_screen_name <list>, lang <chr>,\n# #   quoted_status_id <chr>, quoted_text <chr>,\n# #   quoted_created_at <dttm>, quoted_source <chr>,\n# #   quoted_favorite_count <int>, quoted_retweet_count <int>,\n# #   quoted_user_id <chr>, quoted_screen_name <chr>,\n# #   quoted_name <chr>, quoted_followers_count <int>,\n# #   quoted_friends_count <int>, quoted_statuses_count <int>,\n# #   quoted_location <chr>, quoted_description <chr>,\n# #   quoted_verified <lgl>, retweet_status_id <chr>,\n# #   retweet_text <chr>, retweet_created_at <dttm>,\n# #   retweet_source <chr>, retweet_favorite_count <int>,\n# #   retweet_retweet_count <int>, retweet_user_id <chr>,\n# #   retweet_screen_name <chr>, retweet_name <chr>,\n# #   retweet_followers_count <int>, retweet_friends_count <int>,\n# #   retweet_statuses_count <int>, retweet_location <chr>,\n# #   retweet_description <chr>, retweet_verified <lgl>,\n# #   place_url <chr>, place_name <chr>, place_full_name <chr>,\n# #   place_type <chr>, country <chr>, country_code <chr>,\n# #   geo_coords <list>, coords_coords <list>, bbox_coords <list>,\n# #   status_url <chr>, name <chr>, location <chr>, description <chr>,\n# #   url <chr>, protected <lgl>, followers_count <int>,\n# #   friends_count <int>, listed_count <int>, statuses_count <int>,\n# #   favourites_count <int>, account_created_at <dttm>,\n# #   verified <lgl>, profile_url <chr>, profile_expanded_url <chr>,\n# #   account_lang <lgl>, profile_banner_url <chr>,\n# #   profile_background_url <chr>, profile_image_url <chr>\n\n\n\nThis contains all of the data necessary for vosonSML to\nconstruct Twitter networks.\nSave the Data\nThere are a few methods of saving data depending on where and how it\nwill be used. Two common methods are to use a text-based file format\nsuch as a CSV, or\nalternatively if the data will be used within R we can save\nthe dataframe object to a binary compressed\nRDS (R data object) file using saveRDS\ninstead. Conveniently, the rtweet package has a method to\nsave Twitter data to file in CSV format with the write_as_csv\nfunction that takes care of Twitter nested data and conversion issues,\nand saving an RDS file is also very easy as follows.\n\n\n# save data using rtweet write csv\nwrite_as_csv(auspol_tweets, \"auspol_tweets.csv\")\n\n# save data to file as an R data object\nsaveRDS(auspol_tweets, \"auspol_tweets.rds\")\n\n\n\nCreating Networks with\nvosonSML\nRead the Data\nIf the data was saved to file with the rtweet function\nwrite_as_csv it can be read again using\nread_twitter_csv or readRDS if from an\nRDS file.\n\n\nauspol_tweets <- read_twitter_csv(\"auspol_tweets.csv\")\n\nauspol_tweets <- readRDS(\"auspol_tweets.rds\")\n\n\n\nPrepare the Data\nFor vosonSML to recognize the previously collected data\nas a Twitter data source and be able to internally route it to the\nappropriate network functions a minor change needs to be made to the\ndata frame first. This involves adding two attributes\ndatasource and twitter to the class list of\nthe auspol_tweets data frame object as follows:\n\n\n# original class list\nclass(auspol_tweets)\n\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n# add to the class list\nclass(auspol_tweets) <- append(c(\"datasource\", \"twitter\"), class(auspol_tweets))\n\n# modified class list\nclass(auspol_tweets)\n\n\n[1] \"datasource\" \"twitter\"    \"tbl_df\"     \"tbl\"        \"data.frame\"\n\nThe order of classes is important and for the data frame to be\ncompatible with dplyr - a\nvery common data manipulation package in R, and subsequently usable in\nthe tidyverse and\nvosonSML, then the new attributes need to be added to the\nbeginning of the list.\nFor versions of vosonSML more recent than\n0.29.13 this can now all be managed by using the\nImportData function. This method is preferable as it is\neasier, works for both files and data frames, and will support any\nfuture updates to vosonSML without breaking your code.\n\n\nlibrary(vosonSML)\n\n# use the import data function\nauspol_tweets <- ImportData(auspol_tweets, \"twitter\")\n\n\n\nPlease note that modifying data frame attributes or importing data is\nonly required for rtweet data and not a necessary step for\nTwitter data collected using the vosonSML Twitter\nCollect function.\nObject classes in R are a more advanced topic and not required\nknowledge to use vosonSML but if you would like to learn\nmore a good introduction can be found in the Object-oriented programming\nchapter of Advanced R by Hadley Wickham.\nCreate the Network\nThe tweet data can now be used to create the nodes and edges network\ndata, and a graph by using the vosonSML Create\nand Graph functions:\n\n\n# create the network data\nauspol_actor_network <- Create(auspol_tweets, \"actor\")\n\n\nGenerating twitter actor network...\n-------------------------\ncollected tweets | 100\ntweet mention    | 26\ntweet            | 57\nreply mention    | 15\nreply            | 25\nquote mention    | 7 \nquote            | 18\nnodes            | 149\nedges            | 148\n-------------------------\nDone.\n\n\n\n# create an igraph\nauspol_actor_graph <- Graph(auspol_actor_network)\n\n\nCreating igraph network graph...Done.\n\nThat’s all there is to it, and now the resulting igraph\nnetwork can be plotted.\n\n\nlibrary(igraph)\n\n# set plot margins\npar(mar = c(0, 0, 0, 0))\n\n# auspol actor network with fruchterman-reingold layout\nplot(auspol_actor_graph, layout = layout_with_fr(auspol_actor_graph),\n     vertex.label = NA, vertex.size = 6, edge.arrow.size = 0.4)\n\n\n\n\nFigure 2: Actor network graph for collected #auspol tweets\n\n\n\nFor further information about rtweet, its features and\nhow to use it to collect twitter data please refer to the package site and\nintroductory rtweet\nvignette. For creating different types of networks such as the\nactivity, 2-mode and semantic\ntypes with vosonSML see the package documentation and\nintroductory vosonSML\nvignette.\n\n\n\n",
    "preview": "posts/2021-02-11-twitter-vosonsml-from-rtweet/rtweet_logo_preview.png",
    "last_modified": "2022-08-04T14:14:49+10:00",
    "input_file": {},
    "preview_width": 432,
    "preview_height": 499
  },
  {
    "path": "posts/2021-02-05_welcome/",
    "title": "Welcome to the VOSON Lab Code Blog",
    "description": "The code blog is a space to share tools, methods, tips, examples and code. A place to collect data, construct and analyze online networks.",
    "author": [
      {
        "name": "Robert Ackland",
        "url": "http://vosonlab.net/"
      }
    ],
    "date": "2021-02-04",
    "categories": [
      "rstats",
      "SNA",
      "Computational Social Science"
    ],
    "contents": "\nWelcome to the VOSON Lab Code Blog! We have created this space to share methods, tips, examples and code. It’s also a place where we will demonstrate constructing and analyzing networks from various API and other online data sources.\nMost of our posts will cover techniques around the tools we have developed at the Lab: vosonSML, VOSONDash and voson.tcn, which are available on both CRAN and GitHub. But we also plan to use this space to cover other complementary R packages and open-source software, such as fantastic R packages within the tidyverse, RStudio’s shiny for web apps, and visualization tools such as igraph and Gephi.\nVOSON Lab R Packages - Hex stickersVOSON Lab Open Source Tools\nvosonSML is a R package for social media data collection (currently twitter, youtube, and reddit), hyperlink collection and network generation. VOSONDash is a Shiny app that integrates tools for visualizing and manipulating network graphs, performing network and text analysis, as well as an interface for collecting data with vosonSML.\nMore information on these packages, their development and code can be found on our vosonSML, VOSONDash and voson.tcn github pages.\nWe also have some other guides for using the packages. Check the vosonSML Vignette and the VOSON Dash Userguide for some practical examples and feature reference.\nWe hope you find this content useful!\nThe VOSON Lab team.\nVirtual Observatory for the Study of Online Networks VOSON Lab, School of Sociology, The Australian National University.\n\n\n\n",
    "preview": "posts/2021-02-05_welcome/square-cards.png",
    "last_modified": "2023-02-10T13:54:31+11:00",
    "input_file": {},
    "preview_width": 800,
    "preview_height": 640
  }
]

[
  {
    "path": "posts/2021-06-03-hashtag-twitter-collection-of-the-us-presidential-debates-2020/",
    "title": "#DebateNight 2020: Hashtag Twitter Collection of the US Presidential Debates",
    "description": "Methodology for the bulk collection of tweets containing key hashtags for the US Presidential Debates and generation of Networks for Analysis.",
    "author": [
      {
        "name": "Bryan Gertzel",
        "url": {}
      },
      {
        "name": "Robert Ackland",
        "url": {}
      }
    ],
    "date": "2021-06-03",
    "categories": [
      "rstats",
      "twitter",
      "hashtags",
      "networks",
      "election",
      "debate"
    ],
    "contents": "\r\n\r\nContents\r\nCollection Strategy\r\nHashtags\r\nTimezones\r\n\r\nStreaming Collection\r\nSearch Collection\r\nFirst Presidential Debate Preliminary Results\r\nData Summary\r\nData Tweet Activity\r\n\r\nNetwork Analysis\r\nMerge Collected Data\r\nCreate Networks\r\nReply-network Giant Component\r\n\r\n\r\nThe VOSON Lab undertook a number of Twitter collections on selected hashtags during the 2020 US Presidential debates and townhalls. The Twitter streaming API endpoint was used for a sampled real-time collection of tweets during the debates, and the recent search API was used post-debate to collect all tweets containing hashtags that occurred over the debate telecast periods. The two approaches differ in that the streaming API endpoint allows access to a “roughly 1% random sample of publicly available Tweets in real-time” using provided hashtags or terms as a filter to monitor events. The search API endpoint uses terms in a search query to match all tweets containing hashtags for a recent historical period in time. The streaming collections, a filtered sample, produced much smaller datasets and was useful for a timeley review of the Twitter activity during the debates as well as for identifying tweets to act as bounds for search collections. The retrospective search collections were much larger and produced more comprehensive datasets.\r\nThe R packages rtweet and vosonSML, the latter of which wraps the functionality of rtweet for its Twitter capability, were used in a semi-automated way to collect data with both streaming and search API’s. These packages use the standard Twitter API v1.1 endpoints.\r\nTwo US Presidential debates took place between Donald Trump and Joe Biden on September 29th and October 22nd, with one debate scheduled for October 15th cancelled due to COVID-19 concerns. One VP debate between Kamala Harris and Mike Pence took place on October 7th. This article will focus on the datasets collected for the first debate, widely reported as “pure chaos” and “90 minutes of insult and interjection” by commentators, to demonstrate our collection methodology and some simple network analysis.\r\nFigure 1: Next day headlines, tweet from https://twitter.com/oliverdarcyCollection Strategy\r\nThe US Presidential debates were all scheduled to run for 1.5 hours between 9pm and 10.30pm Eastern Daylight Time (EDT). To capture Twitter discussion surrounding the debate a four hour streaming window was chosen, with the collection starting at 8.30pm and concluding at 12.30am EDT (0.5 hours before and 2 hours after the debate). Streaming collection was performed by multiple team members, each with slightly staggered start times. This is because streaming collections were divided into 15 minute segments, and offsetting allowed tweets to be collected during connections, disconnections and segment changeover intervals. The tweets could then be merged and any duplicates removed in post-processing.\r\nBecause of the very likely large volume of tweets collected using the search API endpoint the collection window was reduced to 2 hours, starting 15 minutes before and concluding 15 minutes after the debates. Twitter searches are historical and page backwards in time, results are returned from most recent tweet as the first observation to the earliest tweet matching search criteria as last observation collected. There are also limits to how many tweets can be collected as defined by the API rate-limit. Using a bearer authentication token to perform searches allows for 45,000 tweets to be retrieved every 15 minutes.\r\nAs tweet status ID’s are sequential numbers, with new tweets incrementing the identifier, they can be used to set bounds for searches. Simply, the first tweet in the collected data from a search (earliest tweet in data) can be used as a starting point for subsequent searches as we move backwards in time. This means that given two tweets, one at the beginning and the other at the end of an event - the debate, we can systematically and programatically collect all tweets in-between working backwards from the end of event tweet. To identify which tweets to use as bounds we performed timestamp search of collected streaming tweets. We identified the highest status ID tweet matching our end time for the collection window 10.45pm EDT (search start) and the lowest ID matching our debate start time 8.45pm EDT (search end).\r\nIn practice, 45,000 tweets were collected with a pause for the 15 minute rate-limit to reset, then a further 45,000 were collected with a pause, and so on until the tweet that we identified as marking the beginning of the debate window was collected. Because this process could take many hours, it was important to perform the search collections within 7 days of the debates.\r\nHashtags\r\nA set of hashtags were selected in order to capture tweets related to the presidential debates. The following 12 were used for streaming and search collections for all debates, with 4 addditonal hashtags for the vice presidential debate and 3 for the townhalls.\r\nTable 1: Debate hashtags\r\n\r\nHashtags\r\nHashtags for all events\r\n#PresidentialDebate, #PresidentialDebates, #Election2020, #Debates2020, #Debates, #DebateNight, #Biden, #Biden2020, #BidenHarris2020, #Trump, #Trump2020, #TrumpPence2020\r\nAdditional vice presidential debate\r\n#VPDebate, #VPDebate2020, #VPDebates, #VPDebates2020\r\nAdditional televised townhalls\r\n#BidenTownHall, #TrumpTownHall, #TownHall\r\nTimezones\r\nThe first US Presidential debate took place in Cleveland, Ohio at 9pm Eastern Daylight Time (EDT) on the 29th September, 2020. Tweet timestamps are all in Universal Coordinated Time (UTC), meaning times in the data need to be offset by -4 to find the debate time in EDT. As collection took place in Canberra, Australia or Australian Eastern Standard Time (AEST), the local system timestamps produced by scripts for logging are in AEST. The first debate time and timezone conversions can be seen in the table below.\r\nTable 2: First debate timezone reference\r\nTimezone\r\nStart time\r\nEnd time\r\nEDT\r\n2020-09-29 21:00\r\n2020-09-29 22:30\r\nUTC\r\n2020-09-30 01:00\r\n2020-09-30 02:30\r\nAEST\r\n2020-09-30 11:00\r\n2020-09-30 12:30\r\nStreaming Collection\r\nFor the streaming collection a directory was created to for easier post-processing. Streaming data was collected and written to file in JSON format using timestamp formatted file names. The streaming collection period was set to 4 hours and divided into segments or files.\r\n\r\n\r\nShow code\r\n\r\nwd <- getwd()\r\n\r\n# check paths and create directories if they do not exist\r\ndata_path <- paste0(wd, \"/data\")\r\nif (!dir.exists(data_path)) { dir.create(data_path, showWarnings = FALSE) }\r\n\r\nstreams_path <- paste0(data_path, \"/pres-debate-streams\")\r\nif (!dir.exists(streams_path)) { dir.create(streams_path, showWarnings = FALSE) }\r\n\r\n# helper functions to write to log file and to create date time based file names\r\nlog <- function(msg, fn) { cat(msg, file = fn, append = TRUE, sep = \"\\n\") }\r\nfname <- function(path, ts) { paste0(path, \"/stream-\", gsub(\"[^[:digit:]_]\", \"\", ts)) }\r\n\r\n# set stream filter hashtags - comma seperated\r\nstream_filter <- paste0(\"#PresidentialDebate,#PresidentialDebates,#Election2020,\",\r\n                       \"#Debates2020,#Debates,#DebateNight,\",\r\n                       \"#Biden,#Biden2020,#BidenHarris2020,\",\r\n                       \"#Trump,#Trump2020,#TrumpPence2020\")\r\n\r\n# set the time period to collect tweets in seconds\r\nstream_period <- 4 * 60 * 60 # 4 hours or 14400 seconds\r\n\r\n# break up streaming collection into segments\r\nnum_segs <- 16 # each segment is 15 minutes\r\nseg_period <- ceiling(stream_period / num_segs)\r\n\r\n\r\n\r\nThe streaming collection is performed by the rtweet function stream_tweets which in our operation uses the query or filter parameter q, a timeout period which is the length of time to collect streaming tweets, and an output JSON file_name. The collection is wrapped in a loop which is for the number of 15 minute segments in the collection period. Each iteration sets up a new timestamped data file and log file.\r\n\r\n\r\nShow code\r\n\r\nlibrary(rtweet)\r\n\r\n# load rtweet auth token\r\ntoken <- readRDS(\"~/.rtweet_oauth1a\")\r\n\r\n# collect streaming tweets with a new file every 15 minutes\r\nfor (i in 1:num_segs) {\r\n\r\n  # create log file and JSON data file\r\n  timestamp <- Sys.time()\r\n  log_file <- paste0(fname(streams_path, timestamp), \".txt\")\r\n  json_file <- paste0(fname(streams_path, timestamp), \".json\")\r\n  \r\n  log(paste0(\"timestamp: \", timestamp, \"\\ntimeout: \",\r\n             seg_period, \" secs\\nfilter: \", stream_filter),\r\n    log_file)\r\n  \r\n  # collect streaming tweets and write to JSON file\r\n  tryCatch({\r\n    rtweet::stream_tweets(\r\n      token = token,\r\n      q = stream_filter,\r\n      timeout = seg_period,\r\n      file_name = json_file,\r\n      parse = FALSE\r\n    )\r\n  }, error = function(e) {\r\n    cat(paste0(e, \"\\n\"))\r\n    log(paste0(\"error: \", e), log_file)\r\n  })\r\n  \r\n  log(paste0(\"completed: \", Sys.time()), log_file)\r\n}\r\n\r\n\r\n\r\nLog entries for each 15 minute iteration confirm the collection period and each file matches a JSON data file (note timestamps are in local time which was AEST).\r\n# data/pres-debate-streams/stream-20200930102859.txt \r\n\r\ntimestamp: 2020-09-30 10:28:59\r\ntimeout: 900 secs\r\nquery: #PresidentialDebate,#PresidentialDebates,#Election2020,\r\n#Debates2020,#Debates,#DebateNight,#Biden,#Biden2020,#BidenHarris2020,\r\n#Trump,#Trump2020,#TrumpPence2020\r\ncompleted: 2020-09-30 10:43:59\r\nFor the first streaming collection iteration an 180MB JSON file was written with 111,244 lines. Each line contains the JSON for a single tweet, meaning the same number of tweets were collected.\r\n\r\n> first_json_file <- \"./data/pres-debate-streams/stream-20200930102859.json\"\r\n> file.size(first_json_file)\r\n[1] 188575977 # 180MB\r\n\r\n> length(readLines(first_json_file))\r\n[1] 111244\r\n\r\n16 JSON data files were written, corresponding to the number of 15 minute segments specified. These were then individually processed and converted to dataframes, which were then merged into a single complete streaming collection dataframe for the first debate.\r\nSearch Collection\r\nAs with the streaming collection directories were created to store collected data and search parameters set. The search query containing hashtags uses the Twitter OR search operator unlike the streaming filter which was comma seperated. A maximum number of tweets for each collection iteration as well as a maximum number of iterations are set. The number of tweets is required for the search request and is set to the maximum rate-limit value for a bearer token. A maximum number of iterations is set as a precaution to prevent infinite collection should a problem arise. The two tweets found from the streaming collection and used as search bounds are also set. The search will start at the latest id and continue until the earliest id is found, or the maximum iterations has been reached.\r\n\r\n\r\nShow code\r\n\r\nwd <- getwd()\r\n\r\n# check paths and create directories if they do not exist\r\ndata_path <- paste0(wd, \"/data\")\r\nif (!dir.exists(data_path)) { dir.create(data_path, showWarnings = FALSE) }\r\n\r\nsearches_path <- paste0(data_path, \"/pres-debate-searches\")\r\nif (!dir.exists(searches_path)) { dir.create(searches_path, showWarnings = FALSE) }\r\n\r\n# set search query hashtags - separated with OR search operator\r\nq <- paste0(\"#PresidentialDebate OR #PresidentialDebates \",\r\n            \"OR #Election2020 OR \",\r\n            \"#Debates2020 OR #Debates OR #DebateNight OR \",\r\n            \"#Biden OR #Biden2020 OR #BidenHarris2020 OR \",\r\n            \"#Trump OR #Trump2020 OR #TrumpPence2020\")\r\n\r\ntype <- \"recent\"\r\nnum_tweets <- 45000\r\nmax_iter <- 40\r\n\r\n# pres debate 1 search\r\nlatest_id <- \"1311121700394807296\"    # start tweet\r\nearliest_id <- \"1311104723978579968\"  # end tweet\r\n\r\n\r\n\r\nThe search collection is performed by the vosonSML function Collect. The process is more involved than the streaming collection in that the reset time for the rate-limit is calculated each collection iteration and the script sleeps for that period of time before continuing. Tracking of search progress is also logged to the console in this approach but was redirected to a log file.\r\n\r\n\r\nShow code\r\n\r\nlibrary(vosonSML)\r\n\r\nauth <- readRDS(\"~/.vsml_oauth2\")\r\n\r\ncat(\"large twitter search\\n\")\r\ncat(paste0(\"type: \", type, \"\\n\"))\r\ncat(paste0(\"tweets per iter: \", num_tweets, \"\\n\"))\r\ncat(paste0(\"max iter: \", max_iter, \" (\", (max_iter * num_tweets), \" tweets)\\n\\n\"))\r\n\r\ni <- 1\r\nwhile (i <= max_iter) {\r\n  cat(paste0(\"iteration \", i, \" of \", max_iter, \"\\n\"))\r\n  cat(paste0(\"time: \", Sys.time(), \"\\n\"))\r\n  cat(paste0(\"set max_id: \", latest_id, \"\\n\"))\r\n  req_time <- as.numeric(Sys.time())\r\n  reset_time <- req_time + (15 * 60) + 10 # add 10 sec buffer\r\n  \r\n  code_wd <- getwd()\r\n  setwd(searches_path)\r\n  \r\n  data <- tryCatch({\r\n    auth %>%\r\n      Collect(searchTerm = q,\r\n              searchType = type,\r\n              numTweets = num_tweets,\r\n              max_id = latest_id,\r\n              verbose = TRUE,\r\n              includeRetweets = TRUE,\r\n              retryOnRateLimit = TRUE,\r\n              writeToFile = TRUE)\r\n  }, error = function(e) {\r\n    cat(paste0(e, \"\\n\"))\r\n    NULL\r\n  })\r\n  \r\n  setwd(code_wd)\r\n  \r\n  if (!is.null(data) && nrow(data)) {\r\n    data_first_obvs_id <- data$status_id[1]\r\n    data_last_obvs_id <- data$status_id[nrow(data)]\r\n    cat(paste0(\"data nrows = \", nrow(data), \"\\n\",\r\n               \"first row status id = \", data_first_obvs_id, \"\\n\",\r\n               \"last row status id = \", data_last_obvs_id, \"\\n\"))\r\n    \r\n    # set latest id to lowest status id in data for NEXT iteration\r\n    # this is typically the last observation\r\n    latest_id <- data_last_obvs_id\r\n    \r\n    # if our target id is passed then stop\r\n    if (earliest_id >= latest_id) {\r\n      cat(\"earliest id reached\\n\")\r\n      break\r\n    }\r\n    now_time <- as.numeric(Sys.time())\r\n    if (i < max_iter) {\r\n      sleep_time <- reset_time - now_time\r\n      if (sleep_time > 0) {\r\n        cat(\"sleeping \", sleep_time, \" secs\\n\")\r\n        Sys.sleep(sleep_time)  \r\n      }      \r\n    }\r\n  } else {\r\n    cat(\"no data\\n\")\r\n    break\r\n  }\r\n  i <- i + 1\r\n}\r\n\r\ncat(paste0(\"completed: \", Sys.time(), \"\\n\"))\r\n\r\n\r\n\r\nThe first search collection iteration collected a full 45,000 tweets and wrote an R dataframe object to an RDS file. The vosonSML output also indicates the minimum and maximum tweet status ID in the data and their timestamp (UTC) to assist with tracking the collection progress, it shows that the first 45,000 tweets were all created within an approximate 2.25 min period. It took 5.5 mins for the first collection to complete, and it slept for over 9.5 mins while the rate-limit reset before iteration 2. Timestamps other than the tweet creation time are in local time AEST.\r\nlarge twitter search\r\ntype: recent\r\ntweets per iter: 45000\r\nmax iter: 40 (1800000 tweets)\r\n\r\niteration 1 of 40\r\ntime: 2020-10-03 08:50:31\r\nset max_id: 1311134926167834628\r\nCollecting tweets for search query...\r\nSearch term: #PresidentialDebate OR #PresidentialDebates OR\r\n#Election2020 OR #Debates2020 OR #Debates OR #DebateNight OR\r\n#Biden OR #Biden2020 OR #BidenHarris2020 OR #Trump OR #Trump2020\r\nOR #TrumpPence2020\r\nRequested 45000 tweets of 45000 in this search rate limit.\r\nRate limit reset: 2020-10-03 09:05:32\r\nDownloading [=========================================] 100%\r\n\r\ntweet  | status_id           | created             | screen_name   \r\n-------------------------------------------------------------------\r\nMin ID | 1311134367985565697 | 2020-09-30 02:42:47 | @pxxxxx_xx   \r\nMax ID | 1311134926167834628 | 2020-09-30 02:45:00 | @ixxxxxxxxxxx\r\nCollected 45000 tweets.\r\nRDS file written: ./data/pres-debate-searches/2020-10-03_085550-TwitterData.rds\r\nDone.\r\nElapsed time: 0 hrs 5 mins 22 secs (321.79)\r\ndata nrows = 45000\r\nfirst row status id = 1311134926167834628\r\nlast row status id = 1311134367985565697\r\nsleeping  588.2078  secs\r\n54 RDS data files containing Twitter collection dataframes were written and the search took approximately 17.5 hours. These files were then merged into a single complete search collection dataframe for the first debate.\r\nFirst Presidential Debate Preliminary Results\r\nData Summary\r\nData was collected by multiple team members, presented are the un-merged results from a single members streaming and search collections for the first presidential debate.\r\nTable 2: Collection summary\r\nTwitter API endpoint\r\nStart time (EDT)\r\nEnd time (EDT)\r\nPeriod (hours)\r\nObservations (unique tweets)\r\nStreaming\r\n2020-09-29 20:30\r\n2020-09-30 00:30\r\n4.00\r\n449,102\r\nSearch\r\n2020-09-29 20:45\r\n2020-09-29 22:45\r\n2.00\r\n2,387,587\r\nData Tweet Activity\r\nTime series plots for the streaming and search collections were created to indicate tweet activity over time. Observations are grouped by tweet type and into 5 minute bins. Perhaps unsurprisingly, retweet activity appears to became more prevalent as the first debate progressed. At around 10.20pm in the search collection just over 100,000 retweets were collected.\r\nFigure 2: Streaming collection time series plotFigure 3: Search collection time series plotNetwork Analysis\r\nUsing vosonSML the Twitter data for both streaming and search collections is able to be converted into networks in the same way. The following code will demonstrate the merging of search collection data, and creation of an activity and actor network for the first debate.\r\nMerge Collected Data\r\n\r\n\r\nlibrary(dplyr)\r\n\r\n# combine the search data files and remove duplicate tweets\r\n\r\n# get all of the files to combine\r\nfiles <- list.files(path = \"./data/pres-debate-searches/\",\r\n                    pattern = \".+TwitterData\\\\.rds$\", full.names = TRUE)\r\n\r\n# merge dataframes\r\ncomb_data <- bind_rows(lapply(files, function(x) { readRDS(x) }))\r\ndata <- comb_data %>% arrange(desc(status_id))\r\n\r\n# find and remove any duplicates\r\ndupes <- data %>% filter(status_id %in% data$status_id[duplicated(data$status_id)])\r\nif (nrow(dupes)) { data <- data %>% distinct(status_id, .keep_all = TRUE) }\r\n\r\nsaveRDS(data, \"./data/pres_debate1_search.rds\") # save combined data\r\n\r\n\r\n\r\nCreate Networks\r\nBecause the combined search data for the first debate is quite large, it can be useful to reduce it to a much smaller window of time for demonstration and network visualization purposes. The following code will extract 15 minutes of tweet data from between 21:30 - 21:45 EDT.\r\n\r\ndata <- readRDS(\"./data/pres_debate1_search.rds\") # load the previously saved data\r\n\r\n# filter out tweets with creation timestamps outside of the 15 min window\r\ndata <- data %>% filter(created_at >= as.POSIXct(\"2020-09-30 01:30:00\", tz = \"UTC\") &\r\n                        created_at <= as.POSIXct(\"2020-09-30 01:45:00\", tz = \"UTC\"))\r\n\r\n> nrow(data)\r\n[1] 349937\r\n> min(data$created_at)\r\n[1] \"2020-09-30 01:30:00 UTC\"\r\n> max(data$created_at)\r\n[1] \"2020-09-30 01:45:00 UTC\"\r\n\r\nThe data is now comprised of 349,937 unique tweets that all were created during our specified window. We can now create our networks using vosonSML.\r\n\r\nlibrary(vosonSML)\r\n\r\n# use the vosonsml create function to create networks\r\n\r\n# activity network\r\n> net_activity <- data %>% Create(\"activity2\")\r\nGenerating twitter activity network...\r\n-------------------------\r\ncollected tweets | 349937\r\ntweet            | 125626\r\nretweet          | 212842\r\nreply            | 7316\r\nquote            | 4210\r\nnodes            | 366029\r\nedges            | 349994\r\n-------------------------\r\nDone.\r\n\r\n# actor network\r\n> net_actor <- data %>% Create(\"actor2\")\r\nGenerating twitter actor network...\r\n-------------------------\r\ncollected tweets | 349937\r\ntweet mention    | 15021\r\ntweet            | 125626\r\nretweet          | 212842\r\nreply mention    | 2202 \r\nreply            | 7316 \r\nquote mention    | 734  \r\nquote            | 4210 \r\nnodes            | 202333\r\nedges            | 367951\r\n-------------------------\r\nDone.\r\n\r\nThe activity network has 366,029 nodes or unique tweets, and the actor network has 202,333 nodes or unique actors for our 15 minute window.\r\nReply-network Giant Component\r\nIf we’re interested in exploring some of the Twitter interactions taking place between users during the 21:30 - 21:45 window of the first debate, the network can be further distilled by looking at the actor network and including only reply edges. This will reveal a number of reply-conversations, but we can select for the giant component to find the largest one. The igraph library can be used to perform a number of common network operations such as removing self-loops, isolates and finding the giant component.\r\n\r\nlibrary(igraph)\r\n\r\n# convert vosonsml actor network to igraph object\r\n> g_actor <- net_actor %>% Graph()\r\nCreating igraph network graph...Done.\r\n\r\n# remove edges that are not replies\r\n# remove self-loops and isolates\r\ng_actor_reply <- g_actor %>% delete_edges(E(g_actor)[E(g_actor)$edge_type != \"reply\"])\r\ng_actor_reply <- g_actor_reply %>% simplify(remove.multiple = FALSE)\r\ng_actor_reply <- g_actor_reply %>%\r\n  delete_vertices(V(g_actor_reply)[which(degree(g_actor_reply) == 0)])\r\n\r\n# find the giant component\r\ncomps <- clusters(g_actor_reply, mode = \"weak\")\r\nlargest_cluster_id <- which.max(comps$csize)\r\nnode_ids <- V(g_actor_reply)[comps$membership == largest_cluster_id]\r\ng_actor_reply_gc <- induced_subgraph(g_actor_reply, node_ids)\r\n\r\nThe giant component in the reply-network has 1743 nodes and 1982 edges. We can use igraphs degree function to further explore who the most prominent actors are in the network by in-degree.\r\n\r\n> V(g_actor_reply_gc)$screen_name[order(degree(g_actor_reply_gc, mode = \"in\"), decreasing = TRUE)]\r\n[1] \"realDonaldTrump\"    \"JoeBiden\"           \"GOPLeader\"         \r\n[4] \"KamalaHarris\"       \"ProjectLincoln\"     \"Alyssa_Milano\"\r\n\r\nTo visualise the conversation occurring during this 15 minutes of the debate, we first removed the Twitter accounts for the two candidates (\"realDonaldTrump\", \"JoeBiden\", \"GOPLeader\"), and then constructed a new giant component from the reply network, which now has 1345 nodes and 1484 edges.\r\nThe visualization of this network (using Gephi) with node size proportional to in-degree is below.\r\nFigure 4: Reply-network giant component with candidates removedThis graph provides some insight into the largest reply network at our chosen point in time, revealing the Twitter actors receiving the most reply attention and their associations.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-06-03-hashtag-twitter-collection-of-the-us-presidential-debates-2020/debate_preview.png",
    "last_modified": "2021-06-25T10:29:33+10:00",
    "input_file": "hashtag-twitter-collection-of-the-us-presidential-debates-2020.knit.md",
    "preview_width": 1024,
    "preview_height": 1024
  },
  {
    "path": "posts/2021-03-23-twitter-conversation-networks/",
    "title": "Twitter Conversation Networks",
    "description": "Getting started with the voson.tcn package.",
    "author": [
      {
        "name": "Bryan Gertzel",
        "url": {}
      },
      {
        "name": "Francisca Borquez",
        "url": {}
      }
    ],
    "date": "2021-03-23",
    "categories": [
      "rstats",
      "twitter",
      "conversations",
      "voson.tcn",
      "networks"
    ],
    "contents": "\r\n\r\nContents\r\nTwitter Developer Access\r\nInstallation\r\nAuthentication\r\nCollection\r\nNetwork Creation\r\nActivity Network\r\nActor Network\r\n\r\nPlot Graphs\r\nActivity Network\r\nActor Network\r\n\r\n\r\n The VOSON Lab has recently published to GitHub a new open source R package called voson.tcn. The package uses the Early-Access Twitter API v2, to collect tweets belonging to specified threaded conversations and generate networks. The Twitter API v2 provides a new tweet identifier: the conversation ID, that is common to all tweets that are part of a conversation, and can be searched for using the API search endpoints. Identifiers and associated metadata for referenced tweets can also be collected in the same search for conversation tweets, allowing us to construct twitter networks with tweet and user metadata whilst minimising API requests.\r\nTwitter Developer Access\r\nThe voson.tcn package requires developer app authentication keys or tokens to access the Twitter API v2. These can be either the Access token & secret of an app or its Bearer token.\r\nTo obtain these credentials and use the early-access API you will need to have or apply for a Twitter Developer Account, as well as have activated the new Developer Portal. Once approved you will then need to create a development project, which is the new management container for apps, and either create a new app or associate one of your existing apps with it.\r\nThere are currently two project types available that correspond to Twitter’s developer product tracks, a standard and academic type. Academic projects are only available to researchers who have completed and have had their application for the academic research track approved for non-commercial research purposes. Standard projects are for more general use, including hobby and educational purposes. The project type features differ in their API access and caps; standard projects can only access the 7-day recent search endpoint whereas an academic project can access the full-archive search endpoint for historical tweets. There are also rate-limits and monthly tweet caps for API v2 search endpoints. At the time of writing, the caps are 500k and 10 million tweets that can be retrieved per month for the standard and academic track projects respectively.\r\nPlease note that there are also terms of use and restricted use cases that should be considered before applying for access or using the Twitter API.\r\nInstallation\r\nThe voson.tcn R package is in development and currently only available on GitHub. It can be installed as follows:\r\n\r\n\r\n# use the remotes package to install the latest dev version of voson.tcn from github\r\nlibrary(remotes)\r\ninstall_github(\"vosonlab/voson.tcn\")\r\n\r\n# Downloading GitHub repo vosonlab/voson.tcn@HEAD\r\n# √  checking for file\r\n# -  preparing 'voson.tcn':\r\n# √  checking DESCRIPTION meta-information ... \r\n# -  checking for LF line-endings in source and make files and shell scripts\r\n# -  checking for empty or unneeded directories\r\n# -  building 'voson.tcn_0.1.6.9000.tar.gz'\r\n#    \r\n# * installing *source* package 'voson.tcn' ...\r\n# ...\r\n# * DONE (voson.tcn)\r\n# Making 'packages.html' ... done\r\n\r\n\r\n\r\nAuthentication\r\nThe voson.tcn package only supports app based authentication using OAuth2.0 tokens which are also known as bearer tokens. We will likely support user based tokens in the future, however at this stage they do not offer any advantages as they have lower rate-limits and we are not using any private metadata of which they permit access (such as user-visible only metrics).\r\nThe token can be created using either your apps access token & secret (also known as consumer keys) or its bearer token. It is recommended that this token is saved for future use; there is no need to perform this step more than once as the token will not change unless it is invalidated or you regenerate keys on the developer portal.\r\n\r\n\r\nlibrary(voson.tcn)\r\n\r\n# retrieves a bearer token from the API using the apps consumer keys\r\ntoken <- tcn_token(consumer_key = \"xxxxxxxx\",\r\n                   consumer_secret = \"xxxxxxxx\")\r\n\r\n# alternatively if you have a bearer token already you can assign it directly\r\ntoken <- list(bearer = \"xxxxxxxxxxxx\")\r\n\r\n# if you save the token to file this step only needs to be done once\r\nsaveRDS(token, \"~/.tcn_token\")\r\n\r\n\r\n\r\nCollection\r\nCollecting conversation tweets requires the tweet ID or URL of a tweet that belongs to each threaded conversation that you are interested in. These are passed to the voson.tcn collect function tcn_threads as a vector or list. Conversation IDs will be tracked by this function to avoid duplication and, if tweet IDs are found to belong to a conversation that has already been collected on, then that conversation will be skipped.\r\nIn the following example, we are collecting the tweets for a threaded conversation belonging to a public lockdown announcement following a COVID-19 outbreak in Brisbane, Queensland, Australia, that took place on March, 29, 2021. The tweet URL or ID (number following the status in the URL) can be passed directly to the collection function.\r\nFigure 1: Public announcement tweet regarding a COVID-19 lockdown of Brisbane, from the Queensland Premier\r\n\r\n# read token from file\r\ntoken <- readRDS(\"~/.tcn_token\")\r\n\r\n# collect the conversation thread tweets for supplied ids           \r\ntweets <- tcn_threads(\"https://twitter.com/AnnastaciaMP/status/1376311897624956929\", token)\r\n\r\n\r\n\r\nWhen completed, a list of named dataframes will be returned, with tweets containing all of the tweets and their metadata, and users containing all of the referenced users in the tweets and their metadata. In our example, 286 tweets were collected with 180 associated users public metadata.\r\nNote that the collection of a threaded tweet conversation is a snapshot of the state of the conversation at a point in time. Metrics and networks produced from our data will not completely match subsequent collections of the same conversation, as it will have likely cumulatively expanded over time.\r\n\r\n\r\n# collected tweets\r\nprint(tweets$tweets, n = 3)\r\n# # A tibble: 286 x 14\r\n#   in_reply_to_user~ conversation_id  source  author_id  tweet_id  ref_tweet_type\r\n#   <chr>             <chr>            <chr>   <chr>      <chr>     <chr>         \r\n# 1 15999~            137631189762495~ Twitte~ 134852208~ 13763373~ replied_to    \r\n# 2 1142316897985163~ 137631189762495~ Twitte~ 126908387~ 13763373~ replied_to    \r\n# 3 25683~            137631189762495~ Twitte~ 137503906~ 13763371~ replied_to    \r\n# # ... with 283 more rows, and 8 more variables: ref_tweet_id <chr>, text <chr>,\r\n# #   created_at <chr>, includes <chr>, public_metrics.retweet_count <int>,\r\n# #   public_metrics.reply_count <int>, public_metrics.like_count <int>,\r\n# #   public_metrics.quote_count <int>\r\n\r\n# users metadata\r\nprint(tweets$users, n = 3)\r\n# # A tibble: 180 x 12\r\n#   profile.username profile.created_~ profile.profile_~ user_id profile.descript~\r\n#   <chr>            <chr>             <chr>             <chr>   <chr>            \r\n# 1 MSMW~            2013-03-30T06:48~ https://pbs.twim~ 131592~ \"Fact checking i~\r\n# 2 bpro~            2012-12-04T02:07~ https://pbs.twim~ 987844~ \"Only way to get~\r\n# 3 scre~            2009-10-22T22:56~ https://pbs.twim~ 844463~ \"I'm a  creative~\r\n# # ... with 177 more rows, and 7 more variables: profile.name <chr>,\r\n# #   profile.verified <lgl>, profile.location <chr>,\r\n# #   profile.public_metrics.followers_count <int>,\r\n# #   profile.public_metrics.following_count <int>,\r\n# #   profile.public_metrics.tweet_count <int>,\r\n# #   profile.public_metrics.listed_count <int>\r\n\r\n\r\n\r\nIf interested in text analysis, the tweet text can be found in the text column of the tweets dataframe and user profile descriptions in profile.description of the users dataframe.\r\nPublic metrics for tweets and users are found in dataframe columns prefixed, with public_metrics and profile.public_metrics respectively.\r\n\r\n\r\nlibrary(dplyr)\r\n\r\nnames(select(tweets$tweets, starts_with(\"public_metrics\")))\r\n# [1] \"public_metrics.retweet_count\" \"public_metrics.reply_count\"\r\n# [3] \"public_metrics.like_count\" \"public_metrics.quote_count\"\r\n\r\nnames(select(tweets$users, starts_with(\"profile.public_metrics\")))\r\n# [1] \"profile.public_metrics.followers_count\"\r\n# [2] \"profile.public_metrics.following_count\"\r\n# [3] \"profile.public_metrics.tweet_count\"\r\n# [4] \"profile.public_metrics.listed_count\"\r\n\r\n\r\n\r\nNetwork Creation\r\nThere are two types of networks that can be generated using voson.tcn: activity and actor network. These differ by the type of node and resulting structure of the networks.\r\nActivity Network\r\nAn activity network is a representation of the conversation as seen on Twitter: nodes are tweets and the edges are how they are related. Tweets (or nodes) are identified by their unique identifier Tweet ID (formerly Status ID). In a Twitter threaded conversation, there are only two types of connections or edges between tweets and these are replied_to and quoted.\r\nReplies are made when a user chooses the reply option and publishes a tweet response to the tweet they are replying to. Quotes are a little different in that the user has included a link to or quoted another tweet in the body of their tweet. In Twitter conversation networks, it is common to quote a tweet as part of a reply tweet, generating in the activity network a replied_to and quoted edge from the same node.\r\n\r\n\r\n# generate an activity network\r\nactivity_net <- tcn_network(tweets, \"activity\")\r\n\r\n# number of nodes\r\nnrow(activity_net$nodes)\r\n# [1] 279\r\n\r\n# number of edges\r\nprint(activity_net$edges, n = 3)\r\n# # A tibble: 281 x 3\r\n#   from                to                  type      \r\n#   <chr>               <chr>               <chr>     \r\n# 1 1376337359126495232 1376328523518898176 replied_to\r\n# 2 1376337350163267584 1376325216658317315 replied_to\r\n# 3 1376337128016113665 1376311897624956929 replied_to\r\n# # ... with 278 more rows\r\n\r\nunique(activity_net$edges$type)\r\n# [1] \"replied_to\" \"quoted\"\r\n\r\n\r\n\r\nActor Network\r\nAn actor network represents the interactions between Twitter users in the conversation: nodes are the users and edges are their connections. As in the activity network, edges are either a reply or a quote but edges represent the classification of a tweet connecting users rather than the activity. Users (or nodes) are identified by their unique Twitter User ID. In the actor network, interactions between users are more apparent and can be measured by the frequency (and direction) of edges between them.\r\n\r\n\r\n# generate an actor network\r\nactor_net <- tcn_network(tweets, \"actor\")\r\n\r\n# number of nodes or actors\r\nnrow(actor_net$nodes)\r\n# [1] 180\r\n\r\nprint(actor_net$edges, n = 3)\r\n# # A tibble: 286 x 6\r\n#   from      to        type  tweet_id     created_at    text                     \r\n#   <chr>     <chr>     <chr> <chr>        <chr>         <chr>                    \r\n# 1 13485220~ 15999128~ reply 13763373591~ 2021-03-29T0~ \"@Ther~ @Scott~\r\n# 2 12690838~ 11423168~ reply 13763373501~ 2021-03-29T0~ \"@Luke~ @Annas~\r\n# 3 13750390~ 25683344~ reply 13763371280~ 2021-03-29T0~ \"@AnnastaciaMP You do un~\r\n# # ... with 283 more rows\r\n\r\nunique(actor_net$edges$type)\r\n# [1] \"reply\" \"quote\" \"tweet\"\r\n\r\n\r\n\r\nNote that in the actor network there is an additional edge type: tweet, which is assigned to a self-loop edge created for the thread’s initial tweet. This is a technique used to retain the initial tweet’s metadata as edge attributes comparable to other edges in the network.\r\nThe initial conversation tweet would not usually be included in the edge list, as the initial conversation tweet is not directed at another user, and hence no edge to attach metadata.For example, this allows the text of the initial tweet to be included in any actor network tweet text analysis. It would not usually be included in the edge list as the initial conversation tweet is not directed at another user, and hence no edge to attach metadata is naturally found in this type of network.\r\nPlot Graphs\r\nActivity Network\r\nVisualisation of the activity network produced with igraph.\r\n\r\n\r\nlibrary(igraph)\r\nlibrary(RColorBrewer)\r\n\r\ng <- graph_from_data_frame(activity_net$edges, vertices = activity_net$nodes)\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\n# change likes to log scale\r\nlike_count <- V(g)$public_metrics.like_count\r\nlike_count[is.na(like_count)] <- 0\r\nln_like_count <- log(like_count)\r\nln_like_count[!is.finite(ln_like_count)] <- 0\r\n\r\n# set node size based on likes, min size 4\r\nsize <- ln_like_count * 2\r\nV(g)$size <- ifelse(size > 0, size + 8, 4)\r\n\r\n# set node label if number of likes >= 2\r\nV(g)$label <- ifelse(like_count >= 2, like_count, NA)\r\nV(g)$label.color <- \"black\"\r\n\r\n# set node colors based on number of retweets low to high is yellow to green\r\n# set tweets with no retweets to grey\r\nrt_count <- V(g)$public_metrics.retweet_count\r\nrt_count[is.na(rt_count)] <- 0\r\ncols <- colorRampPalette(c(\"yellow1\", \"green3\"))\r\ncols <- cols(max(rt_count) + 1)\r\nV(g)$color <- cols[rt_count + 1]\r\nV(g)$color[which(rt_count < 1)] <- \"lightgrey\"\r\n\r\n# set edge color to orange if tweet quoted another tweet\r\nE(g)$color <- ifelse(E(g)$type == \"quoted\", \"orange\", \"grey\")\r\n\r\n\r\n\r\n\r\n\r\n# plot the graph using fruchterman reingold layout\r\nset.seed(200)\r\ntkplot(g,\r\n       canvas.width = 1024, canvas.height = 1024,\r\n       layout = layout_with_fr(g),\r\n       edge.arrow.size = 0.5,\r\n       edge.width = 2)\r\n\r\n\r\n\r\nFigure 2: Conversation activity network - Node size and label represent number of tweet likes, color scale is indicating low to high number of retweets (yellow to green). Orange coloured edges are quoting linked tweet.voson.tcn collects tweets that are all linked to each other via the conversation ID. This means that in a network generated from this data, such as the activity network, all of the nodes (tweets) should be connected in a single component per conversation ID. If multiple conversation IDs were collected on then, it is also possible to have one component because of quote edges. These edges joining conversations occur when a tweet in one conversation has quoted a tweet in another that you have collected.\r\nIn the example activity network above, there are two components even though only one conversation ID was collected on. Multiple components are usually due to a missing conversation tweet not able to be retrieved from the API and producing a broken reply chain. This can often occur if, for example, a tweet has been deleted, or the tweet or user flagged or suspended in some way restricting public availability.\r\nActor Network\r\nVisualisation of the actor network produced with igraph.\r\n\r\n\r\nlibrary(dplyr)\r\nlibrary(magrittr)\r\nlibrary(stringr)\r\n\r\nregex_ic <- function(x) regex(x, ignore_case = TRUE)\r\n\r\n# best effort set the node colour attribute based on presence of city, state,\r\n# or country in the actors profile location field\r\n# value assigned from first match\r\nnodes <- actor_net$nodes %>%\r\n  mutate(color = case_when(\r\n    str_detect(profile.location, regex_ic(\"brisbane|bris\")) ~ \"orange\",\r\n    str_detect(profile.location, regex_ic(\"queensland|qld\")) ~ \"gold\",\r\n    str_detect(profile.location, regex_ic(\"australia|oz\")) ~ \"yellow\",\r\n    TRUE ~ \"lightgrey\"))\r\n\r\ng2 <- graph_from_data_frame(actor_net$edges, vertices = nodes)\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\n# the following code de-clutters the actor network by removing some nodes\r\n# that are not part of conversation chains and are stand-alone replies to\r\n# the initial thread tweet\r\n\r\n# get the author of the initial thread tweet using the conversation id\r\nconversation_ids <- c(\"1376311897624956929\")\r\nthread_authors <- activity_net$nodes %>%\r\n  filter(tweet_id %in% conversation_ids) %>% select(user_id)\r\n\r\n# remove actors replying to the initial tweet that have a degree of 1\r\nthread_spokes <- unlist(\r\n  incident_edges(g2, V(g2)[which(V(g2)$name %in% thread_authors$user_id)],\r\n                 \"in\"))\r\nspokes_tail_nodes <- V(g2)[tail_of(g2, thread_spokes)]$name\r\ng2 <- delete_vertices(g2, degree(g2) == 1 & V(g2)$name %in% spokes_tail_nodes)\r\n\r\n# convert the graph to undirected\r\n# simplify the graph and collapse edges into an edge weight value\r\nE(g2)$weight <- 1\r\ng2 <- as.undirected(simplify(g2, edge.attr.comb = list(weight = \"sum\")))\r\ng2 <- delete_vertices(g2, degree(g2) == 0)\r\n\r\n# use edge weight for graph edge width\r\nE(g2)$width <- ifelse(E(g2)$weight > 1, E(g2)$weight + 1, 1)\r\n\r\n# use the actors followers count for node size \r\nfollowers_count <- log(V(g2)$profile.public_metrics.followers_count)\r\nfollowers_count[!is.finite(followers_count)] <- 0\r\nsize <- followers_count * 3\r\nV(g2)$size <- ifelse(size < 6, 6, size)\r\nV(g2)$label <- ifelse(followers_count > 0,\r\n                      V(g2)$profile.public_metrics.followers_count, NA)\r\n\r\n\r\n\r\n\r\n\r\n# plot the graph using automatically chosen layout\r\nset.seed(201)\r\ntkplot(g2,\r\n       canvas.width = 1024, canvas.height = 1024,\r\n       layout = layout_nicely(g2),\r\n       vertex.label.cex = 0.8,\r\n       vertex.label.color = \"black\")\r\n\r\n\r\n\r\nFigure 3: Conversation actor network - Node size and label represent users follower counts. Node color indicates user self-reported location. Edge width represents number of collapsed edges.\r\n\r\n\r\n",
    "preview": "posts/2021-03-23-twitter-conversation-networks/activity_network.png",
    "last_modified": "2021-06-17T02:02:52+10:00",
    "input_file": {},
    "preview_width": 1024,
    "preview_height": 1025
  },
  {
    "path": "posts/2021-03-15-hyperlink-networks-with-vosonsml/",
    "title": "Hyperlink Networks with vosonSML",
    "description": "An introduction to creating hyperlink networks with vosonSML.",
    "author": [
      {
        "name": "Bryan Gertzel",
        "url": {}
      },
      {
        "name": "Francisca Borquez",
        "url": {}
      }
    ],
    "date": "2021-03-15",
    "categories": [
      "rstats",
      "hyperlinks",
      "vosonSML",
      "networks"
    ],
    "contents": "\r\n\r\nContents\r\nIntroduction\r\nInstallation\r\n\r\nHyperlink Collection\r\nSetting Up\r\nPerforming the Collection\r\n\r\nNetwork Creation\r\nNetworks\r\nPlot a Graph\r\n\r\n\r\nThe VOSON software for hyperlink collection and analysis was an early research output of the VOSON Lab (Ackland 2010). It addressed a need for tools that could help study online social networks, even before the rise of social media, and assisted researchers gain insights into important phenomena such as networks around issue spheres and online social movements [see (Ackland and O’Neil 2011) and (Ackland 2013)]. After many years and many iterations since its inception in 2004, the VOSON Lab is happy to reintroduce the canonical VOSON hyperlink collection software as part of our R open-source toolkit for social media collection: vosonSML.\r\nThis simple guide will demonstrate how to use the new features of the vosonSML package to perform a hyperlink collection and generate networks for analysis.\r\nIntroduction\r\nThe vosonSML hyperlink collection and network creation works similarly to the 3-step process we use with other social media sources: the Authenticate, Collect and Create verb functions. The Authenticate function is first called with the parameter “web” to identify and set up the context for subsequent operations, but it does not require any further credentials in this implementation. vosonSML uses standard web crawling and text-based page scraping techniques to discover hyperlinks and, as such, there is no need to access any restricted data API’s as we commonly do with social media.\r\nInstallation\r\nThe new hyperlink collection and network features are currently available in the development version of vosonSML on GitHub, and are to soon be released on CRAN. The development version can be installed as follows:\r\n\r\n\r\n# use the remotes package to install the latest dev version of vosonSML from github\r\nlibrary(remotes)\r\ninstall_github(\"vosonlab/vosonsml\")\r\n\r\n# Downloading GitHub repo vosonlab/vosonsml@HEAD\r\n# √  checking for file\r\n# -  preparing 'vosonSML':\r\n# √  checking DESCRIPTION meta-information ... \r\n# -  checking for LF line-endings in source and make files and shell scripts\r\n# -  checking for empty or unneeded directories\r\n# -  building 'vosonSML_0.30.00.9000.tar.gz'\r\n#    \r\n# * installing *source* package 'vosonSML' ...\r\n# ...\r\n# * DONE (vosonSML)\r\n# Making 'packages.html' ... done\r\n\r\n\r\n\r\nHyperlink Collection\r\nSetting Up\r\nThe web sites or pages to collect hyperlinks from are specified and input to the Collect function in a dataframe. As there are page specific options that can be used, this format helps us to organise and set the request parameters. The URL’s set in the dataframe for the page column are called ‘seed pages’ and are the starting points for web crawling. Although not explicitly indicated in the URL’s, the seed pages are actually the landing pages or “index” pages of the web sites and a page name can be specified if known or desired.\r\n\r\n\r\n# set sites as seed pages and set each for external crawl with a max depth\r\npages <- data.frame(page = c(\"http://vosonlab.net\",\r\n                             \"https://www.oii.ox.ac.uk\",\r\n                             \"https://sonic.northwestern.edu\"),\r\n                    type = c(\"ext\", \"ext\", \"ext\"),\r\n                    max_depth = c(2, 2, 2))\r\n\r\n\r\n\r\nThe example above shows seed pages with some additional per-seed parameters that are used to control the web crawling. The type parameter can be set to a value of either int, ext or all, which correspond to following only internal, external or following all hyperlinks found on a seeded web page and subsequent pages discovered from that particular seed. How a hyperlink is classified is determined by the seed domain name, for example, if the seed page is https://vosonlab.net a type of ext will follow hyperlinks from that page that do not have a domain name of “vosonlab.net.” A type of int will follow only hyperlinks that match a domain of “vosonlab.net,” and a type of all will follow all hyperlinks found irrespective of their domain. The final parameter max_depth refers to how many levels of pages to follow from the seed page. In the diagram below, the green dots are pages scraped by the web crawler and the blue dots links are the hyperlinks collected from them for a max depth of 1,2 and 3.\r\nFigure 1: Scope of hyperlinks collected using the max_depth parameterAs can be seen, a max depth of 1 directs the crawler to scrape and collect hyperlinks from only seed pages, a max depth of 2 to follow hyperlinks found on the seed pages and collect hyperlinks from those pages as well, and so on radiating outwards. The number of pages and hyperlinks can rise very rapidly so it is best to keep this number as low as possible. If a greater reach in collection sites is desired, this could perhaps more efficiently be achieved by revising and adding more seed pages in the first instance. In the example code the type has been set to “ext” (external) for all three seed sites, so as to limit “mapping” of the internal seed web sites and focus on their outward facing connections. Depth of crawl was set to 2.\r\nIt should be noted that all hyperlinks found are collected from scraped pages and used to generate networks. The type and max_depth parameters only apply to the web crawling and scraping activity.\r\nPerforming the Collection\r\nThe hyperlink data can now be collected using the Collect function with the pages parameter. This produces a dataframe that contains the hyperlink URL’s found, pages they were found on and other metadata that can be used to help construct networks.\r\n\r\n\r\nlibrary(magrittr)\r\nlibrary(dplyr)\r\nlibrary(vosonSML)\r\n\r\n# set up as a web collection and collect the hyperlink data using the\r\n# previously defined seed pages\r\nhyperlinks <- Authenticate(\"web\") %>% Collect(pages)\r\n\r\n# Collecting web page hyperlinks...\r\n# *** initial call to get urls - http://vosonlab.net\r\n# * new domain: http://vosonlab.net \r\n# + http://vosonlab.net (10 secs)\r\n# *** end initial call\r\n# *** set depth: 2\r\n# *** loop call to get urls - nrow: 6 depth: 2 max_depth: 2\r\n# * new domain: http://rsss.anu.edu.au \r\n# + http://rsss.anu.edu.au (0.96 secs)\r\n# ...\r\n\r\n# dataframe structure\r\nglimpse(hyperlinks)\r\n# Rows: 1,163\r\n# Columns: 9\r\n# $ url       <chr> \"http://rsss.anu.edu.au\", \"http://rsss.cass.anu.edu.au\", \"ht~\r\n# $ n         <int> 1, 1, 4, 1, 1, 2, 2, 2, 2, 2, 2, 1, 1, 1, 4, 1, 1, 1, 1, 1, ~\r\n# $ page_err  <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, ~\r\n# $ page      <chr> \"http://vosonlab.net\", \"http://vosonlab.net\", \"http://vosonl~\r\n# $ depth     <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ~\r\n# $ max_depth <dbl> 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ~\r\n# $ parse     <df[,6]> <data.frame[26 x 6]>\r\n# $ seed      <chr> \"http://vosonlab.net\", \"http://vosonlab.net\", \"http://vos~\r\n# $ type      <chr> \"ext\", \"ext\", \"ext\", \"ext\", \"ext\", \"ext\", \"ext\", \"ext\", \"ext~\r\n\r\n# number of pages scraped for hyperlinks\r\nnrow(hyperlinks %>% distinct(page))\r\n# [1] 38\r\n\r\n# number of hyperlinks collected\r\nnrow(hyperlinks)\r\n# [1] 1163\r\n\r\n\r\n\r\nA total of 1,163 hyperlinks were collected from 38 pages followed from our 3 seed pages. Using this data, it is now possible to generate hyperlink networks.\r\nNetwork Creation\r\nNetworks\r\nAs with other vosonSML social media, there are two standard types of networks we can create. An activity network that produces a more structural representation of the network where nodes are web pages and edges are the hyperlink references between them, and an actor network that instead groups pages into entities based on their domain names.\r\n\r\n\r\n# generate a hyperlink activity network\r\nactivity_net <- Create(hyperlinks, \"activity\")\r\n\r\n# generate a hyperlink actor network\r\nactor_net <- Create(hyperlinks, \"actor\")\r\n# Generating web actor network...\r\n# Done.\r\n\r\n\r\n\r\nThe output of the network creation is a named list of two dataframes, one for the nodes and the other for the edges or edge list data. The example below shows the actor_net. Note that the edges of the actor network are also aggregated into a weight value and that actors can link to themselves forming self-loops.\r\n\r\n\r\nprint(as_tibble(actor_net$nodes))\r\n# # A tibble: 185 x 2\r\n#   id                              link_id\r\n#   <chr>                             <int>\r\n# 1 accounts.google.com                   1\r\n# 2 alumni.kellogg.northwestern.edu       2\r\n# 3 anu.edu.au                            3\r\n# # ... with 182 more rows\r\n\r\nprint(as_tibble(actor_net$edges))\r\n# # A tibble: 226 x 3\r\n#   from            to              weight\r\n#   <chr>           <chr>            <int>\r\n# 1 rsss.anu.edu.au anu.edu.au           2\r\n# 2 rsss.anu.edu.au rsss.anu.edu.au     36\r\n# 3 rsss.anu.edu.au soundcloud.com       1\r\n# # ... with 223 more rows\r\n\r\n\r\n\r\nPlot a Graph\r\nNow that the network has been generated, we can create a graph and plot it. The Graph function creates an igraph format object that can be directly plotted or adjusted for presentation using igraph plotting parameters.\r\n\r\n\r\nlibrary(igraph)\r\nlibrary(stringr)\r\n\r\nactor_net <- Create(hyperlinks, \"actor\")\r\n\r\n# identify the seed pages and set a node attribute\r\nseed_pages <- pages %>%\r\n  mutate(page = str_remove(page, \"^http[s]?://\"), seed = TRUE)\r\nactor_net$nodes <- actor_net$nodes %>%\r\n  left_join(seed_pages, by = c(\"id\" = \"page\"))\r\n\r\n# create an igraph from the network\r\ng <- actor_net %>% Graph()\r\n\r\n# set node colours\r\nV(g)$color <- ifelse(degree(g, mode = \"in\") > 1, \"yellow\", \"grey\")\r\nV(g)$color[which(V(g)$seed == TRUE)] <- \"dodgerblue3\"\r\n\r\n# set label colours\r\nV(g)$label.color <- \"black\"\r\nV(g)$label.color[which(V(g)$seed == TRUE)] <- \"dodgerblue4\"\r\n\r\n# set labels for seed sites and nodes with an in-degree > 1\r\nV(g)$label <- ifelse((degree(g, mode = \"in\") > 1 | V(g)$seed), V(g)$name, NA)\r\n\r\n# simplify and plot the graph\r\nset.seed(200)\r\ntkplot(simplify(g),\r\n       canvas.width = 1024, canvas.height = 1024,\r\n       layout = layout_with_dh(g),\r\n       vertex.size = 3 + (degree(g, mode = \"in\")*2),\r\n       vertex.label.cex = 1 + log(degree(g, mode = \"in\")),\r\n       edge.arrow.size = 0.4,\r\n       edge.width = 1 + log(E(g)$weight))\r\n\r\n\r\n\r\nFigure 2: Hyperlink network of actorsWe now have a simple graph of the actor hyperlink network. Our seed actors are indicated by blue nodes and sites with an in-degree greater than one indicated in yellow. Node size and label size reflect most linked to nodes or highest in-degree. Perhaps unsurprisingly, social media sites and the institutions at which the seed pages are located feature most prominently in the network, and the graph plot provides us a view of the actors online presence and connections.\r\nThere is much more network visualisation and analysis that could be performed on the vosonSML hyperlink networks and we will be working to add more features such as text analysis and network refinements in our near future releases. In the meantime, we hope you have found this practical introduction to our new tool useful and look forward to your feedback!\r\n\r\n\r\n\r\nAckland, R. 2010. “WWW Hyperlink Networks.” Edited by D. L. Hansen and B. Shneiderman and M. A. Smith. Morgan-Kaufmann.\r\n\r\n\r\n———. 2013. Web Social Science: Concepts, Data and Tools for Social Scientists in the Digital Age. SAGE Publications.\r\n\r\n\r\nAckland, R., and M. O’Neil. 2011. “Online Collective Identity: The Case of the Environmental Movement.” Social Networks 33 (3): 177–90. https://doi.org/10.1016/j.socnet.2011.03.001.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-03-15-hyperlink-networks-with-vosonsml/hyperlink_network.png",
    "last_modified": "2021-06-17T02:02:52+10:00",
    "input_file": {},
    "preview_width": 1024,
    "preview_height": 1025
  },
  {
    "path": "posts/2021-02-11_twitter_vsml_from_rtweet/",
    "title": "Creating Twitter Networks with vosonSML using rtweet Data",
    "description": "Simple guide to collecting data with rtweet and generating networks with vosonSML.",
    "author": [
      {
        "name": "Bryan Gertzel",
        "url": {}
      },
      {
        "name": "Francisca Borquez",
        "url": {}
      }
    ],
    "date": "2021-02-11",
    "categories": [
      "rstats",
      "twitter",
      "vosonSML",
      "rtweet",
      "networks"
    ],
    "contents": "\r\n\r\nContents\r\nIntroduction\r\nAPI Authentication\r\n\r\nTwitter Data Collection with rtweet\r\nSearch Collection\r\nSave the Data\r\n\r\nCreating Networks with vosonSML\r\nRead the Data\r\nPrepare the Data\r\nCreate the Network\r\n\r\n\r\nIntroduction\r\nSocial media platforms are a rich resource for Social Network data. Twitter is a highly popular public platform for social commentary that, like most social media supporting third-party applications, allow software to access and retrieve it’s data via Application Programming Interfaces or API’s. Because of its popularity with individuals and communities around the world, the ready availability of its data, and low barrier for entry, Twitter has become of great interest as a data source for online empirical research.\r\nThere have been many pieces of software developed across programming languages and environments to access the Twitter API. Within the R ecosystem the most comprehensive and well supported of Twitter packages is rtweet developed by Michael Kearney and part of the rOpenSci initiative. The rtweet package provides R functions to both authenticate and collect timelines, tweets and other metadata using Twitter’s v1.1 standard and premium API’s.\r\nThe VOSON Lab develops and maintains the open source R packages vosonSML and VOSONDash. These were created to integrate online data collection, network generation and analysis into a consistent and easy to use work flow across many popular web and social media platforms. For Twitter, the vosonSML package provides an interface to rtweet’s collection features through which tweets can be searched for and retrieved, and then uses this data to produce networks. There may be cases however, such as in the collection of streaming data or analysis of previously collected twitter data where you haven’t used vosonSML’s collection function but instead simply wish to produce vosonSML generated networks from your rtweet data. Because vosonSML uses rtweet this is easily achievable and with minimal R coding.\r\nAPI Authentication\r\nAccessing the Twitter API to collect tweets requires authentication via a Twitter app. There are generally two ways this can be achieved, you can apply for a Twitter Developer account and create your own app (and access keys) or you can authorize another persons app to access the API on your behalf (using their keys). The latter still requires your own Twitter user account but you do not need to go through the Developer application or app creation process. The vosonSML package requires users to create their own app and use their own keys but the rtweet package supports both methods, and you can collect tweets after a simple one-time web authorization step of their embedded rstats2twitter app.\r\nTwitter Data Collection with rtweet\r\nThe following simple example will demonstrate how to use the rtweet package to collect some tweet data using built-in authentication via the rtweet app.\r\nSearch Collection\r\nA fairly standard tweet collection usually involves using the Twitter Search API endpoint to search for past tweets that meet a certain criteria. This can be done with rtweet and the search_tweets function with the criteria set by passing additional parameters. In our example we will direct the API to search and return 100 tweets (n = 100) containing the hashtag #auspol and excluding any retweets (include_rts = FALSE). By default only the most recent tweets within the last 7 days will be returned by the API.\r\n\r\n\r\n\r\n\r\n\r\nlibrary(rtweet)\r\n\r\n# recent tweet search collection\r\nauspol_tweets <- search_tweets(\"#auspol\", n = 100, include_rts = FALSE)\r\n\r\n\r\n\r\nThe first time rtweet collection functions are run they will open a Twitter web page on your default web browser asking permission to authorize rstats2twitter.\r\n\r\n\r\n\r\nFigure 1: rstats2twitter app authorization\r\n\r\n\r\n\r\nIf API authentication and search succeeds then the search_tweets function will return a data frame of tweet data. The data frame will have up to 100 rows, one for each tweet collected and 90 columns for associated tweet metadata:\r\n\r\n\r\nlibrary(tibble)\r\n\r\n# print the first 2 rows\r\nprint(auspol_tweets, n = 2)\r\n# # A tibble: 100 x 90\r\n#   user_id  status_id  created_at          screen_name text      source\r\n#   <chr>    <chr>      <dttm>              <chr>       <chr>     <chr> \r\n# 1 27007685 136400068~ 2021-02-22 23:54:39 ronth~      \"@janeen~ Twitt~\r\n# 2 1359301~ 136400067~ 2021-02-22 23:54:37 Injur~      \"When th~ Twitt~\r\n\r\n\r\n\r\n\r\n\r\nShow additional columns\r\n\r\n# # ... with 98 more rows, and 84 more variables:\r\n# #   display_text_width <dbl>, reply_to_status_id <chr>,\r\n# #   reply_to_user_id <chr>, reply_to_screen_name <chr>,\r\n# #   is_quote <lgl>, is_retweet <lgl>, favorite_count <int>,\r\n# #   retweet_count <int>, quote_count <int>, reply_count <int>,\r\n# #   hashtags <list>, symbols <list>, urls_url <list>,\r\n# #   urls_t.co <list>, urls_expanded_url <list>, media_url <list>,\r\n# #   media_t.co <list>, media_expanded_url <list>, media_type <list>,\r\n# #   ext_media_url <list>, ext_media_t.co <list>,\r\n# #   ext_media_expanded_url <list>, ext_media_type <chr>,\r\n# #   mentions_user_id <list>, mentions_screen_name <list>, lang <chr>,\r\n# #   quoted_status_id <chr>, quoted_text <chr>,\r\n# #   quoted_created_at <dttm>, quoted_source <chr>,\r\n# #   quoted_favorite_count <int>, quoted_retweet_count <int>,\r\n# #   quoted_user_id <chr>, quoted_screen_name <chr>,\r\n# #   quoted_name <chr>, quoted_followers_count <int>,\r\n# #   quoted_friends_count <int>, quoted_statuses_count <int>,\r\n# #   quoted_location <chr>, quoted_description <chr>,\r\n# #   quoted_verified <lgl>, retweet_status_id <chr>,\r\n# #   retweet_text <chr>, retweet_created_at <dttm>,\r\n# #   retweet_source <chr>, retweet_favorite_count <int>,\r\n# #   retweet_retweet_count <int>, retweet_user_id <chr>,\r\n# #   retweet_screen_name <chr>, retweet_name <chr>,\r\n# #   retweet_followers_count <int>, retweet_friends_count <int>,\r\n# #   retweet_statuses_count <int>, retweet_location <chr>,\r\n# #   retweet_description <chr>, retweet_verified <lgl>,\r\n# #   place_url <chr>, place_name <chr>, place_full_name <chr>,\r\n# #   place_type <chr>, country <chr>, country_code <chr>,\r\n# #   geo_coords <list>, coords_coords <list>, bbox_coords <list>,\r\n# #   status_url <chr>, name <chr>, location <chr>, description <chr>,\r\n# #   url <chr>, protected <lgl>, followers_count <int>,\r\n# #   friends_count <int>, listed_count <int>, statuses_count <int>,\r\n# #   favourites_count <int>, account_created_at <dttm>,\r\n# #   verified <lgl>, profile_url <chr>, profile_expanded_url <chr>,\r\n# #   account_lang <lgl>, profile_banner_url <chr>,\r\n# #   profile_background_url <chr>, profile_image_url <chr>\r\n\r\n\r\n\r\nThis contains all of the data necessary for vosonSML to construct Twitter networks.\r\nSave the Data\r\nThere are a few methods of saving data depending on where and how it will be used. Two common methods are to use a text-based file format such as a CSV, or alternatively if the data will be used within R we can save the dataframe object to a binary compressed RDS (R data object) file using saveRDS instead. Conveniently, the rtweet package has a method to save Twitter data to file in CSV format with the write_as_csv function that takes care of Twitter nested data and conversion issues, and saving an RDS file is also very easy as follows.\r\n\r\n\r\n# save data using rtweet write csv\r\nwrite_as_csv(auspol_tweets, \"auspol_tweets.csv\")\r\n\r\n# save data to file as an R data object\r\nsaveRDS(auspol_tweets, \"auspol_tweets.rds\")\r\n\r\n\r\n\r\nCreating Networks with vosonSML\r\nRead the Data\r\nIf the data was saved to file with the rtweet function write_as_csv it can be read again using read_twitter_csv or readRDS if from an RDS file.\r\n\r\n\r\nauspol_tweets <- read_twitter_csv(\"auspol_tweets.csv\")\r\n\r\nauspol_tweets <- readRDS(\"auspol_tweets.rds\")\r\n\r\n\r\n\r\nPrepare the Data\r\nFor vosonSML to recognize the previously collected data as a Twitter data source and be able to internally route it to the appropriate network functions a minor change needs to be made to the data frame first. This involves adding two attributes datasource and twitter to the class list of the auspol_tweets data frame object as follows:\r\n\r\n\r\n# original class list\r\nclass(auspol_tweets)\r\n\r\n\r\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\r\n\r\n# add to the class list\r\nclass(auspol_tweets) <- append(c(\"datasource\", \"twitter\"), class(auspol_tweets))\r\n\r\n# modified class list\r\nclass(auspol_tweets)\r\n\r\n\r\n[1] \"datasource\" \"twitter\"    \"tbl_df\"     \"tbl\"        \"data.frame\"\r\n\r\nThe order of classes is important and for the data frame to be compatible with dplyr - a very common data manipulation package in R, and subsequently usable in the tidyverse and vosonSML, then the new attributes need to be added to the beginning of the list.\r\nFor versions of vosonSML more recent than 0.29.13 this can now all be managed by using the ImportData function. This method is preferable as it is easier, works for both files and data frames, and will support any future updates to vosonSML without breaking your code.\r\n\r\n\r\n\r\n\r\n\r\nlibrary(vosonSML)\r\n\r\n# use the import data function\r\nauspol_tweets <- ImportData(auspol_tweets, \"twitter\")\r\n\r\n\r\n\r\nPlease note that modifying data frame attributes or importing data is only required for rtweet data and not a necessary step for Twitter data collected using the vosonSML Twitter Collect function.\r\nObject classes in R are a more advanced topic and not required knowledge to use vosonSML but if you would like to learn more a good introduction can be found in the Object-oriented programming chapter of Advanced R by Hadley Wickham.\r\nCreate the Network\r\nThe tweet data can now be used to create the nodes and edges network data, and a graph by using the vosonSML Create and Graph functions:\r\n\r\n\r\n# create the network data\r\nauspol_actor_network <- Create(auspol_tweets, \"actor\")\r\n\r\n\r\nGenerating twitter actor network...\r\n-------------------------\r\ncollected tweets | 100\r\nretweets         | 0 \r\nquoting others   | 18\r\nmentions         | 28\r\nreply mentions   | 17\r\nreplies          | 28\r\nself-loops       | 42\r\nnodes            | 149\r\nedges            | 133\r\n-------------------------\r\nDone.\r\n\r\n\r\n\r\n# create an igraph\r\nauspol_actor_graph <- Graph(auspol_actor_network)\r\n\r\n\r\nCreating igraph network graph...Done.\r\n\r\nThat’s all there is to it, and now the resulting igraph network can be plotted.\r\n\r\n\r\nlibrary(igraph)\r\n\r\n# set plot margins\r\npar(mar = c(0, 0, 0, 0))\r\n\r\n# auspol actor network with fruchterman-reingold layout\r\nplot(auspol_actor_graph, layout = layout_with_fr(auspol_actor_graph),\r\n     vertex.label = NA, vertex.size = 6, edge.arrow.size = 0.4)\r\n\r\n\r\n\r\n\r\nFigure 2: Actor network graph for collected #auspol tweets\r\n\r\n\r\n\r\nFor further information about rtweet, its features and how to use it to collect twitter data please refer to the package site and introductory rtweet vignette. For creating different types of networks such as the activity, 2-mode and semantic types with vosonSML see the package documentation and introductory vosonSML vignette.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-02-11_twitter_vsml_from_rtweet/rtweet_logo_preview.png",
    "last_modified": "2021-06-17T02:02:52+10:00",
    "input_file": {},
    "preview_width": 432,
    "preview_height": 499
  },
  {
    "path": "posts/2021-02-05_welcome/",
    "title": "Welcome to the VOSON Lab Code Blog",
    "description": "The code blog is a space to share tools, methods, tips, examples and code. A place to collect data, construct and analyze online networks.",
    "author": [
      {
        "name": "VOSON Lab",
        "url": "http://vosonlab.net/"
      },
      {
        "name": "",
        "url": {}
      }
    ],
    "date": "2021-02-04",
    "categories": [
      "Rstats",
      "Social Network Analysis",
      "Computational Social Methods"
    ],
    "contents": "\r\nWelcome to the VOSON Lab Code Blog! We have created this space to share methods, tips, examples and code. It’s also a place where we will demonstrate constructing and analyzing networks from various API and other online data sources.\r\nMost of our posts will cover techniques around the tools we have developed at the Lab: vosonSML, VOSONDash and voson.tcn, which are available on both CRAN and GitHub. But we also plan to use this space to cover other complementary R packages and open-source software, such as fantastic R packages within the tidyverse, RStudio’s shiny for web apps, and visualization tools such as igraph and Gephi.\r\nVOSON Lab R Packages - Hex stickersVOSON Lab Open Source Tools\r\nvosonSML is a R package for social media data collection (currently twitter, youtube, and reddit), hyperlink collection and network generation. VOSONDash is a Shiny app that integrates tools for visualizing and manipulating network graphs, performing network and text analysis, as well as an interface for collecting data with vosonSML.\r\nMore information on these packages, their development and code can be found on our vosonSML, VOSONDash and voson.tcn github pages.\r\nWe also have some other guides for using the packages. Check the vosonSML Vignette and the VOSON Dash Userguide for some practical examples and feature reference.\r\nWe hope you find this content useful!\r\nThe VOSON Lab team.\r\nVirtual Observatory for the Study of Online Networks VOSON Lab, School of Sociology, The Australian National University.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-02-05_welcome/square-cards.png",
    "last_modified": "2021-06-17T02:02:52+10:00",
    "input_file": {},
    "preview_width": 800,
    "preview_height": 640
  }
]

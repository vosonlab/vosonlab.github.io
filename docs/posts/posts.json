[
  {
    "path": "posts/2021-03-23-twitter-conversation-networks/",
    "title": "Twitter Conversation Networks",
    "description": "Getting started with the voson.tcn package.",
    "author": [
      {
        "name": "Bryan Gertzel",
        "url": {}
      },
      {
        "name": "Francisca Borquez",
        "url": {}
      }
    ],
    "date": "2021-03-23",
    "categories": [
      "rstats",
      "twitter",
      "conversations",
      "voson.tcn",
      "networks"
    ],
    "contents": "\r\n\r\nContents\r\nTwitter Developer Access\r\nInstallation\r\nAuthentication\r\nCollection\r\nNetwork Creation\r\nActivity Network\r\nActor Network\r\n\r\nPlot Graphs\r\nActivity Network\r\nActor Network\r\n\r\n\r\nThe VOSON Lab has recently published to GitHub a new open source R package called {voson.tcn}. The package uses the Early-Access Twitter API v2, to collect tweets belonging to specified threaded conversations and generate networks. The Twitter API v2 provides a new tweet identifier: the conversation ID, that is common to all tweets that are part of a conversation, and can be searched for using the API search endpoints. Identifiers and associated metadata for referenced tweets can also be collected in the same search for conversation tweets, allowing us to construct twitter networks with tweet and user metadata whilst minimising API requests.\r\nTwitter Developer Access\r\nThe {voson.tcn} package requires developer app authentication keys or tokens to access the Twitter API v2. These can be either the Access token & secret of an app or its Bearer token.\r\nTo obtain these credentials and use the early-access API you will need to have or apply for a Twitter Developer Account, as well as have activated the new Developer Portal. Once approved you will then need to create a development project, which is the new management container for apps, and either create a new app or associate one of your existing apps with it.\r\nThere are currently two project types available that correspond to Twitter’s developer product tracks, a standard and academic type. Academic projects are only available to researchers who have completed and have had their application for the academic research track approved for non-commercial research purposes. Standard projects are for more general use, including hobby and educational purposes. The project type features differ in their API access and caps; standard projects can only access the 7-day recent search endpoint whereas an academic project can access the full-archive search endpoint for historical tweets. There are also rate-limits and monthly tweet caps for API v2 search endpoints. At the time of writing, the caps are 500k and 10 million tweets that can be retrieved per month for the standard and academic track projects respectively.\r\nPlease note that there are also terms of use and restricted use cases that should be considered before applying for access or using the Twitter API.\r\nInstallation\r\nThe {voson.tcn} R package is in development and currently only available on GitHub. It can be installed as follows:\r\n\r\n\r\n# use the remotes package to install the latest dev version of voson.tcn from github\r\nlibrary(remotes)\r\ninstall_github(\"vosonlab/voson.tcn\")\r\n\r\n# Downloading GitHub repo vosonlab/voson.tcn@HEAD\r\n# v  checking for file\r\n# -  preparing 'voson.tcn':\r\n# v  checking DESCRIPTION meta-information ... \r\n# -  checking for LF line-endings in source and make files and shell scripts\r\n# -  checking for empty or unneeded directories\r\n# -  building 'voson.tcn_0.1.6.9000.tar.gz'\r\n#    \r\n# * installing *source* package 'voson.tcn' ...\r\n# ...\r\n# * DONE (voson.tcn)\r\n# Making 'packages.html' ... done\r\n\r\n\r\n\r\nAuthentication\r\nThe {voson.tcn} package only supports app based authentication using OAuth2.0 tokens which are also known as bearer tokens. We will likely support user based tokens in the future, however at this stage they do not offer any advantages as they have lower rate-limits and we are not using any private metadata of which they permit access (such as user-visible only metrics).\r\nThe token can be created using either your apps access token & secret (also known as consumer keys) or its bearer token. It is recommended that this token is saved for future use; there is no need to perform this step more than once as the token will not change unless it is invalidated or you regenerate keys on the developer portal.\r\n\r\n\r\nlibrary(voson.tcn)\r\n\r\n# retrieves a bearer token from the API using the apps consumer keys\r\ntoken <- tcn_token(consumer_key = \"xxxxxxxx\",\r\n                   consumer_secret = \"xxxxxxxx\")\r\n\r\n# alternatively if you have a bearer token already you can assign it directly\r\ntoken <- list(bearer = \"xxxxxxxxxxxx\")\r\n\r\n# if you save the token to file this step only needs to be done once\r\nsaveRDS(token, \"~/.tcn_token\")\r\n\r\n\r\n\r\nCollection\r\nCollecting conversation tweets requires the tweet ID or URL of a tweet that belongs to each threaded conversation that you are interested in. These are passed to the {voson.tcn} collect function tcn_threads as a vector or list. Conversation IDs will be tracked by this function to avoid duplication and, if tweet IDs are found to belong to a conversation that has already been collected on, then that conversation will be skipped.\r\nIn the following example, we are collecting the tweets for a threaded conversation belonging to a public lockdown announcement following a COVID-19 outbreak in Brisbane, Queensland, Australia, that took place on March, 29, 2021. The tweet URL or ID (number following the status in the URL) can be passed directly to the collection function.\r\nFigure 1: Public announcement tweet regarding a COVID-19 lockdown of Brisbane, from the Queensland Premier\r\n\r\n# read token from file\r\ntoken <- readRDS(\"~/.tcn_token\")\r\n\r\n# collect the conversation thread tweets for supplied ids           \r\ntweets <- tcn_threads(\"https://twitter.com/AnnastaciaMP/status/1376311897624956929\", token)\r\n\r\n\r\n\r\nWhen completed, a list of named dataframes will be returned, with tweets containing all of the tweets and their metadata, and users containing all of the referenced users in the tweets and their metadata. In our example, 286 tweets were collected with 180 associated users public metadata.\r\nNote that the collection of a threaded tweet conversation is a snapshot of the state of the conversation at a point in time. Metrics and networks produced from our data will not completely match subsequent collections of the same conversation, as it will have likely cumulatively expanded over time.\r\n\r\n\r\n# collected tweets\r\nprint(tweets$tweets, n = 3)\r\n# # A tibble: 286 x 14\r\n#   in_reply_to_user~ conversation_id  source  author_id  tweet_id  ref_tweet_type\r\n#   <chr>             <chr>            <chr>   <chr>      <chr>     <chr>         \r\n# 1 15999~            137631189762495~ Twitte~ 134852208~ 13763373~ replied_to    \r\n# 2 1142316897985163~ 137631189762495~ Twitte~ 126908387~ 13763373~ replied_to    \r\n# 3 25683~            137631189762495~ Twitte~ 137503906~ 13763371~ replied_to    \r\n# # ... with 283 more rows, and 8 more variables: ref_tweet_id <chr>, text <chr>,\r\n# #   created_at <chr>, includes <chr>, public_metrics.retweet_count <int>,\r\n# #   public_metrics.reply_count <int>, public_metrics.like_count <int>,\r\n# #   public_metrics.quote_count <int>\r\n\r\n# users metadata\r\nprint(tweets$users, n = 3)\r\n# # A tibble: 180 x 12\r\n#   profile.username profile.created_~ profile.profile_~ user_id profile.descript~\r\n#   <chr>            <chr>             <chr>             <chr>   <chr>            \r\n# 1 MSMW~            2013-03-30T06:48~ https://pbs.twim~ 131592~ \"Fact checking i~\r\n# 2 bpro~            2012-12-04T02:07~ https://pbs.twim~ 987844~ \"Only way to get~\r\n# 3 scre~            2009-10-22T22:56~ https://pbs.twim~ 844463~ \"I'm a  creative~\r\n# # ... with 177 more rows, and 7 more variables: profile.name <chr>,\r\n# #   profile.verified <lgl>, profile.location <chr>,\r\n# #   profile.public_metrics.followers_count <int>,\r\n# #   profile.public_metrics.following_count <int>,\r\n# #   profile.public_metrics.tweet_count <int>,\r\n# #   profile.public_metrics.listed_count <int>\r\n\r\n\r\n\r\nIf interested in text analysis, the tweet text can be found in the text column of the tweets dataframe and user profile descriptions in profile.description of the users dataframe.\r\nPublic metrics for tweets and users are found in dataframe columns prefixed, with public_metrics and profile.public_metrics respectively.\r\n\r\n\r\nlibrary(dplyr)\r\n\r\nnames(select(tweets$tweets, starts_with(\"public_metrics\")))\r\n# [1] \"public_metrics.retweet_count\" \"public_metrics.reply_count\"\r\n# [3] \"public_metrics.like_count\" \"public_metrics.quote_count\"\r\n\r\nnames(select(tweets$users, starts_with(\"profile.public_metrics\")))\r\n# [1] \"profile.public_metrics.followers_count\"\r\n# [2] \"profile.public_metrics.following_count\"\r\n# [3] \"profile.public_metrics.tweet_count\"\r\n# [4] \"profile.public_metrics.listed_count\"\r\n\r\n\r\n\r\nNetwork Creation\r\nThere are two types of networks that can be generated using {voson.tcn}: activity and actor network. These differ by the type of node and resulting structure of the networks.\r\nActivity Network\r\nAn activity network is a representation of the conversation as seen on Twitter: nodes are tweets and the edges are how they are related. Tweets (or nodes) are identified by their unique identifier Tweet ID (formerly Status ID). In a Twitter threaded conversation, there are only two types of connections or edges between tweets and these are replied_to and quoted.\r\nReplies are made when a user chooses the reply option and publishes a tweet response to the tweet they are replying to. Quotes are a little different in that the user has included a link to or quoted another tweet in the body of their tweet. In Twitter conversation networks, it is common to quote a tweet as part of a reply tweet, generating in the activity network a replied_to and quoted edge from the same node.\r\n\r\n\r\n# generate an activity network\r\nactivity_net <- tcn_network(tweets, \"activity\")\r\n\r\n# number of nodes\r\nnrow(activity_net$nodes)\r\n# [1] 279\r\n\r\n# number of edges\r\nprint(activity_net$edges, n = 3)\r\n# # A tibble: 281 x 3\r\n#   from                to                  type      \r\n#   <chr>               <chr>               <chr>     \r\n# 1 1376337359126495232 1376328523518898176 replied_to\r\n# 2 1376337350163267584 1376325216658317315 replied_to\r\n# 3 1376337128016113665 1376311897624956929 replied_to\r\n# # ... with 278 more rows\r\n\r\nunique(activity_net$edges$type)\r\n# [1] \"replied_to\" \"quoted\"\r\n\r\n\r\n\r\nActor Network\r\nAn actor network represents the interactions between Twitter users in the conversation: nodes are the users and edges are their connections. As in the activity network, edges are either a reply or a quote but edges represent the classification of a tweet connecting users rather than the activity. Users (or nodes) are identified by their unique Twitter User ID. In the actor network, interactions between users are more apparent and can be measured by the frequency (and direction) of edges between them.\r\n\r\n\r\n# generate an actor network\r\nactor_net <- tcn_network(tweets, \"actor\")\r\n\r\n# number of nodes or actors\r\nnrow(actor_net$nodes)\r\n# [1] 180\r\n\r\nprint(actor_net$edges, n = 3)\r\n# # A tibble: 286 x 6\r\n#   from      to        type  tweet_id     created_at    text                     \r\n#   <chr>     <chr>     <chr> <chr>        <chr>         <chr>                    \r\n# 1 13485220~ 15999128~ reply 13763373591~ 2021-03-29T0~ \"@Ther~ @Scott~\r\n# 2 12690838~ 11423168~ reply 13763373501~ 2021-03-29T0~ \"@Luke~ @Annas~\r\n# 3 13750390~ 25683344~ reply 13763371280~ 2021-03-29T0~ \"@AnnastaciaMP You do un~\r\n# # ... with 283 more rows\r\n\r\nunique(actor_net$edges$type)\r\n# [1] \"reply\" \"quote\" \"tweet\"\r\n\r\n\r\n\r\nNote that in the actor network there is an additional edge type: tweet, which is assigned to a self-loop edge created for the thread’s initial tweet. This is a technique used to retain the initial tweet’s metadata as edge attributes comparable to other edges in the network.\r\nThe initial conversation tweet would not usually be included in the edge list, as the initial conversation tweet is not directed at another user, and hence no edge to attach metadata.For example, this allows the text of the initial tweet to be included in any actor network tweet text analysis. It would not usually be included in the edge list as the initial conversation tweet is not directed at another user, and hence no edge to attach metadata is naturally found in this type of network.\r\nPlot Graphs\r\nActivity Network\r\nVisualisation of the activity network produced with {igraph}.\r\n\r\n\r\nlibrary(igraph)\r\nlibrary(RColorBrewer)\r\n\r\ng <- graph_from_data_frame(activity_net$edges, vertices = activity_net$nodes)\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\n# change likes to log scale\r\nlike_count <- V(g)$public_metrics.like_count\r\nlike_count[is.na(like_count)] <- 0\r\nln_like_count <- log(like_count)\r\nln_like_count[!is.finite(ln_like_count)] <- 0\r\n\r\n# set node size based on likes, min size 4\r\nsize <- ln_like_count * 2\r\nV(g)$size <- ifelse(size > 0, size + 8, 4)\r\n\r\n# set node label if number of likes >= 2\r\nV(g)$label <- ifelse(like_count >= 2, like_count, NA)\r\nV(g)$label.color <- \"black\"\r\n\r\n# set node colors based on number of retweets low to high is yellow to green\r\n# set tweets with no retweets to grey\r\nrt_count <- V(g)$public_metrics.retweet_count\r\nrt_count[is.na(rt_count)] <- 0\r\ncols <- colorRampPalette(c(\"yellow1\", \"green3\"))\r\ncols <- cols(max(rt_count) + 1)\r\nV(g)$color <- cols[rt_count + 1]\r\nV(g)$color[which(rt_count < 1)] <- \"lightgrey\"\r\n\r\n# set edge color to orange if tweet quoted another tweet\r\nE(g)$color <- ifelse(E(g)$type == \"quoted\", \"orange\", \"grey\")\r\n\r\n\r\n\r\n\r\n\r\n# plot the graph using fruchterman reingold layout\r\nset.seed(200)\r\ntkplot(g,\r\n       canvas.width = 1024, canvas.height = 1024,\r\n       layout = layout_with_fr(g),\r\n       edge.arrow.size = 0.5,\r\n       edge.width = 2)\r\n\r\n\r\n\r\nFigure 2: Conversation activity network - Node size and label represent number of tweet likes, color scale is indicating low to high number of retweets (yellow to green). Orange coloured edges are quoting linked tweet.{voson.tcn} collects tweets that are all linked to each other via the conversation ID. This means that in a network generated from this data, such as the activity network, all of the nodes (tweets) should be connected in a single component per conversation ID. If multiple conversation IDs were collected on then, it is also possible to have one component because of quote edges. These edges joining conversations occur when a tweet in one conversation has quoted a tweet in another that you have collected.\r\nIn the example activity network above, there are two components even though only one conversation ID was collected on. Multiple components are usually due to a missing conversation tweet not able to be retrieved from the API and producing a broken reply chain. This can often occur if, for example, a tweet has been deleted, or the tweet or user flagged or suspended in some way restricting public availability.\r\nActor Network\r\nVisualisation of the actor network produced with {igraph}.\r\n\r\n\r\nlibrary(dplyr)\r\nlibrary(magrittr)\r\nlibrary(stringr)\r\n\r\nregex_ic <- function(x) regex(x, ignore_case = TRUE)\r\n\r\n# best effort set the node colour attribute based on presence of city, state,\r\n# or country in the actors profile location field\r\nnodes <- actor_net$nodes %>%\r\n  mutate(color = case_when(\r\n    str_detect(profile.location, regex_ic(\"brisbane|bris\")) ~ \"orange\",\r\n    str_detect(profile.location, regex_ic(\"queensland|qld\")) ~ \"gold\",\r\n    str_detect(profile.location, regex_ic(\"australia|aus\")) ~ \"yellow\",\r\n    TRUE ~ \"lightgrey\"))\r\n\r\ng2 <- graph_from_data_frame(actor_net$edges, vertices = nodes)\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\n# the following code de-clutters the actor network by removing some nodes\r\n# that are not part of conversation chains and are stand-alone replies to\r\n# the initial thread tweet\r\n\r\n# get the author of the initial thread tweet using the conversation id\r\nconversation_ids <- c(\"1376311897624956929\")\r\nthread_authors <- activity_net$nodes %>%\r\n  filter(tweet_id %in% conversation_ids) %>% select(user_id)\r\n\r\n# remove actors replying to the initial tweet that have a degree of 1\r\nthread_spokes <- unlist(\r\n  incident_edges(g2, V(g2)[which(V(g2)$name %in% thread_authors$user_id)],\r\n                 \"in\"))\r\nspokes_tail_nodes <- V(g2)[tail_of(g2, thread_spokes)]$name\r\ng2 <- delete_vertices(g2, degree(g2) == 1 & V(g2)$name %in% spokes_tail_nodes)\r\n\r\n# convert the graph to undirected\r\n# simplify the graph and collapse edges into an edge weight value\r\nE(g2)$weight <- 1\r\ng2 <- as.undirected(simplify(g2, edge.attr.comb = list(weight = \"sum\")))\r\ng2 <- delete_vertices(g2, degree(g2) == 0)\r\n\r\n# use edge weight for graph edge width\r\nE(g2)$width <- ifelse(E(g2)$weight > 1, E(g2)$weight + 1, 1)\r\n\r\n# use the actors followers count for node size \r\nfollowers_count <- log(V(g2)$profile.public_metrics.followers_count)\r\nfollowers_count[!is.finite(followers_count)] <- 0\r\nsize <- followers_count * 3\r\nV(g2)$size <- ifelse(size < 6, 6, size)\r\nV(g2)$label <- ifelse(followers_count > 0,\r\n                      V(g2)$profile.public_metrics.followers_count, NA)\r\n\r\n\r\n\r\n\r\n\r\n# plot the graph using automatically chosen layout\r\nset.seed(201)\r\ntkplot(g2,\r\n       canvas.width = 1024, canvas.height = 1024,\r\n       layout = layout_nicely(g2),\r\n       vertex.label.cex = 0.8,\r\n       vertex.label.color = \"black\")\r\n\r\n\r\n\r\nFigure 3: Conversation actor network - Node size and label represent users follower counts. Node color indicates user self-reported location. Edge width represents number of collapsed edges.\r\n\r\n\r\n",
    "preview": "posts/2021-03-23-twitter-conversation-networks/activity_network.png",
    "last_modified": "2021-04-01T15:13:03+11:00",
    "input_file": "twitter-conversation-networks.utf8.md",
    "preview_width": 1024,
    "preview_height": 1025
  },
  {
    "path": "posts/2021-03-15-hyperlink-networks-with-vosonsml/",
    "title": "Hyperlink Networks with vosonSML",
    "description": "An introduction to creating hyperlink networks with vosonSML.",
    "author": [
      {
        "name": "Bryan Gertzel",
        "url": {}
      },
      {
        "name": "Francisca Borquez",
        "url": {}
      }
    ],
    "date": "2021-03-15",
    "categories": [
      "rstats",
      "hyperlinks",
      "vosonSML",
      "networks"
    ],
    "contents": "\r\n\r\nContents\r\nIntroduction\r\nInstallation\r\n\r\nHyperlink Collection\r\nSetting Up\r\nPerforming the Collection\r\n\r\nNetwork Creation\r\nNetworks\r\nPlot a Graph\r\n\r\n\r\nThe VOSON software for hyperlink collection and analysis was an early research output of the VOSON Lab (Ackland 2010). It addressed a need for tools that could help study online social networks, even before the rise of social media, and assisted researchers gain insights into important phenomena such as networks around issue spheres and online social movements [see (Ackland and O’Neil 2011) and (Ackland 2013)]. After many years and many iterations since its inception in 2004, the VOSON Lab is happy to reintroduce the canonical VOSON hyperlink collection software as part of our R open-source toolkit for social media collection: vosonSML.\r\nThis simple guide will demonstrate how to use the new features of the vosonSML package to perform a hyperlink collection and generate networks for analysis.\r\nIntroduction\r\nThe vosonSML hyperlink collection and network creation works similarly to the 3-step process we use with other social media sources: the Authenticate, Collect and Create verb functions. The Authenticate function is first called with the parameter “web” to identify and set up the context for subsequent operations, but it does not require any further credentials in this implementation. vosonSML uses standard web crawling and text-based page scraping techniques to discover hyperlinks and, as such, there is no need to access any restricted data API’s as we commonly do with social media.\r\nInstallation\r\nThe new hyperlink collection and network features are currently available in the development version of vosonSML on GitHub, and are to soon be released on CRAN. The development version can be installed as follows:\r\n\r\n\r\n# use the remotes package to install the latest dev version of vosonSML from github\r\nlibrary(remotes)\r\ninstall_github(\"vosonlab/vosonsml\")\r\n\r\n# Downloading GitHub repo vosonlab/vosonsml@HEAD\r\n# √  checking for file\r\n# -  preparing 'vosonSML':\r\n# √  checking DESCRIPTION meta-information ... \r\n# -  checking for LF line-endings in source and make files and shell scripts\r\n# -  checking for empty or unneeded directories\r\n# -  building 'vosonSML_0.30.00.9000.tar.gz'\r\n#    \r\n# * installing *source* package 'vosonSML' ...\r\n# ...\r\n# * DONE (vosonSML)\r\n# Making 'packages.html' ... done\r\n\r\n\r\n\r\nHyperlink Collection\r\nSetting Up\r\nThe web sites or pages to collect hyperlinks from are specified and input to the Collect function in a dataframe. As there are page specific options that can be used, this format helps us to organise and set the request parameters. The URL’s set in the dataframe for the page column are called ‘seed pages’ and are the starting points for web crawling. Although not explicitly indicated in the URL’s, the seed pages are actually the landing pages or “index” pages of the web sites and a page name can be specified if known or desired.\r\n\r\n\r\n# set sites as seed pages and set each for external crawl with a max depth\r\npages <- data.frame(page = c(\"http://vosonlab.net\",\r\n                             \"https://www.oii.ox.ac.uk\",\r\n                             \"https://sonic.northwestern.edu\"),\r\n                    type = c(\"ext\", \"ext\", \"ext\"),\r\n                    max_depth = c(2, 2, 2))\r\n\r\n\r\n\r\nThe example above shows seed pages with some additional per-seed parameters that are used to control the web crawling. The type parameter can be set to a value of either int, ext or all, which correspond to following only internal, external or following all hyperlinks found on a seeded web page and subsequent pages discovered from that particular seed. How a hyperlink is classified is determined by the seed domain name, for example, if the seed page is https://vosonlab.net a type of ext will follow hyperlinks from that page that do not have a domain name of “vosonlab.net.” A type of int will follow only hyperlinks that match a domain of “vosonlab.net,” and a type of all will follow all hyperlinks found irrespective of their domain. The final parameter max_depth refers to how many levels of pages to follow from the seed page. In the diagram below, the green dots are pages scraped by the web crawler and the blue dots links are the hyperlinks collected from them for a max depth of 1,2 and 3.\r\nFigure 1: Scope of hyperlinks collected using the max_depth parameterAs can be seen, a max depth of 1 directs the crawler to scrape and collect hyperlinks from only seed pages, a max depth of 2 to follow hyperlinks found on the seed pages and collect hyperlinks from those pages as well, and so on radiating outwards. The number of pages and hyperlinks can rise very rapidly so it is best to keep this number as low as possible. If a greater reach in collection sites is desired, this could perhaps more efficiently be achieved by revising and adding more seed pages in the first instance. In the example code the type has been set to “ext” (external) for all three seed sites, so as to limit “mapping” of the internal seed web sites and focus on their outward facing connections. Depth of crawl was set to 2.\r\nIt should be noted that all hyperlinks found are collected from scraped pages and used to generate networks. The type and max_depth parameters only apply to the web crawling and scraping activity.\r\nPerforming the Collection\r\nThe hyperlink data can now be collected using the Collect function with the pages parameter. This produces a dataframe that contains the hyperlink URL’s found, pages they were found on and other metadata that can be used to help construct networks.\r\n\r\n\r\nlibrary(magrittr)\r\nlibrary(dplyr)\r\nlibrary(vosonSML)\r\n\r\n# set up as a web collection and collect the hyperlink data using the\r\n# previously defined seed pages\r\nhyperlinks <- Authenticate(\"web\") %>% Collect(pages)\r\n\r\n# Collecting web page hyperlinks...\r\n# *** initial call to get urls - http://vosonlab.net\r\n# * new domain: http://vosonlab.net \r\n# + http://vosonlab.net (10 secs)\r\n# *** end initial call\r\n# *** set depth: 2\r\n# *** loop call to get urls - nrow: 6 depth: 2 max_depth: 2\r\n# * new domain: http://rsss.anu.edu.au \r\n# + http://rsss.anu.edu.au (0.96 secs)\r\n# ...\r\n\r\n# dataframe structure\r\nglimpse(hyperlinks)\r\n# Rows: 1,163\r\n# Columns: 9\r\n# $ url       <chr> \"http://rsss.anu.edu.au\", \"http://rsss.cass.anu.edu.au\", \"ht~\r\n# $ n         <int> 1, 1, 4, 1, 1, 2, 2, 2, 2, 2, 2, 1, 1, 1, 4, 1, 1, 1, 1, 1, ~\r\n# $ page_err  <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, ~\r\n# $ page      <chr> \"http://vosonlab.net\", \"http://vosonlab.net\", \"http://vosonl~\r\n# $ depth     <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ~\r\n# $ max_depth <dbl> 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ~\r\n# $ parse     <df[,6]> <data.frame[26 x 6]>\r\n# $ seed      <chr> \"http://vosonlab.net\", \"http://vosonlab.net\", \"http://vos~\r\n# $ type      <chr> \"ext\", \"ext\", \"ext\", \"ext\", \"ext\", \"ext\", \"ext\", \"ext\", \"ext~\r\n\r\n# number of pages scraped for hyperlinks\r\nnrow(hyperlinks %>% distinct(page))\r\n# [1] 38\r\n\r\n# number of hyperlinks collected\r\nnrow(hyperlinks)\r\n# [1] 1163\r\n\r\n\r\n\r\nA total of 1,163 hyperlinks were collected from 38 pages followed from our 3 seed pages. Using this data, it is now possible to generate hyperlink networks.\r\nNetwork Creation\r\nNetworks\r\nAs with other vosonSML social media, there are two standard types of networks we can create. An activity network that produces a more structural representation of the network where nodes are web pages and edges are the hyperlink references between them, and an actor network that instead groups pages into entities based on their domain names.\r\n\r\n\r\n# generate a hyperlink activity network\r\nactivity_net <- Create(hyperlinks, \"activity\")\r\n\r\n# generate a hyperlink actor network\r\nactor_net <- Create(hyperlinks, \"actor\")\r\n# Generating web actor network...\r\n# Done.\r\n\r\n\r\n\r\nThe output of the network creation is a named list of two dataframes, one for the nodes and the other for the edges or edge list data. The example below shows the actor_net. Note that the edges of the actor network are also aggregated into a weight value and that actors can link to themselves forming self-loops.\r\n\r\n\r\nprint(as_tibble(actor_net$nodes))\r\n# # A tibble: 185 x 2\r\n#   id                              link_id\r\n#   <chr>                             <int>\r\n# 1 accounts.google.com                   1\r\n# 2 alumni.kellogg.northwestern.edu       2\r\n# 3 anu.edu.au                            3\r\n# # ... with 182 more rows\r\n\r\nprint(as_tibble(actor_net$edges))\r\n# # A tibble: 226 x 3\r\n#   from            to              weight\r\n#   <chr>           <chr>            <int>\r\n# 1 rsss.anu.edu.au anu.edu.au           2\r\n# 2 rsss.anu.edu.au rsss.anu.edu.au     36\r\n# 3 rsss.anu.edu.au soundcloud.com       1\r\n# # ... with 223 more rows\r\n\r\n\r\n\r\nPlot a Graph\r\nNow that the network has been generated, we can create a graph and plot it. The Graph function creates an igraph format object that can be directly plotted or adjusted for presentation using igraph plotting parameters.\r\n\r\n\r\nlibrary(igraph)\r\nlibrary(stringr)\r\n\r\nactor_net <- Create(hyperlinks, \"actor\")\r\n\r\n# identify the seed pages and set a node attribute\r\nseed_pages <- pages %>%\r\n  mutate(page = str_remove(page, \"^http[s]?://\"), seed = TRUE)\r\nactor_net$nodes <- actor_net$nodes %>%\r\n  left_join(seed_pages, by = c(\"id\" = \"page\"))\r\n\r\n# create an igraph from the network\r\ng <- actor_net %>% Graph()\r\n\r\n# set node colours\r\nV(g)$color <- ifelse(degree(g, mode = \"in\") > 1, \"yellow\", \"grey\")\r\nV(g)$color[which(V(g)$seed == TRUE)] <- \"dodgerblue3\"\r\n\r\n# set label colours\r\nV(g)$label.color <- \"black\"\r\nV(g)$label.color[which(V(g)$seed == TRUE)] <- \"dodgerblue4\"\r\n\r\n# set labels for seed sites and nodes with an in-degree > 1\r\nV(g)$label <- ifelse((degree(g, mode = \"in\") > 1 | V(g)$seed), V(g)$name, NA)\r\n\r\n# simplify and plot the graph\r\nset.seed(200)\r\ntkplot(simplify(g),\r\n       canvas.width = 1024, canvas.height = 1024,\r\n       layout = layout_with_dh(g),\r\n       vertex.size = 3 + (degree(g, mode = \"in\")*2),\r\n       vertex.label.cex = 1 + log(degree(g, mode = \"in\")),\r\n       edge.arrow.size = 0.4,\r\n       edge.width = 1 + log(E(g)$weight))\r\n\r\n\r\n\r\nFigure 2: Hyperlink network of actors\r\n\r\n\r\nAckland, R. 2010. “WWW Hyperlink Networks.” Edited by D. L. Hansen and B. Shneiderman and M. A. Smith. Morgan-Kaufmann.\r\n\r\n\r\n———. 2013. Web Social Science: Concepts, Data and Tools for Social Scientists in the Digital Age. SAGE Publications.\r\n\r\n\r\nAckland, R., and M. O’Neil. 2011. “Online Collective Identity: The Case of the Environmental Movement.” Social Networks 33 (3): 177–90. https://doi.org/10.1016/j.socnet.2011.03.001.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-03-15-hyperlink-networks-with-vosonsml/hyperlink_network.png",
    "last_modified": "2021-03-31T19:41:43+11:00",
    "input_file": "hyperlink-networks-with-vosonsml.utf8.md",
    "preview_width": 1024,
    "preview_height": 1025
  },
  {
    "path": "posts/2021-02-11_twitter_vsml_from_rtweet/",
    "title": "Creating Twitter Networks with vosonSML using rtweet Data",
    "description": "Simple guide to collecting data with rtweet and generating networks with vosonSML.",
    "author": [
      {
        "name": "Bryan Gertzel",
        "url": {}
      },
      {
        "name": "Francisca Borquez",
        "url": {}
      }
    ],
    "date": "2021-02-11",
    "categories": [
      "rstats",
      "twitter",
      "vosonSML",
      "rtweet",
      "networks"
    ],
    "contents": "\r\n\r\nContents\r\nIntroduction\r\nAPI Authentication\r\n\r\nTwitter Data Collection with rtweet\r\nSearch Collection\r\nSave the Data\r\n\r\nCreating Networks with vosonSML\r\nRead the Data\r\nPrepare the Data\r\nCreate the Network\r\n\r\n\r\nIntroduction\r\nSocial media platforms are a rich resource for Social Network data. Twitter is a highly popular public platform for social commentary that, like most social media supporting third-party applications, allow software to access and retrieve it’s data via Application Programming Interfaces or API’s. Because of its popularity with individuals and communities around the world, the ready availability of its data, and low barrier for entry, Twitter has become of great interest as a data source for online empirical research.\r\nThere have been many pieces of software developed across programming languages and environments to access the Twitter API. Within the R ecosystem the most comprehensive and well supported of Twitter packages is rtweet developed by Michael Kearney and part of the rOpenSci initiative. The rtweet package provides R functions to both authenticate and collect timelines, tweets and other metadata using Twitter’s v1.1 standard and premium API’s.\r\nThe VOSON Lab develops and maintains the open source R packages vosonSML and VOSONDash. These were created to integrate online data collection, network generation and analysis into a consistent and easy to use work flow across many popular web and social media platforms. For Twitter, the vosonSML package provides an interface to rtweet’s collection features through which tweets can be searched for and retrieved, and then uses this data to produce networks. There may be cases however, such as in the collection of streaming data or analysis of previously collected twitter data where you haven’t used vosonSML’s collection function but instead simply wish to produce vosonSML generated networks from your rtweet data. Because vosonSML uses rtweet this is easily achievable and with minimal R coding.\r\nAPI Authentication\r\nAccessing the Twitter API to collect tweets requires authentication via a Twitter app. There are generally two ways this can be achieved, you can apply for a Twitter Developer account and create your own app (and access keys) or you can authorize another persons app to access the API on your behalf (using their keys). The latter still requires your own Twitter user account but you do not need to go through the Developer application or app creation process. The {vosonSML} package requires users to create their own app and use their own keys but the rtweet package supports both methods, and you can collect tweets after a simple one-time web authorization step of their embedded rstats2twitter app.\r\nTwitter Data Collection with rtweet\r\nThe following simple example will demonstrate how to use the rtweet package to collect some tweet data using built-in authentication via the rtweet app.\r\nSearch Collection\r\nA fairly standard tweet collection usually involves using the Twitter Search API endpoint to search for past tweets that meet a certain criteria. This can be done with rtweet and the search_tweets function with the criteria set by passing additional parameters. In our example we will direct the API to search and return 100 tweets (n = 100) containing the hashtag #auspol and excluding any retweets (include_rts = FALSE). By default only the most recent tweets within the last 7 days will be returned by the API.\r\n\r\n\r\n\r\n\r\n\r\nlibrary(rtweet)\r\n\r\n# recent tweet search collection\r\nauspol_tweets <- search_tweets(\"#auspol\", n = 100, include_rts = FALSE)\r\n\r\n\r\n\r\nThe first time rtweet collection functions are run they will open a Twitter web page on your default web browser asking permission to authorize rstats2twitter.\r\n\r\n\r\n\r\nFigure 1: rstats2twitter app authorization\r\n\r\n\r\n\r\nIf API authentication and search succeeds then the search_tweets function will return a data frame of tweet data. The data frame will have up to 100 rows, one for each tweet collected and 90 columns for associated tweet metadata:\r\n\r\n\r\nlibrary(tibble)\r\n\r\n# print the first 2 rows\r\nprint(auspol_tweets, n = 2)\r\n# # A tibble: 100 x 90\r\n#   user_id  status_id  created_at          screen_name text      source\r\n#   <chr>    <chr>      <dttm>              <chr>       <chr>     <chr> \r\n# 1 27007685 136400068~ 2021-02-22 23:54:39 ronth~      \"@janeen~ Twitt~\r\n# 2 1359301~ 136400067~ 2021-02-22 23:54:37 Injur~      \"When th~ Twitt~\r\n\r\n\r\n\r\n\r\n\r\nShow additional columns\r\n\r\n# # ... with 98 more rows, and 84 more variables:\r\n# #   display_text_width <dbl>, reply_to_status_id <chr>,\r\n# #   reply_to_user_id <chr>, reply_to_screen_name <chr>,\r\n# #   is_quote <lgl>, is_retweet <lgl>, favorite_count <int>,\r\n# #   retweet_count <int>, quote_count <int>, reply_count <int>,\r\n# #   hashtags <list>, symbols <list>, urls_url <list>,\r\n# #   urls_t.co <list>, urls_expanded_url <list>, media_url <list>,\r\n# #   media_t.co <list>, media_expanded_url <list>, media_type <list>,\r\n# #   ext_media_url <list>, ext_media_t.co <list>,\r\n# #   ext_media_expanded_url <list>, ext_media_type <chr>,\r\n# #   mentions_user_id <list>, mentions_screen_name <list>, lang <chr>,\r\n# #   quoted_status_id <chr>, quoted_text <chr>,\r\n# #   quoted_created_at <dttm>, quoted_source <chr>,\r\n# #   quoted_favorite_count <int>, quoted_retweet_count <int>,\r\n# #   quoted_user_id <chr>, quoted_screen_name <chr>,\r\n# #   quoted_name <chr>, quoted_followers_count <int>,\r\n# #   quoted_friends_count <int>, quoted_statuses_count <int>,\r\n# #   quoted_location <chr>, quoted_description <chr>,\r\n# #   quoted_verified <lgl>, retweet_status_id <chr>,\r\n# #   retweet_text <chr>, retweet_created_at <dttm>,\r\n# #   retweet_source <chr>, retweet_favorite_count <int>,\r\n# #   retweet_retweet_count <int>, retweet_user_id <chr>,\r\n# #   retweet_screen_name <chr>, retweet_name <chr>,\r\n# #   retweet_followers_count <int>, retweet_friends_count <int>,\r\n# #   retweet_statuses_count <int>, retweet_location <chr>,\r\n# #   retweet_description <chr>, retweet_verified <lgl>,\r\n# #   place_url <chr>, place_name <chr>, place_full_name <chr>,\r\n# #   place_type <chr>, country <chr>, country_code <chr>,\r\n# #   geo_coords <list>, coords_coords <list>, bbox_coords <list>,\r\n# #   status_url <chr>, name <chr>, location <chr>, description <chr>,\r\n# #   url <chr>, protected <lgl>, followers_count <int>,\r\n# #   friends_count <int>, listed_count <int>, statuses_count <int>,\r\n# #   favourites_count <int>, account_created_at <dttm>,\r\n# #   verified <lgl>, profile_url <chr>, profile_expanded_url <chr>,\r\n# #   account_lang <lgl>, profile_banner_url <chr>,\r\n# #   profile_background_url <chr>, profile_image_url <chr>\r\n\r\n\r\n\r\nThis contains all of the data necessary for {vosonSML} to construct Twitter networks.\r\nSave the Data\r\nThere are a few methods of saving data depending on where and how it will be used. Two common methods are to use a text-based file format such as a CSV, or alternatively if the data will be used within R we can save the dataframe object to a binary compressed RDS (R data object) file using saveRDS instead. Conveniently, the rtweet package has a method to save Twitter data to file in CSV format with the write_as_csv function that takes care of Twitter nested data and conversion issues, and saving an RDS file is also very easy as follows.\r\n\r\n\r\n# save data using rtweet write csv\r\nwrite_as_csv(auspol_tweets, \"auspol_tweets.csv\")\r\n\r\n# save data to file as an R data object\r\nsaveRDS(auspol_tweets, \"auspol_tweets.rds\")\r\n\r\n\r\n\r\nCreating Networks with vosonSML\r\nRead the Data\r\nIf the data was saved to file with the rtweet function write_as_csv it can be read again using read_twitter_csv or readRDS if from an RDS file.\r\n\r\n\r\nauspol_tweets <- read_twitter_csv(\"auspol_tweets.csv\")\r\n\r\nauspol_tweets <- readRDS(\"auspol_tweets.rds\")\r\n\r\n\r\n\r\nPrepare the Data\r\nFor {vosonSML} to recognize the previously collected data as a Twitter data source and be able to internally route it to the appropriate network functions a minor change needs to be made to the data frame first. This involves adding two attributes datasource and twitter to the class list of the auspol_tweets data frame object as follows:\r\n\r\n\r\n# original class list\r\nclass(auspol_tweets)\r\n\r\n\r\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\r\n\r\n# add to the class list\r\nclass(auspol_tweets) <- append(c(\"datasource\", \"twitter\"), class(auspol_tweets))\r\n\r\n# modified class list\r\nclass(auspol_tweets)\r\n\r\n\r\n[1] \"datasource\" \"twitter\"    \"tbl_df\"     \"tbl\"        \"data.frame\"\r\n\r\nThe order of classes is important and for the data frame to be compatible with dplyr - a very common data manipulation package in R, and subsequently usable in the tidyverse and {vosonSML}, then the new attributes need to be added to the beginning of the list.\r\nFor versions of {vosonSML} more recent than 0.29.13 this can now all be managed by using the ImportData function. This method is preferable as it is easier, works for both files and data frames, and will support any future updates to {vosonSML} without breaking your code.\r\n\r\n\r\n\r\n\r\n\r\nlibrary(vosonSML)\r\n\r\n# use the import data function\r\nauspol_tweets <- ImportData(auspol_tweets, \"twitter\")\r\n\r\n\r\n\r\nPlease note that modifying data frame attributes or importing data is only required for rtweet data and not a necessary step for Twitter data collected using the {vosonSML} Twitter Collect function.\r\nObject classes in R are a more advanced topic and not required knowledge to use {vosonSML} but if you would like to learn more a good introduction can be found in the Object-oriented programming chapter of Advanced R by Hadley Wickham.\r\nCreate the Network\r\nThe tweet data can now be used to create the nodes and edges network data, and a graph by using the {vosonSML} Create and Graph functions:\r\n\r\n\r\n# create the network data\r\nauspol_actor_network <- Create(auspol_tweets, \"actor\")\r\n\r\n\r\nGenerating twitter actor network...\r\n-------------------------\r\ncollected tweets | 100\r\nretweets         | 0 \r\nquoting others   | 18\r\nmentions         | 28\r\nreply mentions   | 17\r\nreplies          | 28\r\nself-loops       | 42\r\nnodes            | 149\r\nedges            | 133\r\n-------------------------\r\nDone.\r\n\r\n\r\n\r\n# create an igraph\r\nauspol_actor_graph <- Graph(auspol_actor_network)\r\n\r\n\r\nCreating igraph network graph...Done.\r\n\r\nThat’s all there is to it, and now the resulting igraph network can be plotted.\r\n\r\n\r\nlibrary(igraph)\r\n\r\n# set plot margins\r\npar(mar = c(0, 0, 0, 0))\r\n\r\n# auspol actor network with fruchterman-reingold layout\r\nplot(auspol_actor_graph, layout = layout_with_fr(auspol_actor_graph),\r\n     vertex.label = NA, vertex.size = 6, edge.arrow.size = 0.4)\r\n\r\n\r\n\r\n\r\nFigure 2: Actor network graph for collected #auspol tweets\r\n\r\n\r\n\r\nFor further information about rtweet, its features and how to use it to collect twitter data please refer to the package site and introductory rtweet vignette. For creating different types of networks such as the activity, 2-mode and semantic types with {vosonSML} see the package documentation and introductory vosonSML vignette.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-02-11_twitter_vsml_from_rtweet/rtweet_logo_preview.png",
    "last_modified": "2021-03-31T19:50:18+11:00",
    "input_file": "twitter_vsml_from_rtweet.utf8.md",
    "preview_width": 432,
    "preview_height": 499
  },
  {
    "path": "posts/2021-02-05_welcome/",
    "title": "Welcome to the VOSON Lab Coding Blog",
    "description": "The coding blog is a space to share tools, methods, tips, examples and code. A place to collect data, construct and analyze online networks.",
    "author": [
      {
        "name": "VOSON Lab",
        "url": "http://vosonlab.net/"
      },
      {
        "name": "",
        "url": {}
      }
    ],
    "date": "2021-02-04",
    "categories": [
      "Rstats",
      "Social Network Analysis",
      "Computational Social Methods"
    ],
    "contents": "\r\nWelcome to the VOSON Lab Coding Blog! We have created this space to share methods, tips, examples and code. It’s also a place where we will demonstrate constructing and analyzing networks from various API and other online data sources.\r\nMost of our posts will cover techniques around the tools we have developed at the Lab: vosonSML, VOSONDash and voson.tcn, which are available on both CRAN and GitHub. But we also plan to use this space to cover other complementary R packages and open-source software, such as fantastic R packages within the tidyverse, RStudio’s shiny for web apps, and visualization tools such as igraph and Gephi.\r\nVOSON Lab R Packages - Hex stickersVOSON Lab Open Source Tools\r\nvosonSML is a R package for social media data collection (currently twitter, youtube, and reddit), hyperlink collection and network generation. VOSONDash is a Shiny app that integrates tools for visualizing and manipulating network graphs, performing network and text analysis, as well as an interface for collecting data with vosonSML.\r\nMore information on these packages, their development and code can be found on our vosonSML, VOSONDash and voson.tcn github pages.\r\nWe also have some other guides for using the packages. Check the vosonSML Vignette and the VOSON Dash Userguide for some practical examples and feature reference.\r\nWe hope you find this content useful!\r\nThe VOSON Lab team.\r\nVirtual Observatory for the Study of Online Networks VOSON Lab, School of Sociology, The Australian National University.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-02-05_welcome/square-cards.png",
    "last_modified": "2021-03-28T23:42:39+11:00",
    "input_file": {},
    "preview_width": 800,
    "preview_height": 640
  }
]

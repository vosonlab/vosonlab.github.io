[
  {
    "path": "posts/2021-03-15-hyperlink-networks-with-vosonsml/",
    "title": "Hyperlink Networks with vosonSML",
    "description": "An introduction to creating hyperlink networks with vosonSML.",
    "author": [
      {
        "name": "Bryan Gertzel",
        "url": {}
      },
      {
        "name": "Francisca Borquez",
        "url": {}
      }
    ],
    "date": "2021-03-15",
    "categories": [
      "rstats",
      "hyperlinks",
      "vosonSML",
      "networks"
    ],
    "contents": "\r\n\r\nContents\r\nIntroduction\r\nInstallation\r\n\r\nHyperlink Collection\r\nSetting Up\r\nPerforming the Collection\r\n\r\nNetwork Creation\r\nNetworks\r\nGraphing\r\n\r\n\r\nThe VOSON software for hyperlink collection and analysis was an early research output of the VOSON Lab (Ackland 2010). It addressed a need for tools that could help study online social networks, even before the rise of social media, and assisted researchers gain insights into important phenomena such as networks around issue spheres and online social movements [see (Ackland and O’Neil 2011) and (Ackland 2013)]. After many years and many iterations since its inception in 2004, the VOSON Lab is happy to reintroduce the canonical VOSON hyperlink collection software as part of our R open-source toolkit for social media collection: vosonSML.\r\nThis simple guide will demonstrate how to use the new features of the vosonSML package to perform a hyperlink collection and generate networks for analysis.\r\nIntroduction\r\nThe vosonSML hyperlink collection and network creation works similarly to the 3-step process we use with other social media sources: the Authenticate, Collect and Create verb functions. The Authenticate function is first called with the parameter “web” to identify and set up the context for subsequent operations, but it does not require any further credentials in this implementation. vosonSML uses standard web crawling and text-based page scraping techniques to discover hyperlinks and, as such, there is no need to access any restricted data API’s as we commonly do with social media.\r\nInstallation\r\nThe new hyperlink collection and network features are currently available in the development version of vosonSML on GitHub, and are to soon be released on CRAN. The development version can be installed as follows:\r\n\r\n\r\n# use the remotes package to install the latest dev version of vosonSML from github\r\nlibrary(remotes)\r\ninstall_github(\"vosonlab/vosonsml\")\r\n\r\n# Downloading GitHub repo vosonlab/vosonsml@HEAD\r\n# v  checking for file\r\n# -  preparing 'vosonSML':\r\n# v  checking DESCRIPTION meta-information ... \r\n# -  checking for LF line-endings in source and make files and shell scripts\r\n# -  checking for empty or unneeded directories\r\n# -  building 'vosonSML_0.30.00.9000.tar.gz'\r\n#    \r\n# * installing *source* package 'vosonSML' ...\r\n# ...\r\n# * DONE (vosonSML)\r\n# Making 'packages.html' ... done\r\n\r\n\r\n\r\nHyperlink Collection\r\nSetting Up\r\nThe web sites or pages to collect hyperlinks from are specified and input to the Collect function in a dataframe. As there are page specific options that can be used, this format helps us to organise and set the request parameters. The URL’s set in the dataframe for the page column are called ‘seed pages’ and are the starting points for web crawling. Although not explicitly indicated in the URL’s, the seed pages are actually the landing pages or “index” pages of the web sites and a page name can be specified if known or desired.\r\n\r\n\r\n# set sites as seed pages and set each for external crawl with a max depth\r\npages <- data.frame(page = c(\"http://vosonlab.net\",\r\n                             \"https://www.oii.ox.ac.uk\",\r\n                             \"https://sonic.northwestern.edu\"),\r\n                    type = c(\"ext\", \"ext\", \"ext\"),\r\n                    max_depth = c(2, 2, 2))\r\n\r\n\r\n\r\nThe example above shows seed pages with some additional per-seed parameters that are used to control the web crawling. The type parameter can be set to a value of either int, ext or all, which correspond to following only internal, external or following all hyperlinks found on a seeded web page and subsequent pages discovered from that particular seed. How a hyperlink is classified is determined by the seed domain name, for example, if the seed page is https://vosonlab.net a type of ext will follow hyperlinks from that page that do not have a domain name of “vosonlab.net.” A type of int will follow only hyperlinks that match a domain of “vosonlab.net,” and a type of all will follow all hyperlinks found irrespective of their domain. The final parameter max_depth refers to how many levels of pages to follow from the seed page. In the diagram below, the green dots are pages scraped by the web crawler and the blue dots links are the hyperlinks collected from them for a max depth of 1,2 and 3.\r\nScope of hyperlinks collected using the max_depth parameterAs can be seen, a max depth of 1 directs the crawler to scrape and collect hyperlinks from only seed pages, a max depth of 2 to follow hyperlinks found on the seed pages and collect hyperlinks from those pages as well, and so on radiating outwards. The number of pages and hyperlinks can rise very rapidly so it is best to keep this number as low as possible. If a greater reach in collection sites is desired, this could perhaps more efficiently be achieved by revising and adding more seed pages in the first instance. In the example code the type has been set to “ext” (external) for all three seed sites, so as to limit “mapping” of the internal seed web sites and focus on their outward facing connections. Depth of crawl was set to 2.\r\nIt should be noted that all hyperlinks found are collected from scraped pages and used to generate networks. The type and max_depth parameters only apply to the web crawling and scraping activity.\r\nPerforming the Collection\r\nThe hyperlink data can now be collected using the Collect function with the pages parameter. This produces a dataframe that contains the hyperlink URL’s found, pages they were found on and other metadata that can be used to help construct networks.\r\n\r\n\r\nlibrary(magrittr)\r\nlibrary(dplyr)\r\nlibrary(vosonSML)\r\n\r\n# set up as a web collection and collect the hyperlink data using the\r\n# previously defined seed pages\r\nhyperlinks <- Authenticate(\"web\") %>% Collect(pages)\r\n\r\n# Collecting web page hyperlinks...\r\n# *** initial call to get urls - http://vosonlab.net\r\n# * new domain: http://vosonlab.net \r\n# + http://vosonlab.net (10 secs)\r\n# *** end initial call\r\n# *** set depth: 2\r\n# *** loop call to get urls - nrow: 6 depth: 2 max_depth: 2\r\n# * new domain: http://rsss.anu.edu.au \r\n# + http://rsss.anu.edu.au (0.96 secs)\r\n# ...\r\n\r\n# dataframe structure\r\nglimpse(hyperlinks)\r\n# Rows: 1,163\r\n# Columns: 9\r\n# $ url       <chr> \"http://rsss.anu.edu.au\", \"http://rsss.cass.anu.edu.au\", \"ht~\r\n# $ n         <int> 1, 1, 4, 1, 1, 2, 2, 2, 2, 2, 2, 1, 1, 1, 4, 1, 1, 1, 1, 1, ~\r\n# $ page_err  <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, ~\r\n# $ page      <chr> \"http://vosonlab.net\", \"http://vosonlab.net\", \"http://vosonl~\r\n# $ depth     <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ~\r\n# $ max_depth <dbl> 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ~\r\n# $ parse     <df[,6]> <data.frame[26 x 6]>\r\n# $ seed      <chr> \"http://vosonlab.net\", \"http://vosonlab.net\", \"http://vos~\r\n# $ type      <chr> \"ext\", \"ext\", \"ext\", \"ext\", \"ext\", \"ext\", \"ext\", \"ext\", \"ext~\r\n\r\n# number of pages scraped for hyperlinks\r\nnrow(hyperlinks %>% distinct(page))\r\n# [1] 38\r\n\r\n# number of hyperlinks collected\r\nnrow(hyperlinks)\r\n# [1] 1163\r\n\r\n\r\n\r\nA total of 1,163 hyperlinks were collected from 38 pages followed from our 3 seed pages. Using this data, it is now possible to generate hyperlink networks.\r\nNetwork Creation\r\nNetworks\r\nAs with other vosonSML social media, there are two standard types of networks we can create. An activity network that produces a more structural representation of the network where nodes are web pages and edges are the hyperlink references between them, and an actor network that instead groups pages into entities based on their domain names.\r\n\r\n\r\n# generate a hyperlink activity network\r\nactivity_net <- Create(hyperlinks, \"activity\")\r\n\r\n# generate a hyperlink actor network\r\nactor_net <- Create(hyperlinks, \"actor\")\r\n# Generating web actor network...\r\n# Done.\r\n\r\n\r\n\r\nThe output of the network creation is a named list of two dataframes, one for the nodes and the other for the edges or edge list data. The example below shows the actor_net. Note that the edges of the actor network are also aggregated into a weight value and that actors can link to themselves forming self-loops.\r\n\r\n\r\nprint(as_tibble(actor_net$nodes))\r\n# # A tibble: 185 x 2\r\n#   id                              link_id\r\n#   <chr>                             <int>\r\n# 1 accounts.google.com                   1\r\n# 2 alumni.kellogg.northwestern.edu       2\r\n# 3 anu.edu.au                            3\r\n# # ... with 182 more rows\r\n\r\nprint(as_tibble(actor_net$edges))\r\n# # A tibble: 226 x 3\r\n#   from            to              weight\r\n#   <chr>           <chr>            <int>\r\n# 1 rsss.anu.edu.au anu.edu.au           2\r\n# 2 rsss.anu.edu.au rsss.anu.edu.au     36\r\n# 3 rsss.anu.edu.au soundcloud.com       1\r\n# # ... with 223 more rows\r\n\r\n\r\n\r\nGraphing\r\nNow that the network has been generated, we can create a graph and plot it. The Graph function creates an igraph format object that can be directly plotted or adjusted for presentation using igraph plotting parameters.\r\n\r\n\r\nlibrary(igraph)\r\nlibrary(stringr)\r\n\r\nactor_net <- Create(hyperlinks, \"actor\")\r\n\r\n# identify the seed pages and set a node attribute\r\nseed_pages <- pages %>%\r\n  mutate(page = str_remove(page, \"^http[s]?://\"), seed = TRUE)\r\nactor_net$nodes <- actor_net$nodes %>%\r\n  left_join(seed_pages, by = c(\"id\" = \"page\"))\r\n\r\n# create an igraph from the network\r\ng <- actor_net %>% Graph()\r\n\r\n# set node colours\r\nV(g)$color <- ifelse(degree(g, mode = \"in\") > 1, \"yellow\", \"grey\")\r\nV(g)$color[which(V(g)$seed == TRUE)] <- \"dodgerblue3\"\r\n\r\n# set label colours\r\nV(g)$label.color <- \"black\"\r\nV(g)$label.color[which(V(g)$seed == TRUE)] <- \"dodgerblue4\"\r\n\r\n# set labels for seed sites and nodes with an in-degree > 1\r\nV(g)$label <- ifelse((degree(g, mode = \"in\") > 1 | V(g)$seed), V(g)$name, NA)\r\n\r\n# simplify and plot the graph\r\nset.seed(200)\r\ntkplot(simplify(g),\r\n       canvas.width = 1024, canvas.height = 1024,\r\n       layout = layout_with_dh(g),\r\n       vertex.size = 3 + (degree(g, mode = \"in\")*2),\r\n       vertex.label.cex = 1 + log(degree(g, mode = \"in\")),\r\n       edge.arrow.size = 0.4,\r\n       edge.width = 1 + log(E(g)$weight))\r\n\r\n\r\n\r\nHyperlink network of actors\r\n\r\n\r\nAckland, R. 2010. “WWW Hyperlink Networks.” Edited by D. L. Hansen and B. Shneiderman and M. A. Smith. Morgan-Kaufmann.\r\n\r\n\r\n———. 2013. “Web Social Science: Concepts, Data and Tools for Social Scientists in the Digital Age.” SAGE Publications.\r\n\r\n\r\nAckland, R., and M. O’Neil. 2011. “Online Collective Identity: The Case of the Environmental Movement.” Social Networks 3 (3): 1–18. https://doi.org/10.1016/j.socnet.2011.03.001.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-03-15-hyperlink-networks-with-vosonsml/hyperlink_network.png",
    "last_modified": "2021-03-29T15:34:19+11:00",
    "input_file": "hyperlink-networks-with-vosonsml.utf8.md",
    "preview_width": 1024,
    "preview_height": 1025
  },
  {
    "path": "posts/2021-02-11_twitter_vsml_from_rtweet/",
    "title": "Creating Twitter Networks with vosonSML using rtweet Data",
    "description": "Simple guide to collecting data with rtweet and generating networks with vosonSML.",
    "author": [
      {
        "name": "Bryan Gertzel",
        "url": {}
      },
      {
        "name": "Francisca Borquez",
        "url": {}
      }
    ],
    "date": "2021-02-11",
    "categories": [
      "rstats",
      "twitter",
      "vosonSML",
      "rtweet",
      "networks"
    ],
    "contents": "\r\n\r\nContents\r\nIntroduction\r\nAPI Authentication\r\n\r\nTwitter Data Collection with rtweet\r\nSearch Collection\r\nSave the Data\r\n\r\nCreating Networks with vosonSML\r\nRead the Data\r\nPrepare the Data\r\nCreate the Network\r\n\r\n\r\nIntroduction\r\nSocial media platforms are a rich resource for Social Network data. Twitter is a highly popular public platform for social commentary that, like most social media supporting third-party applications, allow software to access and retrieve it’s data via Application Programming Interfaces or API’s. Because of its popularity with individuals and communities around the world, the ready availability of its data, and low barrier for entry, Twitter has become of great interest as a data source for online empirical research.\r\nThere have been many pieces of software developed across programming languages and environments to access the Twitter API. Within the R ecosystem the most comprehensive and well supported of Twitter packages is rtweet developed by Michael Kearney and part of the rOpenSci initiative. The rtweet package provides R functions to both authenticate and collect timelines, tweets and other metadata using Twitter’s v1.1 standard and premium API’s.\r\nThe VOSON Lab develops and maintains the open source R packages vosonSML and VOSONDash. These were created to integrate online data collection, network generation and analysis into a consistent and easy to use work flow across many popular web and social media platforms. For Twitter, the vosonSML package provides an interface to rtweet’s collection features through which tweets can be searched for and retrieved, and then uses this data to produce networks. There may be cases however, such as in the collection of streaming data or analysis of previously collected twitter data where you haven’t used vosonSML’s collection function but instead simply wish to produce vosonSML generated networks from your rtweet data. Because vosonSML uses rtweet this is easily achievable and with minimal R coding.\r\nAPI Authentication\r\nAccessing the Twitter API to collect tweets requires authentication via a Twitter app. There are generally two ways this can be achieved, you can apply for a Twitter Developer account and create your own app (and access keys) or you can authorize another persons app to access the API on your behalf (using their keys). The latter still requires your own Twitter user account but you do not need to go through the Developer application or app creation process. The {vosonSML} package requires users to create their own app and use their own keys but the rtweet package supports both methods, and you can collect tweets after a simple one-time web authorization step of their embedded rstats2twitter app.\r\nTwitter Data Collection with rtweet\r\nThe following simple example will demonstrate how to use the rtweet package to collect some tweet data using built-in authentication via the rtweet app.\r\nSearch Collection\r\nA fairly standard tweet collection usually involves using the Twitter Search API endpoint to search for past tweets that meet a certain criteria. This can be done with rtweet and the search_tweets function with the criteria set by passing additional parameters. In our example we will direct the API to search and return 100 tweets (n = 100) containing the hashtag #auspol and excluding any retweets (include_rts = FALSE). By default only the most recent tweets within the last 7 days will be returned by the API.\r\n\r\n\r\n\r\n\r\n\r\nlibrary(rtweet)\r\n\r\n# recent tweet search collection\r\nauspol_tweets <- search_tweets(\"#auspol\", n = 100, include_rts = FALSE)\r\n\r\n\r\n\r\nThe first time rtweet collection functions are run they will open a Twitter web page on your default web browser asking permission to authorize rstats2twitter.\r\n\r\n\r\n\r\nFigure 1: rstats2twitter app authorization\r\n\r\n\r\n\r\nIf API authentication and search succeeds then the search_tweets function will return a data frame of tweet data. The data frame will have up to 100 rows, one for each tweet collected and 90 columns for associated tweet metadata:\r\n\r\n\r\nlibrary(tibble)\r\n\r\n# print the first 2 rows\r\nprint(head(auspol_tweets, n = 2))\r\n\r\n\r\n# A tibble: 2 x 90\r\n  user_id  status_id  created_at          screen_name text      source\r\n  <chr>    <chr>      <dttm>              <chr>       <chr>     <chr> \r\n1 27007685 136400068~ 2021-02-22 23:54:39 ronthorp    \"@janeen~ Twitt~\r\n2 1359301~ 136400067~ 2021-02-22 23:54:37 InjuredNsw  \"When th~ Twitt~\r\n# ... with 84 more variables: display_text_width <dbl>,\r\n#   reply_to_status_id <chr>, reply_to_user_id <chr>,\r\n#   reply_to_screen_name <chr>, is_quote <lgl>, is_retweet <lgl>,\r\n#   favorite_count <int>, retweet_count <int>, quote_count <int>,\r\n#   reply_count <int>, hashtags <list>, symbols <list>,\r\n#   urls_url <list>, urls_t.co <list>, urls_expanded_url <list>,\r\n#   media_url <list>, media_t.co <list>, media_expanded_url <list>,\r\n#   media_type <list>, ext_media_url <list>, ext_media_t.co <list>,\r\n#   ext_media_expanded_url <list>, ext_media_type <chr>,\r\n#   mentions_user_id <list>, mentions_screen_name <list>, lang <chr>,\r\n#   quoted_status_id <chr>, quoted_text <chr>,\r\n#   quoted_created_at <dttm>, quoted_source <chr>,\r\n#   quoted_favorite_count <int>, quoted_retweet_count <int>,\r\n#   quoted_user_id <chr>, quoted_screen_name <chr>,\r\n#   quoted_name <chr>, quoted_followers_count <int>,\r\n#   quoted_friends_count <int>, quoted_statuses_count <int>,\r\n#   quoted_location <chr>, quoted_description <chr>,\r\n#   quoted_verified <lgl>, retweet_status_id <chr>,\r\n#   retweet_text <chr>, retweet_created_at <dttm>,\r\n#   retweet_source <chr>, retweet_favorite_count <int>,\r\n#   retweet_retweet_count <int>, retweet_user_id <chr>,\r\n#   retweet_screen_name <chr>, retweet_name <chr>,\r\n#   retweet_followers_count <int>, retweet_friends_count <int>,\r\n#   retweet_statuses_count <int>, retweet_location <chr>,\r\n#   retweet_description <chr>, retweet_verified <lgl>,\r\n#   place_url <chr>, place_name <chr>, place_full_name <chr>,\r\n#   place_type <chr>, country <chr>, country_code <chr>,\r\n#   geo_coords <list>, coords_coords <list>, bbox_coords <list>,\r\n#   status_url <chr>, name <chr>, location <chr>, description <chr>,\r\n#   url <chr>, protected <lgl>, followers_count <int>,\r\n#   friends_count <int>, listed_count <int>, statuses_count <int>,\r\n#   favourites_count <int>, account_created_at <dttm>,\r\n#   verified <lgl>, profile_url <chr>, profile_expanded_url <chr>,\r\n#   account_lang <lgl>, profile_banner_url <chr>,\r\n#   profile_background_url <chr>, profile_image_url <chr>\r\n\r\nThis contains all of the data necessary for {vosonSML} to construct Twitter networks.\r\nSave the Data\r\nThere are a few methods of saving data depending on where and how it will be used. Two common methods are to use a text-based file format such as a CSV, or alternatively if the data will be used within R we can save the dataframe object to a binary compressed RDS (R data object) file using saveRDS instead. Conveniently, the rtweet package has a method to save Twitter data to file in CSV format with the write_as_csv function that takes care of Twitter nested data and conversion issues, and saving an RDS file is also very easy as follows.\r\n\r\n\r\n# save data using rtweet write csv\r\nwrite_as_csv(auspol_tweets, \"auspol_tweets.csv\")\r\n\r\n# save data to file as an R data object\r\nsaveRDS(auspol_tweets, \"auspol_tweets.rds\")\r\n\r\n\r\n\r\nCreating Networks with vosonSML\r\nRead the Data\r\nIf the data was saved to file with the rtweet function write_as_csv it can be read again using read_twitter_csv or readRDS if from an RDS file.\r\n\r\n\r\nauspol_tweets <- read_twitter_csv(\"auspol_tweets.csv\")\r\n\r\nauspol_tweets <- readRDS(\"auspol_tweets.rds\")\r\n\r\n\r\n\r\nPrepare the Data\r\nFor {vosonSML} to recognize the previously collected data as a Twitter data source and be able to internally route it to the appropriate network functions a minor change needs to be made to the data frame first. This involves adding two attributes datasource and twitter to the class list of the auspol_tweets data frame object as follows:\r\n\r\n\r\n# original class list\r\nclass(auspol_tweets)\r\n\r\n\r\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\r\n\r\n# add to the class list\r\nclass(auspol_tweets) <- append(c(\"datasource\", \"twitter\"), class(auspol_tweets))\r\n\r\n# modified class list\r\nclass(auspol_tweets)\r\n\r\n\r\n[1] \"datasource\" \"twitter\"    \"tbl_df\"     \"tbl\"        \"data.frame\"\r\n\r\nThe order of classes is important and for the data frame to be compatible with dplyr - a very common data manipulation package in R, and subsequently usable in the tidyverse and {vosonSML}, then the new attributes need to be added to the beginning of the list.\r\nFor versions of {vosonSML} more recent than 0.29.13 this can now all be managed by using the ImportData function. This method is preferable as it is easier, works for both files and data frames, and will support any future updates to {vosonSML} without breaking your code.\r\n\r\n\r\n\r\n\r\n\r\nlibrary(vosonSML)\r\n\r\n# use the import data function\r\nauspol_tweets <- ImportData(auspol_tweets, \"twitter\")\r\n\r\n\r\n\r\nPlease note that modifying data frame attributes or importing data is only required for rtweet data and not a necessary step for Twitter data collected using the {vosonSML} Twitter Collect function.\r\nObject classes in R are a more advanced topic and not required knowledge to use {vosonSML} but if you would like to learn more a good introduction can be found in the Object-oriented programming chapter of Advanced R by Hadley Wickham.\r\nCreate the Network\r\nThe tweet data can now be used to create the nodes and edges network data, and a graph by using the {vosonSML} Create and Graph functions:\r\n\r\n\r\n# create the network data\r\nauspol_actor_network <- Create(auspol_tweets, \"actor\")\r\n\r\n\r\nGenerating twitter actor network...\r\n-------------------------\r\ncollected tweets | 100\r\nretweets         | 0 \r\nquoting others   | 18\r\nmentions         | 28\r\nreply mentions   | 17\r\nreplies          | 28\r\nself-loops       | 42\r\nnodes            | 149\r\nedges            | 133\r\n-------------------------\r\nDone.\r\n\r\n\r\n\r\n# create an igraph\r\nauspol_actor_graph <- Graph(auspol_actor_network)\r\n\r\n\r\nCreating igraph network graph...Done.\r\n\r\nThat’s all there is to it, and now the resulting igraph network can be plotted.\r\n\r\n\r\nlibrary(igraph)\r\n\r\n# set plot margins\r\npar(mar = c(0, 0, 0, 0))\r\n\r\n# auspol actor network with fruchterman-reingold layout\r\nplot(auspol_actor_graph, layout = layout_with_fr(auspol_actor_graph),\r\n     vertex.label = NA, vertex.size = 6, edge.arrow.size = 0.4)\r\n\r\n\r\n\r\n\r\nFigure 2: Actor network graph for collected #auspol tweets\r\n\r\n\r\n\r\nFor further information about rtweet, its features and how to use it to collect twitter data please refer to the package site and introductory rtweet vignette. For creating different types of networks such as the activity, 2-mode and semantic types with {vosonSML} see the package documentation and introductory vosonSML vignette.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-02-11_twitter_vsml_from_rtweet/rtweet_logo_preview.png",
    "last_modified": "2021-03-28T23:45:20+11:00",
    "input_file": {},
    "preview_width": 432,
    "preview_height": 499
  },
  {
    "path": "posts/2021-02-05_welcome/",
    "title": "Welcome to the VOSON Lab Coding Blog",
    "description": "The coding blog is a space to share tools, methods, tips, examples and code. A place to collect data, construct and analyze online networks.",
    "author": [
      {
        "name": "VOSON Lab",
        "url": "http://vosonlab.net/"
      },
      {
        "name": "",
        "url": {}
      }
    ],
    "date": "2021-02-04",
    "categories": [
      "Rstats",
      "Social Network Analysis",
      "Computational Social Methods"
    ],
    "contents": "\r\nWelcome to the VOSON Lab Coding Blog! We have created this space to share methods, tips, examples and code. It’s also a place where we will demonstrate constructing and analyzing networks from various API and other online data sources.\r\nMost of our posts will cover techniques around the tools we have developed at the Lab: vosonSML, VOSONDash and voson.tcn, which are available on both CRAN and GitHub. But we also plan to use this space to cover other complementary R packages and open-source software, such as fantastic R packages within the tidyverse, RStudio’s shiny for web apps, and visualization tools such as igraph and Gephi.\r\nVOSON Lab R Packages - Hex stickersVOSON Lab Open Source Tools\r\nvosonSML is a R package for social media data collection (currently twitter, youtube, and reddit), hyperlink collection and network generation. VOSONDash is a Shiny app that integrates tools for visualizing and manipulating network graphs, performing network and text analysis, as well as an interface for collecting data with vosonSML.\r\nMore information on these packages, their development and code can be found on our vosonSML, VOSONDash and voson.tcn github pages.\r\nWe also have some other guides for using the packages. Check the vosonSML Vignette and the VOSON Dash Userguide for some practical examples and feature reference.\r\nWe hope you find this content useful!\r\nThe VOSON Lab team.\r\nVirtual Observatory for the Study of Online Networks VOSON Lab, School of Sociology, The Australian National University.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-02-05_welcome/square-cards.png",
    "last_modified": "2021-03-28T23:42:39+11:00",
    "input_file": {},
    "preview_width": 800,
    "preview_height": 640
  }
]

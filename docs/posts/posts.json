[
  {
    "path": "posts/2023-01-20-hyperlink-networks/",
    "title": "Hyperlink networks: data pre-processing techniques",
    "description": "Steps for collecting hyperlink networks with vosonSML and processing hyperlink data using R tools",
    "author": [
      {
        "name": "Robert Ackland",
        "url": {
          "https://orcid.org/0000-0002-0008-1766": {}
        }
      },
      {
        "name": "Sidiq Madya",
        "url": {}
      },
      {
        "name": "Francisca Borquez",
        "url": {}
      }
    ],
    "date": "2023-02-22",
    "categories": [
      "rstats",
      "hyperlink networks",
      "vosonsml"
    ],
    "contents": "\n\nContents\n1. Introduction\n2. Collecting hyperlink data via web crawling\n3. Creating hyperlink networks\n4. Processing nodes in the hyperlink actor network\n4.1 Pagegrouping\n4.2 Pruning\n4.3 Preserving\n\n\n1. Introduction\nThe VOSON software web app – first made publicly available in 2006 – was a tool designed to enable researchers to study Web 1.0 websites as online social networks (Ackland 2010, 2013; Ackland and O’Neil 2011; Lusher and Ackland 2011). The canonical VOSON hyperlink collection was reintroduced in 2021 in vosonSML) and it was made available in VOSONDash version 0.6.1 in February 2023. This post provides the methodological steps and code for collecting hyperlink data using vosonSML and pre-processing hyperlink network data via R, including the specific approaches that were previously available via VOSON software web app (pruning, preserving and pagegrouping).\n2. Collecting hyperlink data via web crawling\nWe collected the hyperlink data using vosonSML.\nFirst load the libraries.\n\n\nlibrary(magrittr)             #only need to load this if we use %>% as pipe, otherwise can use |>\nlibrary(vosonSML)\nlibrary(igraph)\nlibrary(dplyr)\nlibrary(knitr)\nlibrary(stringr)\n\n\nWe create a dataframe containing the pages to be crawled. We are reading a csv file with pages for 20 organisations. These organisations are a sample of non-state or non-government entities (for-profit and non-for-profit) which actively engage in ‘data sovereignty’ debates as part of their concern in contemporary data politics. These organisations include NGO, research think tank, companies, industry associations, and movements or initiatives from communities. These different group of organisations deal with various issues and values when promoting their agenda ranging from security, privacy, innovation, entrepreneurship, to human rights and social justice.\nBeing involved in the emerging issues of global data politics, these organisations are based or headquartered in different countries across the globe include the US, UK, Canada, Germany, The Netherlands, Belgium, and Denmark which represent the Global North, and South Africa, India and Hong Kong representing the Global South. The websites are being used by these organisations to participate in the emerging debates on data politics. Their participation in the debates are becoming more intense in the midst of the ongoing process of massive ‘digitisation’ and ‘datafication’ in societies. We have chosen 1 or 2 pages from each website and are using these as “seed pages” where the crawler will start: in this way we are directing the crawler to the pages we think will be most “productive” in terms of containing hyperlinks and text that are of interest to the study.\n\n\n#For more information on collecting hyperlink networks using vosonSML, see:\n#https://vosonlab.github.io/posts/2021-03-15-hyperlink-networks-with-vosonsml/\n\n# The dataframe needs to have the folllowing fields, for hyperlink collection via vosonSML:\n# page: the seed pages\n# type: the type of crawl, with allowed values:\n#   int: collect all hyperlinks but only follow links that have same domain as seed page (internal)\n#   ext: collect all hyperlinks but only follow links that have different domain as seed page (external)\n#   all: collect and follow all hyperlinks\n# max_depth: how many levels of hyperlinks to follow from seed page\n#For example:\n#pages <- data.frame(page = c(\"http://vosonlab.net\",\n#                             \"https://rsss.cass.anu.edu.au\",\n#                             \"https://www.ansna.org.au\"),\n#                    type = c(\"int\", \"ext\", \"all\"),\n#                    max_depth = c(1, 1, 1))\n\npages <- read.csv(\"seed_sites_20.csv\")\nkable(head(pages))\n\npage\ntype\nmax_depth\ndomain\ncountry\nhttps://womeninlocalization.com/partners/\nint\n1\nwomeninlocalization.com\nUS\nhttps://womeninlocalization.com/data-localization-laws-around-the-world/\nint\n1\nwomeninlocalization.com\nUS\nhttps://iwgia.org/en/network.html\nint\n1\niwgia.org\nDenmark\nhttps://www.iwgia.org/en/indigenous-data-sovereignty/4699-iw-2022-indigenous-data-sovereignty.html?filter_tag[0]=37\nint\n1\niwgia.org\nDenmark\nhttps://indigenousdatalab.org/networks/\nint\n1\nindigenousdatalab.org\nUS\nhttps://indigenousdatalab.org/projects-working/\nint\n1\nindigenousdatalab.org\nUS\n\n#remove pages that caused error with crawler\n#Rob to check this again..error might no longer be present with new version of vosonSML\npages <- pages %>% filter(!grepl(\"ispa.org.za\", pages$page))\n\n\nNote that for the crawler all we need is a dataframe with three columns: page, type, max_depth (see code above for an explantion of these terms and also see the VOSON Lab blog post on crawling using vosonSML), but of course it might be useful to include other meta data in this file that is used in analysis later.\nNow run the crawl.\n\n\n#Remember to set `verbose=TRUE` to see the crawler working\ncrawlDF <- Authenticate(\"web\") %>% Collect(pages, verbose=TRUE)\n#crawlDF\n\n#We will save this dataframe, for use later\n#saveRDS(crawlDF, \"crawlDF.rds\")\nsaveRDS(crawlDF, \"crawlDF_20_sites_depth1.rds\")\n\n\n3. Creating hyperlink networks\nLet’s now create networks from the crawl data.\n\n\ncrawlDF <- readRDS(\"crawlDF_20_sites_depth1.rds\")\n\n# explore dataframe structure\nglimpse(crawlDF)\n\nRows: 2,826\nColumns: 9\n$ url       <chr> \"http://goap-global.com\", \"http://www.reddit.com/s…\n$ n         <int> 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1,…\n$ page_err  <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ page      <chr> \"https://womeninlocalization.com/partners\", \"https…\n$ depth     <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ max_depth <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ parse     <df[,6]> <data.frame[23 x 6]>\n$ seed      <chr> \"https://womeninlocalization.com/partners\", \"ht…\n$ type      <chr> \"int\", \"int\", \"int\", \"int\", \"int\", \"int\", \"int\", \"…\n\nFirst we create the activity network: with hyperlink activity networks, the nodes are web pages. For the purpose of visualisation we will simplify the network by removing loops.\n\n\n# create activity network: nodes are pages hyperlinks were collected from\nnet_activity <- crawlDF %>% Create(\"activity\")\ng_activity <- net_activity %>% Graph()\n#simplify the network - remove loops and multiple edges\ng_activity <- simplify(g_activity)\n\n\n\n\npng(\"activity_network.png\", width=800, height=800)\nplot(g_activity, layout=layout_with_fr(g_activity), vertex.label=\"\", vertex.size=3, edge.width=1, edge.arrow.size=0.5)\ndev.off()\n\n\nFigure 1: Hyperlink activity network - Nodes are web pages.We can see from the visualisation that the hyperlink activity network consists of a number of connected components (sets of nodes that are connected either directly or indirectly). We can look further at these clusters or components.\n\n\ncc <- components(g_activity)\nstr(cc)\n\nList of 3\n $ membership: Named num [1:1736] 1 2 3 4 5 4 3 3 3 3 ...\n  ..- attr(*, \"names\")= chr [1:1736] \"http://1001lakes.com/products\" \"http://abo-peoples.org\" \"http://aimitindia.com\" \"http://artexte.ca/lang/en\" ...\n $ csize     : num [1:15] 195 222 77 55 253 191 68 203 103 92 ...\n $ no        : int 15\n\nsummary(cc$csize)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   25.0    52.5    92.0   115.7   193.0   253.0 \n\nThe above indicates we have 15 weakly-connected components, and these range in size from 25 nodes to 253 nodes. It is not unexpected that the hyperlink activity network is quite disconnected since two seed pages \\(i\\) and \\(j\\) will only be directly connected if they link to each other and they will be indirectly connected if they link to the same third page.\nWe now create the actor network: with hyperlink actor networks, the nodes in the actor network are web domains (in this document we use “site” and “domain” interchangeably). For the purpose of visualisation we will simplify the network by removing loops and multiple edges. Multiple edges can arise because multiple pages within a domain can link to the same or multiple pages within another domain. We will also create an edge weight (“weight”) to store this information on multiple links between domains.\n\n\n# create actor network: nodes are site domains of pages hyperlinks were collected from\nnet_actor <- crawlDF %>% Create(\"actor\")\ng_actor <- net_actor %>% Graph()\nvcount(g_actor)\n\n[1] 497\n\necount(g_actor)\n\n[1] 2826\n\n#simplify the network - remove loops and multiple edges\n#we will also create an edge attribute \"weight\"\nE(g_actor)$weight <- 1\ng_actor <- simplify(g_actor)\necount(g_actor)\n\n[1] 559\n\n\n\npng(\"actor_network.png\", width=800, height=800)\nplot(g_actor, layout=layout_with_fr(g_actor), vertex.label=\"\", vertex.size=3, edge.width=E(g_actor)$weight, edge.arrow.size=0.5)\ndev.off()\n\n\nFigure 2: Hyperlink actor network - Nodes are web domains.\n\ncc <- components(g_actor)\nstr(cc)\n\nList of 3\n $ membership: Named num [1:497] 1 1 1 1 1 1 1 1 1 1 ...\n  ..- attr(*, \"names\")= chr [1:497] \"1001lakes.com\" \"2022.mydata.org\" \"abo-peoples.org\" \"ada-x.org\" ...\n $ csize     : num 497\n $ no        : int 1\n\nThe actor network has 497 nodes and 559 edges. The above indicates we have a single (weakly-connected) component: with hyperlink actor networks seeds are more likely to be connected to one another (either directly or indirectly) as they only need to link to a common domain.\n4. Processing nodes in the hyperlink actor network\nFor the rest of this exercise we will use the actor network. We will now look at three approaches for processing hyperlink network data: pagegrouping, pruning, and preserving.\n4.1 Pagegrouping\nPagegrouping refers to merging nodes within a hyperlink network. A common example of a situation where you may want to apply pagegrouping is when you would like to ensure that a subdomain node is not shown separately in the network i.e. it is merged with its domain node. For example, we might want to merge www.anu.edu.au with anu.edu.au (or rename www.anu.edu.au to anu.edu.au if the latter does not already exist). It should be noted that whether this pagegrouping is enacted is dependent on the research setting: it might be the case that you want to keep particular subdomains separate to their parent domain. For example we might want rsss.anu.edu.au or cass.anu.edu.au to be separate nodes, and not merged with anu.edu.au.\nThere are two approaches we can used to undertake pagerouping: (1) operating on the vosonSML network object (that is then used to create the igraph graph object); (2) operating on the igraph graph object. In this document we will demonstrate method (2).\n4.1.1 Merging the “www” subdomain into a canonical domain\nFirst we are going to consolidate nodes into a single canonical domain. This will involve stripping “www” from www.x.y if only www.x.y exists and if both www.x.y and x.y exist then the former will be merged into the latter. Note that the following code also works if the canonical domain contains three or more parts e.g. x.y.z. Also note that the following step will only modify the www subdomain (e.g. www.x.y): other subbdomains (e.g. othersub.x.y) are handled below.\nLet’s focus on an example present in this dataset.\n\n\ng <- g_actor             #easier to write code\n\nV(g)$name[grep(\"mydata.org\", V(g)$name)]\n\n[1] \"2022.mydata.org\"   \"mydata.org\"        \"oldwww.mydata.org\"\n[4] \"www.mydata.org\"   \n\nSo this first step will involve merging www.mydata.org into mydata.org.\n\n\ng <- g_actor             #easier to write code\n\n#create vector of the sites that start with \"www\"\nt2 <- 1\nwww_sites <- V(g)$name[grep(\"^www\\\\.\", V(g)$name)]\nfor (c in www_sites){\n\n  #cat(\"working on:\", c, \"\\n\")\n  if (t2%%100==0)\n    cat(\"Finished working on\", c, \"(\", t2, \"of\", length(www_sites), \")\\n\")\n\n  ind_i <- grep(paste0(\"^\",c,\"$\"), V(g)$name)\n  i <- str_remove(c,\"^www.\")\n  \n  #if (c==\"www.mydata.org\")\n  #  break\n  #next\n  #print(i)\n  #print(V(g)$name[grep(i, V(g)$name)])\n  #num_parts <- str_count(c, \"\\\\.\")\n  #print(num_parts)\n  #ind <- grep(paste0(i,\"$\"), as.character(V(g)$name))\n  #ind <- grep(paste0(\"\\\\.\",i,\"$\"), as.character(V(g)$name))\n  #this is a hack...want to match on \".abc.com\" or \"abc.com\"\n  #ind <- union(grep(paste0(\"\\\\.\",i,\"$\"), as.character(V(g)$name)), grep(paste0(\"^\",i,\"$\"), as.character(V(g)$name)))\n  \n  ind <- grep(paste0(\"^\",i,\"$\"), as.character(V(g)$name))\n\n  if (!length(ind)){     #there is only the \"www\" version, just rename node to domain\n    V(g)$name[ind_i] <- i\n    t2 <- t2 + 1\n    next\n  }\n    \n  #Otherwise, we have two versions: www.x.y and x.y: merge these\n  #Note we will deal with situation where we have e.g. othersubdomain.x.y below\n  \n  ind <- sort(c(ind_i, ind))\n  #ind <- sort(ind)\n  #print(ind)\n\n  #if (length(ind)>1)\n  #  break\n    \n  #merging nodes involves creating a map of vertices e.g. if we have 5 vertices \n  #and want to merge nodes 1 and 2 and also merge nodes 3 and 4 the map needs to be:\n  #1 1 2 2 3\n  #i.e. nodes 1 and 2 become node 1, nodes 3 and 4 become node 2, and node 5 becomes node 3\n  map_i <- 1:ind[1]\n  t <- ind[1]+1\n  for (j in (ind[1]+1):vcount(g)){\n    #print(j)\n    if (j %in% ind){     #node to merge \n      map_i <- c(map_i, ind[1])\n    }else{               #not node to merge\n      map_i <- c(map_i, t)\n      t <- t + 1\n    }\n  }\n  \n  #need to use vertex.attr.comb=\"first\" or else get weird lists in attribute\n  #and it messes things up.  Replaced anyway...\n  g <-contract(g, map_i, vertex.attr.comb=\"first\")\n  V(g)$name[ind[1]] <- i                #rename the node to the non-www version\n  \n  t2 <- t2 + 1\n  \n}\n\nFinished working on www.globalhealthstrategies.com ( 100 of 262 )\nFinished working on www.privacyshield.gov ( 200 of 262 )\n\n#We have reduced the number of nodes from:\nvcount(g_actor)\n\n[1] 497\n\n#to\nvcount(g)\n\n[1] 478\n\n4.1.2 Merging other subdomains into the canonical domain\nNow have only a single canonical version of the mydata.org domain, but there are still two subdomains (other than www).\n\n\nV(g)$name[grep(\"mydata.org\", V(g)$name)]\n\n[1] \"2022.mydata.org\"   \"mydata.org\"        \"oldwww.mydata.org\"\n\nIn this next step we will merge all remaining subdomains into the canonical domain.\nHowever, there might be examples of subdomains that we want “preserved” (not merged into the canonical domain); they should be specified in advance.\nA good example is websites created using Wordpress.\n\n\nV(g)$name[grep(\"wordpress.com\", V(g)$name)]\n\n[1] \"coporwa1en.wordpress.com\"              \n[2] \"datasovereigntynow.files.wordpress.com\"\n[3] \"en.wordpress.com\"                      \n[4] \"indigenousdatalab.wordpress.com\"       \n[5] \"institutdayakologi.wordpress.com\"      \n[6] \"isocbg.wordpress.com\"                  \n[7] \"ourgovdotin.files.wordpress.com\"       \n[8] \"subscribe.wordpress.com\"               \n[9] \"wordpress.com\"                         \n\nIf we don’t “preserve” these wordpress websites (which are run by different organisations or groups) then they will all be merged into a single node wordpress.com.\nWe will implement two types of preserving: (1) preserve specific subdomains, (2) preserve all subdomains via a wildcard match.\nIn the example below we will preserve specific wordpress subdomains (and the rest will be merged into the canonincal domain wordpress.com) and we will preserve all subdomains for mydata.org.\nA final point to note is that the “wildcard” approach to preserving subddomains (what we are doing for mydata.org) will only work if mydata.org exists in the network. From the following, we can see that mydata.org exists, but fraunhofer.de does not exist.\n\n\nV(g)$name[grep(\"mydata.org\", V(g)$name)]\n\n[1] \"2022.mydata.org\"   \"mydata.org\"        \"oldwww.mydata.org\"\n\nV(g)$name[grep(\"fraunhofer.de\", V(g)$name)]\n\n[1] \"isst.fraunhofer.de\"              \n[2] \"websites.fraunhofer.de\"          \n[3] \"dataspaces.fraunhofer.de\"        \n[4] \"iee.fraunhofer.de\"               \n[5] \"iml.fraunhofer.de\"               \n[6] \"medical-data-space.fraunhofer.de\"\n\nIf the canonical domain doesn’t exist, as in the case of fraunhofer.de, then we need a separate process to undertake pagegrouping. We do this for fraunhofer.de in the section on “custom merging” below.\n\n\n#vector containing the information for preserving subdomains\n#we would most likely store this as a csv file with notes on choices made\npreserve <- c(\"*mydata.org\", \"coporwa1en.wordpress.com\", \"institutdayakologi.wordpress.com\",\n              \"indigenousdatalab.wordpress.com\")\n\nfor (i in V(g)$name){\n\n  #cat(\"working on:\", i, \"\\n\")\n  ind_i <- which(V(g)$name==i)\n  \n  #want to match only on \".x.y\" not \"x.y\"\n  ind <- grep(paste0(\"\\\\.\",i,\"$\"), as.character(V(g)$name))\n  \n  #skip any subdomains that are to be preserved\n  ind_g <- grep(i, preserve)\n  for (j in ind_g){\n    if ( grepl(\"^\\\\*\",preserve[j]) ){        #wildcard, skip all subdomains\n      ind <- NULL\n      break\n    }else{\n      ind_rem <- which(V(g)$name==preserve[j])\n      ind <- ind[-which(ind==ind_rem)]\n    }\n  }\n      \n  if (!length(ind))              #there is no subdomain(s)\n    next\n\n  cat(\"working on:\", i, \"\\n\")\n  \n  #We have one or more subdomains\n\n  print(V(g)$name[ind])\n\n  ind <- sort(c(ind_i, ind))\n  print(ind)\n\n  map_i <- 1:ind[1]\n  t <- ind[1]+1\n  for (j in (ind[1]+1):vcount(g)){\n    #print(j)\n    if (j %in% ind){     #node to merge\n      map_i <- c(map_i, ind[1])\n    }else{               #not node to merge\n      map_i <- c(map_i, t)\n      t <- t + 1\n    }\n  }\n  \n  g <-contract.vertices(g, map_i, vertex.attr.comb=\"first\")\n  V(g)$name[ind[1]] <- i\n\n}\n\nworking on: arizona.edu \n[1] \"nni.arizona.edu\"           \"nnigovernance.arizona.edu\"\n[1]   9 163 164\nworking on: innovalia.org \n[1] \"idsa.innovalia.org\"\n[1] 118 127\nworking on: internationaldataspaces.org \n[1] \"docs.internationaldataspaces.org\"\n[1]  66 128\nworking on: wordpress.com \n[1] \"datasovereigntynow.files.wordpress.com\"\n[2] \"en.wordpress.com\"                      \n[3] \"isocbg.wordpress.com\"                  \n[4] \"ourgovdotin.files.wordpress.com\"       \n[5] \"subscribe.wordpress.com\"               \n[1]  50  79 131 168 200 228\nworking on: amazon.com \n[1] \"aws.amazon.com\"\n[1]  13 240\nworking on: google.com \n[1] \"docs.google.com\"     \"maps.google.com\"     \"podcasts.google.com\"\n[4] \"policies.google.com\"\n[1]  65 143 172 173 323\nworking on: linkedin.com \n[1] \"de.linkedin.com\"\n[1]  53 364\nworking on: microsoft.com \n[1] \"azure.microsoft.com\"\n[1]  14 376\nworking on: nativeweb.org \n[1] \"ayf.nativeweb.org\"   \"saiic.nativeweb.org\"\n[1] 248 380 413\nworking on: wto.org \n[1] \"docs.wto.org\"\n[1]  67 456\n\n#We have reduced the number of nodes from:\nvcount(g_actor)\n\n[1] 497\n\n#to\nvcount(g)\n\n[1] 459\n\nThe following shows that the correct subdomains have been preserved. The two subdomains of mydata.org have been preserved.\n\n\nV(g)$name[grep(\"mydata.org\", V(g)$name)]\n\n[1] \"2022.mydata.org\"   \"mydata.org\"        \"oldwww.mydata.org\"\n\nWe have preserved three subdomains of wordpress.com, and the rest have been merged into the canonical domain.\n\n\nV(g)$name[grep(\"wordpress.com\", V(g)$name)]\n\n[1] \"coporwa1en.wordpress.com\"        \n[2] \"wordpress.com\"                   \n[3] \"indigenousdatalab.wordpress.com\" \n[4] \"institutdayakologi.wordpress.com\"\n\n4.1.3 Custom merging of domains\nThe above code showed how to merge subdomains “in bulk”, and we also showed how to control this merging by “preserving” subdomains. But we might need to undertake an additional step for merging domains and subdomains. When an organisation uses two or more websites, it might be desirable to merge the relevant domains into a single node in the actor network. The following approach also takes account of the situation we found above for fraunhofer.de: the bulk merging did not work because the canonical domain fraunhofer.de doesn’t yet exist in our hyperlink actor network.\nThe following code does this with reference to the following examples:\n\n\nV(g)$name[grep(\"womeninlocalization\", V(g)$name)]\n\n[1] \"womeninlocalization.com\" \"womeninlocalization.org\"\n\nV(g)$name[grep(\"fraunhofer\", V(g)$name)]\n\n[1] \"isst.fraunhofer.de\"              \n[2] \"websites.fraunhofer.de\"          \n[3] \"dataspaces.fraunhofer.de\"        \n[4] \"iee.fraunhofer.de\"               \n[5] \"iml.fraunhofer.de\"               \n[6] \"medical-data-space.fraunhofer.de\"\n\nV(g)$name[grep(\"undocs|undp|uneca|unpo|unhcr|unstats|unwomen|unesco|un\\\\.org\", V(g)$name)]\n\n [1] \"digitallibrary.un.org\" \"en.unesco.org\"        \n [3] \"lac.unwomen.org\"       \"tierracomun.org\"      \n [5] \"undocs.org\"            \"unstats.un.org\"       \n [7] \"undp.org\"              \"uneca.org\"            \n [9] \"unhcr.org\"             \"unpo.org\"             \n\nAs with the above, this step involves pre-specification of how the sites are to be merged. We will use a wildcard to merge the two womeninlocalization sites. We will also bulk merge subdomains of fraunhofer.de, preserving two subdomains. Finally, we will merge selected domains from the UN into a canonical domain un.org. Note that in the case of fraunhofer and the UN, the decision of exactly which subdomains and domains to merge is arbitrary: this is just for purposes of showing example code.\n\n\n#vector containing the information for preserving subdomains\n#we would most likely store this as a csv file with notes on choices made\npreserve <- c(\"dataspaces.fraunhofer.de\",\"medical-data-space.fraunhofer.de\")\n\n#the above process of merging subdomains will only work if there was www.x.y in the original dataset\n#if this is not the case, and want to merge subdomains using a wildcard, then specify the canonical domain here\n#note that here we use a dataframe with the first column being the canonical domain (what other domains/subdomains will be merged into), and subsequent columns are the other domains/subdomains.  If only the first column is specified, then a bulk merge will be undertaken\n#There are two types of merge below:\n# (1) bulk merge: this is when there is only one column in the data frame, the canonical domain.  This domain needs to be contained in the subdomains that are to be merged\n# (2) bespoke merge: this is when the first column contains the domain which all the other subdomains/domains will be merged into.  Note that this domain does not even need to exist in the hyperlink actor network\n#the following will only work if the canonical domain is contained in at least one \n#Note that this would normally be stored in csv file\nex1 <- data.frame(V1=\"womeninlocalization.org\", V2=\"womeninlocalization.com\")    #bespoke merge\nex2 <- data.frame(V1=\"fraunhofer.de\")                                            #bulk merge\nex3 <- data.frame(V1=\"un.org\", V2=\"en.unesco.org\", V3=\"lac.unwomen.org\", V4=\"undocs.org\", V5=\"unstats.un.org\", V6=\"undp.org\", V7=\"uneca.org\")         #bespoke merge\nmergeDF <- bind_rows(ex1, ex2, ex3)\n#note that we preserve particular subdomains of fraunhofer.de by including in preserve above\n\n#seems most efficient to iterate over nodes first, rather than iterating over merges first\nfor (i in V(g)$name){\n  \n  #cat(\"working on:\", i, \"\\n\")\n  ind_i <- which(V(g)$name==i)\n\n  for (m in 1:nrow(mergeDF)){\n    col1 <- mergeDF[m,1]\n    othcol <- NULL\n    for (m2 in 2:ncol(mergeDF)){\n      if ( !is.na(mergeDF[m,m2]) )\n        othcol <- c(othcol, mergeDF[m,m2])\n      else\n        break\n    }\n    #print(col1)\n    #print(othcol)\n\n    if (is.null(othcol))                 #bulk match - grep match on first col\n      ind_g <- grep(col1,i)              #should search on \".[pattern]\"\n    else\n      ind_g <- match(i,othcol)           #exact match on othcol\n\n    if (!length(ind_g))\n      ind_g <- NA\n      \n    if (!is.na(ind_g))                   #we have a match\n      break\n  }\n\n  if (is.na(ind_g))        #no merging for this node\n    next\n    \n  #we are going to merge only node i (this is different to the above code)\n    \n  #break\n  \n  #but first check that this node is not to be preserved\n  #can be exact match for this\n  if (!is.na(match(i, preserve))){\n    cat(i, \"is to be preserved\\n\")\n    next\n  }\n  \n  #Also check that the node we are merging is not the same as the node we are merging to i.e. not merging\n  #fraunhofer.de -> fraunhofer.de\n  if (i==col1){\n    cat(i, \", trying to merge node with itself...\\n\")\n    next\n  }\n    \n  #break\n  \n  cat(i, \"merged to\", col1, \"\\n\") \n\n  #next we check if the node that i is being merged with even exists\n  #if it doesn't then we just rename i, and we are done\n  dom_match <- match(col1, V(g)$name)\n  if (is.na(dom_match)){\n    V(g)$name[ind_i] <- col1\n    cat(\"renaming\", i, \"to\", col1, \"\\n\")\n    next\n  }\n   \n  #break\n  \n  #the following will only have two nodes in it, but use it so can re-use code from above\n  ind <- sort(c(ind_i, dom_match))\n  \n  #break\n  \n  map_i <- 1:ind[1]\n  t <- ind[1]+1\n  for (j in (ind[1]+1):vcount(g)){\n    #print(j)\n    if (j %in% ind){     #node to merge\n      map_i <- c(map_i, ind[1])\n    }else{               #not node to merge\n      map_i <- c(map_i, t)\n      t <- t + 1\n    }\n  }\n  \n  g <-contract.vertices(g, map_i, vertex.attr.comb=\"first\")\n  V(g)$name[ind[1]] <- col1\n  \n}\n\nen.unesco.org merged to un.org \nrenaming en.unesco.org to un.org \nisst.fraunhofer.de merged to fraunhofer.de \nrenaming isst.fraunhofer.de to fraunhofer.de \nlac.unwomen.org merged to un.org \nundocs.org merged to un.org \nunstats.un.org merged to un.org \nwebsites.fraunhofer.de merged to fraunhofer.de \nwomeninlocalization.com merged to womeninlocalization.org \ndataspaces.fraunhofer.de is to be preserved\niee.fraunhofer.de merged to fraunhofer.de \niml.fraunhofer.de merged to fraunhofer.de \nmedical-data-space.fraunhofer.de is to be preserved\nundp.org merged to un.org \nuneca.org merged to un.org \n\n#We have reduced the number of nodes from:\nvcount(g_actor)\n\n[1] 497\n\n#to\nvcount(g)\n\n[1] 450\n\nThe following confirms that the merges have taken place correctly.\n\n\nV(g)$name[grep(\"womeninlocalization\", V(g)$name)]\n\n[1] \"womeninlocalization.org\"\n\nV(g)$name[grep(\"fraunhofer\", V(g)$name)]\n\n[1] \"fraunhofer.de\"                   \n[2] \"dataspaces.fraunhofer.de\"        \n[3] \"medical-data-space.fraunhofer.de\"\n\nV(g)$name[grep(\"undocs|undp|uneca|unpo|unhcr|unstats|unwomen|unesco|un\\\\.org\", V(g)$name)]\n\n[1] \"digitallibrary.un.org\" \"un.org\"               \n[3] \"tierracomun.org\"       \"unhcr.org\"            \n[5] \"unpo.org\"             \n\n4.2 Pruning\nPruning refers to removing nodes that are considered not relevant to the analysis. It is highly likely that the web crawler will pick up pages that are not relevant to the study, and so we use pruning to identify and remove these irrelevant pages.\n4.2.1 Creatng the network containing just the seed sites\nWe will first create an actor network containing just the seed sites: in effect we are “pruning” non-seed sites. To start with, we need to identify what are the seed sites: we do this using the pages dataframe we created above for the web crawl. Recall that the pages dataframe contains web pages that we have crawled, but the hyperlink actor network contains websites (or domains). So we need to process the pages dataframe so as to identify what are the seed sites (not seed pages).\n\n\n# identify the seed pages and set a node attribute\n# we are also removing http tag and trailing forward slash\nseed_sites <- pages %>%\n  mutate(site = str_remove(page, \"^http[s]?://\"), seed = TRUE)\n# also remove trailing \"/\"\nseed_sites <- seed_sites %>%\n  mutate(site = str_remove(site, \"/$\"))\n\n#The following will return just the domain name\na <- str_match(seed_sites$site, \"(.+?)/\")\nseed_sites$site <- ifelse(grepl(\"/\", seed_sites$site), a[,2], seed_sites$site)\n\n#remove redundant column \"page\" and also user-created column \"domain\" (to avoid confusion, since not using)\n#and put \"site\" colum first\nseed_sites <- seed_sites %>% select(-c(page,domain)) %>% relocate(site)\n\nseed_sites <- seed_sites %>% distinct(site, .keep_all=TRUE)\n\nkable(head(seed_sites))\n\nsite\ntype\nmax_depth\ncountry\nseed\nwomeninlocalization.com\nint\n1\nUS\nTRUE\niwgia.org\nint\n1\nDenmark\nTRUE\nwww.iwgia.org\nint\n1\nDenmark\nTRUE\nindigenousdatalab.org\nint\n1\nUS\nTRUE\nbotpopuli.net\nint\n1\nIndia\nTRUE\ncipesa.org\nint\n1\nSouth Africa\nTRUE\n\nnrow(seed_sites)\n\n[1] 21\n\nWe have created a dataframe seed_sites which contains the domains extracted from the seed pages used for crawling - this is stored in the column site. We have created a column seed and set this to TRUE (this will be used below when we use this dataframe to identify seed sites in the hyperlink actor network). Note that there was also a column domain but this was manually created at the time the seed pages were identified (in the same way that the column was country was manually created); we do not make use of that column in what follows, and so we removed it from the seed_sites dataframe.\nBefore proceeding, we need to first make some modifications to the seed_sites dataframe. The pagegrouping step above means that we no longer have the “www” subdomain present in the hyperlink actor network, and so we need to modify seed_sites by stripping “www” from the seed sites stored in the column site. Also, the pagegrouping means that we no longer have womeninlocalization.com in the network, so we need to change the seed site to womeninlocalization.org.\n\n\nseed_sites <- seed_sites %>% mutate(site=gsub(\"^www\\\\.\",\"\",site))\n\nseed_sites$site[which(seed_sites$site==\"womeninlocalization.com\")] <- \"womeninlocalization.org\"\n\nseed_sites <- seed_sites %>% distinct(site, .keep_all=TRUE)\n\nkable(head(seed_sites))\n\nsite\ntype\nmax_depth\ncountry\nseed\nwomeninlocalization.org\nint\n1\nUS\nTRUE\niwgia.org\nint\n1\nDenmark\nTRUE\nindigenousdatalab.org\nint\n1\nUS\nTRUE\nbotpopuli.net\nint\n1\nIndia\nTRUE\ncipesa.org\nint\n1\nSouth Africa\nTRUE\nmydata.org\nint\n1\nFinland\nTRUE\n\nnrow(seed_sites)\n\n[1] 20\n\nSo we have 20 seed sites and we are now ready to create the “seeds only” hyperlink network. To do this, we will create a node attribute “seed” which indicates whether a site is a seed or not.\n\n\nV(g)$seed <- seed_sites$seed[match(V(g)$name, seed_sites$site)]\n\ntable(V(g)$seed)\n\n\nTRUE \n  20 \n\nThe above indicates that there are now 20 nodes in the network with the attribute seed equal to 1 (the rest have values of NA), so we have correctly identified our seed sites in the network. We can now visualise the seeds only hyperlink network.\n\n\n#for the remainder of this exercise we will work with the simiplified network\n#simplify the network - remove loops and multiple edges\nE(g)$weight <- 1\ng <- simplify(g)\n\ng_seeds <- induced.subgraph(g, which(V(g)$seed==1))\n\n\n\n\npng(\"seeds.png\", width=800, height=800)\nplot(g_seeds, vertex.label.color=\"black\", vertex.size=3, edge.width=E(g_seeds)$weight, edge.arrow.size=0.5)\ndev.off()\n\n\nFigure 3: Seeds hyperlink network.The above figure shows that the seed sites are very disconnected, with only four pairs of sites connected; this is to be expected given we have only crawled to depth 1 (i.e. only the seed page is crawled). If we increased the crawl depth to 2, then it is to be expected that the seeds hyperlink network would become more connected.\n4.2.2 Creatng the network containing the seeds plus “important” non-seed sites\nNext we will create the “seeds plus important” network: this contains the seed sites plus those other sites that are hyperlinked to by two or more seeds.\n\n\ng_seedsImp <- induced.subgraph(g, which(V(g)$seed==1 | (degree(g)>=2)))\n\n#red nodes are seeds, blue nodes are importnant non-seeds\nV(g_seedsImp)$color <- ifelse(V(g_seedsImp)$seed==1, \"red\", \"blue\")\n\n\n\n\npng(\"seeds_important.png\", width=800, height=800)\nplot(g_seedsImp, vertex.label.color=\"black\", vertex.size=3+degree(g_seedsImp, mode=\"in\"), edge.width=E(g_seedsImp)$weight, edge.arrow.size=0.5)\ndev.off()\n\n\n\nThe above visualisation indicates that we now have a much more connected network, indeed it is fully-connected (no isolates) but a casual inspection indicates that many of the so-called “important” sites are probably not relevant to the analysis. For example, the most central nodes are social media sites (e.g. twitter.com, youtube.com). This brings us to the final step in the processing of the hyperlink data: manually pruning irrelevant sites.\n4.2.3 Manually pruning irrelevant sites\nWhether a site is relevant to the research is something only the researcher will know, so this final step is necessarily a manual one. We recommend that the “seeds plus important” network nodes are written to csv file, and then this csv file is manually coded so either the relevant sites are kept or the irrelevant sites are removed. In the example below, we are keeping those sites that are deemed relevant.\n\n\n#write the sites to csv file, for manual coding\n#also write seed stauts, since this will be used to determine whether a site is relevant\nwrite.csv(data.frame(site=V(g_seedsImp)$name, seed=V(g_seedsImp)$seed), \"seeds_plus_important.csv\")\n\n\nThe sites are manually coded - a new column “keep” is created, with a 1 indicating that the site is either a seed or it is a relevant non-seed site. The coded csv file is then used to create the new sub-network.\n\n\ndf2 <- read.csv(\"seeds_plus_important_coded.csv\")\nV(g_seedsImp)$keep <- df2$keep[match(V(g_seedsImp)$name, df2$site)]\ng_seedsImp2 <- induced.subgraph(g_seedsImp, which(V(g_seedsImp)$keep==1))\n\n\n\n\npng(\"seeds_important2.png\", width=800, height=800)\nplot(g_seedsImp2, vertex.label.color=\"black\", vertex.size=3+degree(g_seedsImp2, mode=\"in\"), edge.width=E(g_seedsImp2)$weight, edge.arrow.size=0.5)\ndev.off()\n\n\n\nThe result is a graph presenting a connection or disconnection among organisations discussing ‘data sovereignty’ issues on the web. It shows that the debates interestingly place the already powerful institutions such as the UN and the World Bank at the center which makes them appear to be influential actors in the network of data sovereignty debates. Given their prominent positions in the network connecting otherwise separated organisations across countries or regions interested in data politics, it is interesting to gain insight into how organisations such as the UN and the World Bank impose their interest on data sovereignty agenda.\n4.3 Preserving\nWe have already mentioned preserving of subdomains above (to prevent all subdomains being merged into a single node when pagregrouping is undertaken). Another example of preserving is when we want to preserve sub-directories. By default, vosonSML does not preserve sub-directories and so www.example.com/subdir1 and www.example.com/subdir2 would, by default, be merged to the node www.example.com. Preserving these sub-diretories would involve working with the activity network object returned by vosonSML:\n\n\nhead(net_activity$edges)\n\n# A tibble: 6 × 2\n  from                                     to                         \n  <chr>                                    <chr>                      \n1 https://womeninlocalization.com/partners http://goap-global.com     \n2 https://womeninlocalization.com/partners http://www.reddit.com/subm…\n3 https://womeninlocalization.com/partners https://csa-research.com   \n4 https://womeninlocalization.com/partners https://euatc.org          \n5 https://womeninlocalization.com/partners https://locworld.com       \n6 https://womeninlocalization.com/partners https://slator.com         \n\nIt would be necessary to process the URLs in the from and to using similar techniques to those presented above e.g. converting URLs to the domain names, but preserving certain sub-directories. Then the vosonSML Create() function can be used to create the igraph graph object. We will not cover this in full here, but leave this for a future blog post.\n\n\n\nAckland, R. 2010. “WWW Hyperlink Networks.” In Analyzing Social Media Networks with NodeXL: Insights from a Connected World, edited by D. L. Hansen, B. Shneiderman, and M. A. Smith. Burlington, MA: Morgan-Kaufmann.\n\n\n———. 2013. Web Social Science: Concepts, Data and Tools for Social Scientists in the Digital Age. London: SAGE Publications.\n\n\nAckland, R., and M. O’Neil. 2011. “Online Collective Identity: The Case of the Environmental Movement.” Social Networks 33: 177–90. https://doi.org/https://doi.org/10.1016/j.socnet.2011.03.001.\n\n\nLusher, D., and R. Ackland. 2011. “A Relational Hyperlink Analysis of an Online Social Movement.” Journal of Social Structure 12 (5).\n\n\n\n\n",
    "preview": "posts/2023-01-20-hyperlink-networks/seeds_important2.png",
    "last_modified": "2023-02-22T22:50:26+11:00",
    "input_file": "hyperlink-networks.knit.md",
    "preview_width": 800,
    "preview_height": 800
  },
  {
    "path": "posts/2022-06-05-egocentric-networks-from-twitter-timelines/",
    "title": "Egocentric Networks from Twitter timelines",
    "description": "Demonstration of how to use rtweet and vosonSML to construct an ego net from Twitter users timelines.",
    "author": [
      {
        "name": "Bryan Gertzel",
        "url": {}
      }
    ],
    "date": "2022-08-22",
    "categories": [
      "rstats",
      "twitter",
      "networks",
      "egocentric",
      "rtweet",
      "vosonsml",
      "timeline"
    ],
    "contents": "\n\nContents\nIntroduction\nCollect the ego timeline\nCollect timelines of\nalters\nCreate an actor network\nOptionally add\nuser metadata as node attributes\nCreate an ego subgraph\nVisualise with an\ninteractive visNetwork plot\n\n\nUpdated for rtweet v1.0.2 and\nvosonSML\nv0.32.7.This article does not require Twitter API keys, but does\nrequire a twitter account and for the user to authorise the rtweet\nrstats2twitter app when prompted.\n\nIntroduction\nEgocentric networks or ego nets are networks that focus on a\nparticular actor (the ego) and map out their connections to other\nactors. In an ego net other actors are referred to as alters, and by\ncollecting the outward expanding connections of alters, ego nets of\nvarying degrees can be constructed (see Hogan,\n2011, p. 168). Some literature and software refer to these as\nneighborhood networks, with varying orders instead of degrees. For\nexample in a friendship network, a neighborhood network of the first\norder (1.0 degree) contains just the friends of the ego, whereas a\nnetwork of the second order (sometimes second step) also contains\n“friends of friends” (a 2.0 degree ego net).\nBy collecting the tweets in a Twitter users timeline, and the\ntimelines of users referenced, we can create a 1.0, 2.0 or 1.5 degree\nnetwork for the ego. A 1.5 degree network is similar to the 1.0 degree,\nexcept it also contains relationships or ties between the alters, or\n“between friends” of the ego from the previous friendship network\nexample.\nIt should be noted that by using user timelines that this is not\nnecessarily a friendship network, but instead a network of twitter users\nwho are associated through tweet activity. This kind of ego net can lead\nto insights beyond simply declared friendships (obtained from Twitter’s\nfriend/follower metadata) as the network structure is the result of\nusers interactions on the platform over a period of time.\nThis post will demonstrate how to construct an ego networks from a\ntwitter timelines using the rtweet package to collect\ntweets and vosonSML to create an actor network.\nCollect the ego timeline\nThe first step is to collect the ego’s timelime. In this post we will\nbe using the VOSON Lab @vosonlab twitter account, and\ncollecting the Twitter timeline using rtweet. The Twitter\nAPI restricts the number of timeline tweets that can be collected to the\nmost\nrecent 3,200 tweets, but we can set this to a lesser value e.g most\nrecent 100 tweets, and also use the same parameter for alters timelines\nfor the purposes of this demonstration.\n\n\nlibrary(dplyr)\nlibrary(DT)\nlibrary(rtweet)\nlibrary(vosonSML)\n\n# get twitter user timeline\nego_tweets <- get_timeline(c(\"vosonlab\"), n = 100, token = NULL)\n\n\n\n\n# convert rtweet data into vosonSML format\nego_tweets <- ego_tweets |> ImportRtweet()\n\n# create actor network from timeline tweets\nego_net <- ego_tweets |> Create(\"actor\", verbose = TRUE)\n\nGenerating twitter actor network...\n-------------------------\ncollected tweets | 100\ntweet mention    | 62\ntweet            | 25\nretweet          | 66\nquote mention    | 13\nquote            | 9 \nnodes            | 41\nedges            | 175\n-------------------------\nDone.\n\nA result of 41 nodes indicates that there are\n40 alters in the network with ties to the ego.\n\n\n\nFigure 1: vosonlab 1.0 degree actor network\n\n\n\nCollect timelines of alters\nFrom the previous step we created an actor network represented as\nnodes and edges dataframes from the ego’s tweet timeline. We can now use\nthis to extract all of the user ids of the alters in the network.\nNote that we have not specified the degree of the ego net at this\nstage, however by virtue of the twitter data (timeline tweets) having\nall been created by the ego user, we can assume all of the alters\n(referenced users) are connected to the ego in this network.\n\n\n# get ego user id\nego_user_id <- ego_net$nodes |>\n  filter(screen_name == \"vosonlab\") |> pull(user_id)\n\n# get list of alter user ids from network\nalter_user_ids <- ego_net$nodes |>\n  filter(user_id != ego_user_id) |> distinct(user_id) |> pull()\n\n\nUsing the alters user ids the timeline tweets can be collected and\nimported into vosonSML as follows:\n\n\n# get 100 most recent tweets from all of the alters timelines\n# and convert to vosonSML format\nalters_tweets <- alter_user_ids |>\n  get_timeline(n = 100, retryonratelimit = TRUE) |>\n  ImportRtweet()\n\n# Error: Number of tweet observations does not match number of users. 3526 != 99\n\n\n\nPlease note there seems to be an inconsistency in timeline results for\nthis version of rtweet and the following workaround can be used instead:\n\n\n\n# workaround for rtweet timeline users issue\nget_alters_timelines <- function(x) {\n  ImportRtweet(get_timeline(user = x, n = 100, retryonratelimit = TRUE))\n}\n\n# collects timelines individually and place into a list\nrequire(purrr)\nalters_tweets <- map(alter_user_ids, get_alters_timelines)\n\n\nAlternatively, if you have your own API access the\nvosonSML Collect function can also be used\nwith the endpoint = \"timeline\" parameter:\n\n\n# requires a previously saved vosonSML twitter auth object\nauth_twitter <- readRDS(\"~/.vsml_auth_tw\")\n\nalters_tweets2 <- auth_twitter |>\n  Collect(\n    endpoint = \"timeline\",\n    users = alter_user_ids,\n    numTweets = 100,\n    verbose = TRUE\n  )\n\n# Collecting timeline tweets for users...\n# Requested 4000 tweets of 150000 in this search rate limit.\n# Rate limit reset: 2022-08-22 06:21:30\n# \n# tweet        | status_id           | created            \n# --------------------------------------------------------\n# Latest Obs   | 1560130378366562304 | 2022-08-18 05:04:02\n# Earliest Obs | 1544727926645596162 | 2022-07-06 17:00:12\n# Collected 3525 tweets.\n# Done.\n\n\nCreate an actor network\nNow that all of the tweets from the alters timelines have also been\ncollected, the data can be merged and a single actor network created.\nThis actor network can be considered a 2.0 degree\nnetwork, as it contains not only the associations or “friends” from the\nego’s timeline, but also the associations or “friends” of the alters\nfrom their timelines.\n\n\n# combine all of the tweets from ego and alters timelines using vosonSML merge\ntweets <- do.call(Merge, alters_tweets)\ntweets <- Merge(ego_tweets, tweets)\n\n# create actor network from combined timeline tweets\nactor_net <- tweets |> Create(\"actor\", verbose = TRUE)\n\nGenerating twitter actor network...\n-------------------------\ncollected tweets | 3626\ntweet mention    | 1030\ntweet            | 1160\nretweet          | 1657\nreply mention    | 735\nreply            | 584\nquote mention    | 187\nquote            | 232\nnodes            | 1818\nedges            | 5585\n-------------------------\nDone.\n\nHere we can see an actor network of 1818 nodes and\n5585 edges, substantially larger than our initial actor\nnetwork.\n\n\n\nFigure 2: vosonlab 2.0 degree actor network\n\n\n\nOptionally add\nuser metadata as node attributes\nAt this point we can optionally add some user metadata to our network\nas node attributes. This allows us to change visual properties of the\nnetwork graph based on actor attributes. For example, we could map the\nnode size to number of followers a twitter user may have.\nPlease note this step requires a vosonSML\ntwitter auth object if you want to use the look up feature for\ncomplete users’ metadata.\n\n\n# this step requires a previously saved vosonSMML twitter auth object\nauth_twitter <- readRDS(\"~/.vsml_auth_tw\")\n\n# add user profile metadata\nactor_net_meta <- actor_net |>\n  AddUserData(tweets, lookupUsers = TRUE, twitterAuth = auth_twitter)\n\n\nHere is a sample of the actor metadata available and an example of\nhow it can be presented and explored using a data table:\n\n\n# node attributes\nnames(actor_net_meta$nodes)\n\n [1] \"user_id\"                 \"screen_name\"            \n [3] \"u.user_id\"               \"u.name\"                 \n [5] \"u.screen_name\"           \"u.location\"             \n [7] \"u.description\"           \"u.url\"                  \n [9] \"u.protected\"             \"u.followers_count\"      \n[11] \"u.friends_count\"         \"u.listed_count\"         \n[13] \"u.created_at\"            \"u.favourites_count\"     \n[15] \"u.verified\"              \"u.statuses_count\"       \n[17] \"u.profile_banner_url\"    \"u.default_profile\"      \n[19] \"u.default_profile_image\" \"u.withheld_in_countries\"\n[21] \"u.derived\"               \"u.withheld_scope\"       \n[23] \"u.utc_offset\"            \"u.time_zone\"            \n[25] \"u.geo_enabled\"           \"u.lang\"                 \n[27] \"u.has_extended_profile\" \n\n# explore actors metadata\nactors_table <- actor_net_meta$nodes |>\n  filter(user_id %in% c(ego_user_id, alter_user_ids)) |>\n  mutate(u.screen_name = paste0(\"@\", screen_name)) |>\n  select(name = u.screen_name,\n         display = u.name,\n         locationu = u.location,\n         description = u.description,\n         followers = u.followers_count,\n         tweets = u.statuses_count) |>\n  slice_head(n = 5)\n\nlibrary(reactable)\n\nreactable(actors_table, bordered = TRUE, striped = TRUE, resizable = TRUE,\n          wrap = FALSE, searchable = TRUE, paginationType = \"simple\")\n\n\n\nCreate an ego subgraph\nA 1.5 degree network can be useful to reveal the associations between\nan ego’s alters. This can be achieved by creating a subgraph of the 2.0\nego network that retains only the previously identified alters (see\nbelow igraph::induced_subgraph). As we know every alter is\nconnected to the ego so it is also often useful to visualise ego\nnetworks without the ego as it is then easier to observe clustering.\n\n\nlibrary(igraph)\n\n# use the vosonSML to convert the network dataframes into an igraph object\ng <- actor_net_meta |> Graph()\n\n# create a subgraph with ego removed\ng2 <- induced_subgraph(g, c(alter_user_ids))\n\ng2\n\nIGRAPH 953ba4f DN-- 40 1922 -- \n+ attr: type (g/c), name (v/c), screen_name (v/c), u.user_id\n| (v/c), u.name (v/c), u.screen_name (v/c), u.location (v/c),\n| u.description (v/c), u.url (v/c), u.protected (v/l),\n| u.followers_count (v/n), u.friends_count (v/n),\n| u.listed_count (v/n), u.created_at (v/c), u.favourites_count\n| (v/n), u.verified (v/l), u.statuses_count (v/n),\n| u.profile_banner_url (v/c), u.default_profile (v/l),\n| u.default_profile_image (v/l), u.withheld_in_countries\n| (v/x), u.derived (v/c), u.withheld_scope (v/l), u.utc_offset\n| (v/l), u.time_zone (v/l), u.geo_enabled (v/l), u.lang (v/l),\n| u.has_extended_profile (v/l), status_id (e/c), created_at\n| (e/c), edge_type (e/c)\n+ edges from 953ba4f (vertex names):\n\nAs we saw in our initial actor network constructed from only the\nego’s timeline we now have 40 nodes again, matching the\nnumber of alters. This actor network has many more edges however, as\n1922 ties or relations between the alters were captured\nfrom the collection of the alters timelines.\nVisualise with an\ninteractive visNetwork plot\nUsing the igraph and visNetwork package we\ncan create a simplified and undirected ego network graph of alters.\nCommunity detection can be performed and visualised using the\nigraph walktrap clustering algorithm and\nFruchterman-Reingold force-directed layout. We can further map some\nvisual properties of nodes to attributes - with node size corresponding\nto the node degree, edge width to combined weight, and color to\nclustering community group.\n\n\nlibrary(visNetwork)\n\n# combine and weight the edges between nodes\nE(g2)$weight <- 1\ng2 <- igraph::simplify(g2, edge.attr.comb = list(weight = \"sum\"))\ng2 <- as.undirected(g2)\n\n# perform some community detection using a random walk algorithm\nc <- walktrap.community(g2)\nV(g2)$group <- membership(c)\n\n# map visual properties of graph to attributes\nE(g2)$width <- ifelse(E(g2)$weight > 1, log(E(g2)$weight) + 1, 1.1)\n\nV(g2)$size <- degree(g2) + 5\nV(g2)$label <- paste0(\"@\", V(g2)$u.screen_name)\n\nvisIgraph(g2, idToLabel = FALSE) |>\n  visIgraphLayout(layout = \"layout_with_fr\") |>\n  visOptions(\n    nodesIdSelection = TRUE,\n    highlightNearest = TRUE\n  )\n\n\n\n\nFigure 3: vosonlab 1.5 degree actor network\n\n\n\nThe final result is an ego net with some clear associations. Colours\nand placement of nodes found to represent some interesting domains and\ncommunity relationships between the Twitter @vosonlab\naccount and its timeline network alters. Isolates represent more distant\nconnections with no detected community ties.\n\n\n\nHogan, B. (2011). Chapter 11 - visualizing and interpreting facebook\nnetworks [Book Section]. In D. L. Hansen, B. Shneiderman & M. A.\nSmith (Eds.), Analyzing social media networks with NodeXL (pp.\n165–179). Morgan Kaufmann. https://doi.org/10.1016/B978-0-12-382229-1.00011-4\n\n\n\n\n",
    "preview": "posts/2022-06-05-egocentric-networks-from-twitter-timelines/egocentric_500x500.png",
    "last_modified": "2022-08-25T15:58:00+10:00",
    "input_file": {},
    "preview_width": 500,
    "preview_height": 500
  },
  {
    "path": "posts/2021-12-06-collecting-youtube-comments-with-vosondash/",
    "title": "Collecting YouTube comments with VOSONDash",
    "description": "Collect YouTube comments and create networks for analysis with VOSONDash",
    "author": [
      {
        "name": "Francisca Borquez",
        "url": {}
      }
    ],
    "date": "2021-12-06",
    "categories": [
      "rstats",
      "SNA",
      "VOSONDash",
      "networks",
      "YouTube",
      "visualisation"
    ],
    "contents": "\nThis guide provides a practical demonstration for collecting comments from YouTube videos and constructing networks, using the VOSON Lab’s interactive R/Shiny app VOSONDash.\nThe GitHub page provides instructions to install and run VOSONDash. More information on features can be accessed in the VOSONDash Userguide(Borquez et al. 2020).\nCollecting YouTube data\nAs for Twitter, YouTube collection requires API keys, which are provided via the Google API console. Similarly, we enter the key in the API Keys window, YouTube tab in VOSONDash and the token can be saved to disk for future use.\nIn this example, we are collecting comments from a YouTube video titled Update on reinfection caused by Omicron variant, which was uploaded by the World Health Organization (WHO) on 5th December 2021 and had attracted 182 comments at the time of data collection (17 December 2021).\nYouTube networks\nVOSONDash (via vosonSML) provides two types of YouTube networks:\nActivity networks – nodes are either comments or videos (videos represent a starting comment), and edges are interactions between comments. In this example, there are 119 nodes and 118 edges. The most central node is the initial post, which receives 100 comments.\nActor networks – nodes are users who have commented on videos and the videos themselves are included in the network as special nodes. Edges are the interactions between users in the comments. We can distinguish two types of edges “top-level comment,” which is a reply to the initial post (video), and “reply to a reply,” when users mention the username of the person they are replying to. In this example, there are 81 nodes and 119 edges. The most central node is the initial post, which receives 110 comments.\nFigure 1: VOSONDash – YouTube collection, Activity and Actor networksThe Blog post Analysing online networks with VOSONDash provides more detail into features for network analysis, network visualisation and text analysis.\n\n\n\nBorquez, F., B. Gertzel, X. Cai, and R. Ackland. 2020. VOSON Dashboard Userguide. Canberra, Australia: VOSON Lab, Australian National University. https://vosonlab.github.io/VOSONDashDocs/.\n\n\n\n\n",
    "preview": "posts/2021-12-06-collecting-youtube-comments-with-vosondash/youtube.png",
    "last_modified": "2022-08-04T14:14:49+10:00",
    "input_file": {},
    "preview_width": 1283,
    "preview_height": 635
  },
  {
    "path": "posts/2021-11-25-collecting-twitter-data-with-vosondash/",
    "title": "Collecting Twitter data with VOSONDash",
    "description": "A short guide to collecting Twitter data and constructing networks for analysis with VOSONDash.",
    "author": [
      {
        "name": "Francisca Borquez",
        "url": {}
      }
    ],
    "date": "2021-11-25",
    "categories": [
      "rstats",
      "SNA",
      "VOSONDash",
      "networks",
      "twitter",
      "visualisation"
    ],
    "contents": "\nThis guide provides a practical demonstration for collecting Twitter data and constructing networks, using VOSON Lab’s interactive R/Shiny app VOSONDash.\nThe GitHub page provides instructions to install and run VOSONDash. More information on features can be accessed in the VOSONDash Userguide(Borquez et al. 2020).\nCollecting Twitter data\nTwitter collection require authentication with OAuth1.0 API keys, provided via Twitter Developer account. Simply, enter the fields in the API Keys window in VOSONDash. The token can be saved to disk for future use.\nIn this example, data were collected on 6 December 2021 and include 200 recent tweets with the hashtags #auspol and #COVID-19 (Fig. 1).\nTwitter networks\nVOSONDash – via vosonSML– provides four types of Twitter networks for analysis:\nActivity networks – where nodes represent tweets and edge types are: replies, retweets and quoted retweets. In this example, there are 225 nodes (excluding isolates) and 200 edges (including multiple edges and loops).\nActor networks – where nodes represent users who have tweeted, or else are mentioned or replied to in tweets. Edges represent interactions between Twitter users, and an edge attribute indicates whether the interaction is a mention, reply, retweet, quoted retweet or self-loop. In this example, there are 212 nodes and 213 edges (including multiple edges and loops).\nFigure 1: VOSONDash – Twitter collection, Activity and Actor networksTwo-mode networks – where nodes are actors (Twitter users) and hashtags, and there is an edge from user i to hashtag j if user i authored a tweet containing hashtag j. In this example, we have removed the hashtags we used in our collection #auspol and #COVID-19. The resulting network has 242 nodes and 213 edges. When clicking on Label attribute, we can observe hashtags and handles used in those tweets.\nFigure 2: VOSONDash – Two-mode networkSemantic networks – where nodes represent entities extracted from the tweet text: common words, hashtags and usernames. Edges reflect co-occurrence of terms. In this example, we have removed the terms #auspol and #COVID-19, and set the parameters to include 5% most frequent words and 50% most frequent hashtags. The resulting network has 55 nodes (excluding isolates), and 127 edges. Label attribute option displays terms in network visualisation.\nFigure 3: VOSONDash – Semantic networkTo learn more about VOSONDash network and text analysis features, see our previous post Analysing online networks with VOSONDash.\n\n\n\nBorquez, F., B. Gertzel, X. Cai, and R. Ackland. 2020. VOSON Dashboard Userguide. Canberra, Australia: VOSON Lab, Australian National University. https://vosonlab.github.io/VOSONDashDocs/.\n\n\n\n\n",
    "preview": "posts/2021-11-25-collecting-twitter-data-with-vosondash/VOSONDash-t.png",
    "last_modified": "2021-12-16T23:45:37+11:00",
    "input_file": {},
    "preview_width": 1366,
    "preview_height": 768
  },
  {
    "path": "posts/2021-10-01-testing-bot-status-of-twitter-users/",
    "title": "Testing the bot status of users in Twitter networks collected via vosonSML",
    "description": "We use vosonSML to collect Twitter data and then use Botometer to test the bot status of a subset of Twitter users.",
    "author": [
      {
        "name": "Robert Ackland",
        "url": {
          "https://orcid.org/0000-0002-0008-1766": {}
        }
      },
      {
        "name": "Francisca Borquez",
        "url": {}
      }
    ],
    "date": "2021-10-08",
    "categories": [
      "rstats",
      "python",
      "SNA",
      "vosonSML",
      "networks",
      "Botometer",
      "bot detection"
    ],
    "contents": "\nIntroduction\nAs social media platforms like Twitter become important spaces for information diffusion, discussion and opinion formation, serious concerns have been raised about the role of malicious socialbots in interfering, manipulating and influencing communication and public opinion Badawy, Ferrara, and Lerman (2018). Their detection and the understanding of consequent dynamics of behaviour are relevant to researchers and it is central to the research collaboration the VOSON Lab is involved in (see Cimiano et al. 2020).\nIn this post we will use vosonSML to collect data via the Twitter API and construct a network represented as an igraph graph object in R. Then, we will identify a subset of users and test their bot status using Botometer1 (in python) and include the bot scores as node attributes in the network graph in R.\nCollecting the Twitter network using vosonSML in R\nThe vosonSML vignette (Ackland, Gertzel, and Borquez 2020) provides comprehensive instructions on how to use vosonSML. In this post, we are going to focus on the essential steps for Twitter collection and network generation via vosonSML. The first step involves loading the vosonSML package into the R session, and use the Web Auth approach to create a Twitter API access token:\n\n\nlibrary(magrittr)\nlibrary(vosonSML)\n\ntwitterAuth <-\n   Authenticate(\n      \"twitter\",\n      appName = \"An App\",\n      apiKey = \"xxxxxxxxxxxx\",\n      apiSecret = \"xxxxxxxxxxxx\")\n\n#Optionally, save the access token to disk:\nsaveRDS(twitterAuth, file = \"twitter_auth.rds\")\n\n#The following loads into the current session a previously-created access token:\ntwitterAuth <- readRDS(\"twitter_auth.rds\")\n\n\n\nThen, we collect 100 tweets that contain the Australian politics hashtag #auspol. Data is saved in .rds dataframe format.\n\n\ntwitterData <- twitterAuth %>%\n   Collect(\n      searchTerm = \"#auspol\",\n      numTweets = 100,\n      includeRetweets = FALSE,\n      retryOnRateLimit = TRUE,\n      writeToFile = TRUE)\n\n\n\nTo read the Twitter dataframe from disk, the ImportData() function modifies the class values for the object before it is used with vosonSML:\n\n\ntwitterData <- ImportData(\"2021-09-30_182359-TwitterData.rds\", \"twitter\")\n\n\n\nAnd now we use the vosonSML functions Create(\"actor\") to create an Actor network and Graph() to create an igraph graph object g.\nThe Create(\"actor\") function generates a named list containing two dataframes named nodes and edges. In this Actor network nodes are users who have either tweeted using the search term #auspol, or else are mentioned or replied to in tweets featuring the search terms. Edges represent interactions between Twitter users, and an edge attribute indicates whether the interaction is a mention, reply, retweet, quoted retweet or self-loop.\n\n\nactorNetwork <- twitterData %>% Create(\"actor\", vertbose=TRUE)\ng <- actorNetwork %>% Graph()\ng\n\n\n\nThe output in the console loos like this:\n\n> actorNetwork <- twitterData %>% Create(\"actor\", vertbose=TRUE)\nGenerating twitter actor network...\n-------------------------\ncollected tweets | 100\ntweet mention    | 119\ntweet            | 62\nreply mention    | 27\nreply            | 21\nquote mention    | 5 \nquote            | 17\nnodes            | 226\nedges            | 251\n-------------------------\nDone.\n`> g <- actorNetwork %>% Graph()\nCreating igraph network graph...Done.\n\nThe graph object g prints as follows:\n\n> g\nGRAPH 527682d DN-- 226 251 -- \n+ attr: type (g/c), name (v/c), screen_name (v/c), status_id (e/c), created_at (e/c),\n| edge_type (e/c)\n+ edges from 527682d (vertex names):\n [1] 1362279599191691264->1116612139          43447495           ->43447495           \n [3] 518488471          ->518488471           353381552          ->353381552          \n [5] 576131356          ->576131356           28305154           ->3288075858         \n [7] 1310795233651601408->3079563404          3842652433         ->3842652433         \n [9] 3219321554         ->3219321554          1327357902424666112->29387813           \n[11] 1327357902424666112->1548253015          1327357902424666112->1327357902424666112\n[13] 37891446           ->37891446            1296548400         ->1296548400         \n+ ... omitted several edges\n\nWe now use igraph to manipulate the network. The simplify(g) function removes multiple edges and loops from the network. Then, we proceed to identify a subset of Twitter users (5), based on indegree, which we will later use in our bot status analysis.\n\n\nlibrary(igraph)\n\n#remove multiple and loop edges\ng <- simplify(g)\n\nV(g)$screen_name[order(degree(g, mode=\"in\"), decreasing=TRUE)][1:5]\n\n\n\nGiven this network was created using tweets that contain the #auspol hashtag, it is not surprising that the top 5 Twitter users based on indegree are four politicians and a political commentator:\n\n[1] \"ScottMorrisonMP\" \"DanielAndrewsMP\" \"JoshFrydenberg\"  \"GladysB\"         \"bruce_haigh\"```  \n\nSince we are going to access the Botometer API via Python, first we need to print the Twitter handles we want to check (5) with Botometer to a .csv file.\n\n\nwrite.csv(data.frame(user=V(g)$screen_name[order(degree(g, mode=\"in\"), decreasing=TRUE)][1:5]), \"top-5_auspol.csv\", row.names=FALSE)\n\n\n\nFinding the bot scores using Botometer in python\nGetting started with Botometer\nWe are now going use the Botometer API to find the bot scores for the 5 Twitter accounts. We will use the python client Botometer-python provided by the Botometer team.\nTo use the Botometer API you need to be able to authenticate using Twitter developer app API keys (same keys you use for Dev Auth approach to authenticating for Twitter collection via vosonSML). You also need a free RapidAPI (previously Mashape) account with the Botometer Pro API enabled (the Basic plan is free).\nBelow is a test of the Botometer python client v4 , using code from the Indiana University Network Science Institute GitHub page.\nTo access Botometer, enter the following code in a python shell or script. The second step involves checking the bot status of a single Twitter account:\n\nimport botometer\n\nrapidapi_key = \"xx\"\ntwitter_app_auth = {\n                    'consumer_key': \"xx\",\n                    'consumer_secret': \"xx\",\n                    'access_token': \"xx\",\n                    'access_token_secret': \"xx\"\n                   }\n\nbom = botometer.Botometer(wait_on_ratelimit=True,\n                          rapidapi_key=rapidapi_key,\n                          **twitter_app_auth)\n\n# Check a single account by screen name\nresult = bom.check_account('@clayadavis')\nprint(result)\n\nThe result of our test prints as follows:\n\n{\n    \"cap\": {\n        \"english\": 0.4197222421546159,\n        \"universal\": 0.6608500314332488\n    },\n    \"display_scores\": {\n        \"english\": {\n            \"astroturf\": 0.2,\n            \"fake_follower\": 1.2,\n            \"financial\": 0.0,\n            \"other\": 0.3,\n            \"overall\": 0.4,\n            \"self_declared\": 0.2,\n            \"spammer\": 0.0\n        },\n        \"universal\": {\n            \"astroturf\": 0.2,\n            \"fake_follower\": 0.9,\n            \"financial\": 0.0,\n            \"other\": 0.3,\n            \"overall\": 0.8,\n            \"self_declared\": 0.0,\n            \"spammer\": 0.1\n        }\n    },\n    \"raw_scores\": {\n        \"english\": {\n            \"astroturf\": 0.04,\n            \"fake_follower\": 0.23,\n            \"financial\": 0.0,\n            \"other\": 0.06,\n            \"overall\": 0.08,\n            \"self_declared\": 0.05,\n            \"spammer\": 0.01\n        },\n        \"universal\": {\n            \"astroturf\": 0.04,\n            \"fake_follower\": 0.18,\n            \"financial\": 0.0,\n            \"other\": 0.06,\n            \"overall\": 0.17,\n            \"self_declared\": 0.0,\n            \"spammer\": 0.02\n        }\n    },\n    \"user\": {\n        \"majority_lang\": \"en\",\n        \"user_data\": {\n            \"id_str\": \"11330\",\n            \"screen_name\": \"test_screen_name\"\n        }\n    }\n}\n\nThe descriptions of elements in the response e.g. users, raw scores, etc., are specified in the GitHub page.\nAnalysing bot status with Botometer in phyton\nThis step involves reading in the .csv file with the 5 Twitter handles into python and run them through the botometer API:\n\nimport pandas as pd\nusers = pd.read_csv(\"top-5_auspol.csv\")\nprint(users)\n\n\ncat(\"user\\n 0  ScottMorrisonMP\\n 1  DanielAndrewsMP\\n 2   JoshFrydenberg\\n 3          GladysB\\n 4  bruce_haigh\\n\")\n\nNow, we collect the botscores and print the Complete Automation Probability (CAP) to the csv file.\n\nresults_dict = {}     #use this to save all botometer results to file\ncap = []              #use this for writing botometer CAP to csv\nfor i in users.user:\n   #print(i)\n   result = bom.check_account('@'+i)\n   #print(result)\n   cap.append([i, result['cap']['english']])\n   results_dict[i] = result\n\n#write CAP score to csv\ndf = pd.DataFrame(cap, columns=[\"user\", \"cap\"])\n#print(df)\ndf.to_csv(\"top-5_auspol_cap.csv\")\n\n#write results dictionary to file\nimport json\njson.dump(results_dict, open(\"top-5_auspol_botometer_results.txt\",'w'))\n#can be read back in with\n#d2 = json.load(open(\"top-5_auspol_botometer_results.txt\"))\n\nThe Botometer API provides scores as Complete Automation Probability (CAP), defined as the probability, according to Botometer models, that an account with a certain score or greater is a bot. More information on how to interpret the scores is available here.\nBot scores as node attributes in the graph in R\nFinally, we can read the botometer scores back into R and include them as a node attribute in the graph.\n\n\ndf2 <- read.csv(\"top-5_auspol_cap.csv\")\ndf2\n\n\n\nThe dataframe looks like this:\n\n> df2 <- read.csv(\"top-5_auspol_cap.csv\")\n> df2\n  X            user       cap\n1 0 ScottMorrisonMP 0.7384783\n2 1 DanielAndrewsMP 0.7966467\n3 2  JoshFrydenberg 0.4756770\n4 3         GladysB 0.7966369\n5 4     bruce_haigh 0.7874002\n\nTo add the bot scores as node attributes, we create a new node attribute “cap” and copy in the scores from the csv file.\n\n\nV(g)$cap <- NA\nV(g)$cap[match(df2$user,V(g)$screen_name)] <- df2$cap\nV(g)$screen_name[!is.na(V(g)$cap)]\n\n\n\nThe console output presenting nodes with the node attribute we just created is as follows:\n\n[1] \"bruce_haigh\"     \"DanielAndrewsMP\" \"JoshFrydenberg\"  \"ScottMorrisonMP\" \"GladysB\" \n\nTo inspect the values, we run the following code:\n\n\nV(g)$cap[!is.na(V(g)$cap)]\n\n\n\n\n[1] 0.7874002 0.7966467 0.4756770 0.7384783 0.7966369\n\n\n\n\nAckland, R., B. Gertzel, and F. Borquez. 2020. Introduction to vosonSML. Canberra, Australia: VOSON Lab, Australian National University. https://cran.r-project.org/web/packages/vosonSML/vignettes/Intro-to-vosonSML.html.\n\n\nBadawy, A., E. Ferrara, and K. Lerman. 2018. “Analyzing the Digital Traces of Political Manipulation: The 2016 Russian Interference Twitter Campaign.” In 2018 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM).\n\n\nCimiano, P., F. Muhle, O. Putz, B. Schiffhauer, E. Esposito, R. Ackland, U. Seelmeyer, and T. Veale. 2020. “Bots Building Bridges (3b): Theoretical, Empirical, and Technological Foundations for Systems That Monitor and Support Political Deliberation Online.” Volkswagen Foundation Grant in AI and the Society of the Future stream (2021-2025).\n\n\nRizoiu, M-A., T. Graham, R. Zhang, Y. Zhang, R. Ackland, and L. Xie. 2018. “#DebateNight: The Role and Influence of Socialbots on Twitter During the 1st U.S. Presidential Debate.” In International AAAI Conference on Web and Social Media.\n\n\nBotometer is a joint project of the Observatory on Social Media (OSoMe) and the Network Science Institute (IUNI) at Indiana University, USA↩︎\n",
    "preview": "posts/2021-10-01-testing-bot-status-of-twitter-users/bot.png",
    "last_modified": "2021-12-16T23:58:11+11:00",
    "input_file": {},
    "preview_width": 1260,
    "preview_height": 600
  },
  {
    "path": "posts/2021-08-05-exploring-issues-in-reddit-using-voson-dash/",
    "title": "Exploring issues in Reddit using VOSON Dash",
    "description": "An easy guide to explore issues in Redddit and construct networks for analysis using VOSONDash.",
    "author": [
      {
        "name": "Francisca Borquez",
        "url": {}
      }
    ],
    "date": "2021-09-23",
    "categories": [
      "rstats",
      "SNA",
      "VOSONDash",
      "networks",
      "text analysis",
      "Reddit"
    ],
    "contents": "\nThe following guide provides a practical demonstration for collecting Reddit data and constructing networks, using VOSON Lab’s interactive R/Shiny app VOSONDash. Reddit – a social news aggregation, content rating, and discussion website – provides the opportunity for researchers to access a wide range of themed-based online discussion data, and to understand the dynamics of these conversations.\nSNA approach to studying online networks\nVOSONDash (and vosonSML) method for network construction is based on Ackland and Zhu (2015) approach, whereby edges in Reddit networks represent implicitly directed ties, i.e. reflecting exchange of opinion between users rather than an explicit social relationship. Conversations threads can be analysed as networks and VOSONDash provides two approaches to constructing Reddit networks:\nActor networks – where nodes represent users who have posted original posts and comments, and edges are the comment interactions between users.\nActivity networks – where nodes are comments or initial thread posts and edges represent either replies to the original post, or replies to comments.\nMethodology\nIn this example, we will collect data from a Reddit post relating to the COVID-19 lockdown in Sydney, Australia, and proceed to use VOSONDash features to demonstrate the data outputs and a quick overview of analysis tools.\nThe post titled Sydney Lockdown extended until the end of September was created on 20 August 2021, and by the time of data collection (23 September 2021) it had attracted 557 comments.\nThe GitHub page provides instructions to install and run VOSONDash. More information on features can be accessed in the VOSONDash Userguide(Borquez et al. 2020).\nCollecting Reddit data\nReddit collection does not require API authentication. Simply go to the Reddit view, enter the URL, and click on Collect Threads. The output of the collection is presented in the right pane (Figure 1). In this example, 494 comments were collected. At this stage, the data can be saved as .rds dataframe.\nFigure 1: VOSONDash – Reddit collectionCreating Reddit Activity networks with VOSONDash\nActivity networks represent the three-like structure of conversations, with nodes being comments or the initial post, and edges being replies to comments or replies to initial post. In this example, we selected the Add text option, so the .graphml file contains text data.\nThe console displays the output of the activity network (Figure 2). The Activity network has 495 nodes (including the initial post), and 494 edges (comments). The network can be saved ad .graphml, if you prefer to use a different tool for analysis.\nCreating Reddit Actor networks with VOSONDash\nIn a similar workflow, we can use the data we just collected to create Actor networks, to observe Reddit users interactions. As mentioned earlier, in Actor networks, nodes are users who have commented, or who have created initial thread posts, and edges represent either replies to the original post, or replies to comments. Again, the Add text option was selected, for the .graphml file to contain text data.\nThe console displays the output of the activity network once the network is created. The Activity network has 302 nodes, and 495 edges.\nFigure 2: Reddit Activity and Actor networksVOSONDash provides three approaches to analyse networks: Network graph, Network metrics (SNA), and Text analysis. These tools are presented in more detail in the post Analysing networks with VOSONDash.\nWe hope this guide has been useful and easy to follow. In the next post, we will cover Twitter data collection with VOSONDash.\n\n\n\nAckland, R., and J. Zhu. 2015. “Social Network Analysis.” In Innovations in Digital Research Methods, edited by P. Halfpenny and R. Procter. London: SAGE Publications.\n\n\nBorquez, F., B. Gertzel, X. Cai, and R. Ackland. 2020. VOSON Dashboard Userguide. Canberra, Australia: VOSON Lab, Australian National University. https://vosonlab.github.io/VOSONDashDocs/.\n\n\n\n\n",
    "preview": "posts/2021-08-05-exploring-issues-in-reddit-using-voson-dash/preview.png",
    "last_modified": "2021-12-16T23:46:43+11:00",
    "input_file": {},
    "preview_width": 1266,
    "preview_height": 700
  },
  {
    "path": "posts/2021-08-06-analysing-online-networks-with-vosondash/",
    "title": "Analysing online networks with VOSONDash",
    "description": "A quick introduction to VOSONDash network and text analysis features",
    "author": [
      {
        "name": "Francisca Borquez",
        "url": {}
      }
    ],
    "date": "2021-09-09",
    "categories": [
      "rstats",
      "SNA",
      "VOSONDash",
      "networks",
      "text analysis",
      "visualisation"
    ],
    "contents": "\nThis post introduces VOSONDash network analysis tools, which include network visualisation, network metrics, and text analysis. Users can analyse different networks including those collected with VOSONDash (Twitter, YouTube and Reddit), or import graphml files collected elsewhere.\nAnalysing online networks with VOSONDash is the first of a series of posts where we will cover VOSONDash features. Data collection with VOSONDash is covered in the following posts:\nTwitter – Collecting Twitter data with VOSONDash\nReddit – Exploring issues in Reddit using VOSON Dash\nYouTube – Collecting YouTube comments with VOSONDash\nAbout VOSONDash\nVOSONDash is an output of computational social methods research, designed to be a “Swiss Army knife” for studying online networks. The R/Shiny dashboard tool enables online data collection, and network and text analysis (including visualisation) within the same environment. VOSONDash builds on a number of R packages, in particular vosonSML for data collection and network generation, and igraph for network analysis. The package provides a graphical user interface which does not require users to have R programming skills and it is available on CRAN and GitHub. Bryan Gertzel is the lead developer and maintainer of VOSONDash.\nStarting VOSONDash\nThe GitHub page provides instructions to install VOSONDash via R or Rstudio. Once the package is installed, run VOSONDash from the RStudio console entering the following code; VOSONDash will open in a web browser.\n\n\nlibrary(VOSONDash)\nrunVOSONDash()\n\n\n\nNetwork data\nTo ease replication, in this example we will use the EnviroActivistsWebsite_2006 demo dataset which is provided in the package. The dataset is a hyperlink network collected with VOSON in 2006, as part of a research piece (Ackland and O’Neil 2011). The network has 161 nodes (websites representing environmental organisations) and 1,444 edges representing hyperlinks between these organisations. In this dataset, text data is stored as node attribute and categorical values are assigned depending on type of environmental organisations (Bios, Globals, and Toxics).\nNetwork analysis using VOSONDash\nThere are three main approaches to analysing online networks with VOSONDash: Network graph, Network metrics (SNA), and Text analysis. More information on features can be accessed in the VOSONDash Userguide (Borquez et al. 2020).\nNetwork graph\nIn Network graph provides two options to explore networks: network visualisation via igraph and visNetwork; and tabulations for nodes and edges. The Network graph pane provides the following options for manipulating the network:\nLabels – to display or not labels.\nGraph Filters – to display or not multiple edges, loops, and isolates.\nLayout – to select graph layout and spread.\nNode Size – to select node size by metric e.g. indegree and define size (multiplier).\nCategorical filter – option available when data contains pre-set categorical values. New collections do not have that option.\nComponent filter – to display weak or strong components and define component range.\nNeighbourhood select – to create subnetworks. It uses ego network terminology of order, where order 1 include ties between the alters.\nFigure 1: VOSONDash network visualisationNetwork metrics\nVia the Network metrics pane, we can observe basic SNA metrics, including network level and node level metrics (e.g. centralisation). Network metrics reflect the applied filters for the visualisation; in this example we removed isolates (3 nodes), so network size is 158 and the Component distribution is 1 (one connected component). Degree distribution is only available for undirected networks; Indegree distribution and Outdegree distribution charts are available for directed networks, like this example. Accordingly, in this network, there are 15 nodes receiving one hyperlink, and three nodes receiving 35 hyperlinks. While 19 nodes link out to only one other site, there are two organisations in this network that link out to 50 sites.\nAssortativity metrics (Homogeneity and Homophily indexes, including mixing matrix and population share) are presented for networks with categorical node attributes. In this example, we have selected the categorical attribute Type. The mixing matrix table presents links across the three types of organisations Globals, Bios and Toxics. The Bios and Globals sub-movements show a strong tendency towards linking to their own type. Population shares, Homogeneity indexes and Homophily indexes are presented by type. Controlling for group size, Globals are the group more biased towards its own type, where 53% of their ties to other Global organisations can be explained by homophily.\nFigure 2: SNA and Assortativity metricsText analysis\nFor a network with text data stored as either node or edge attribute, it is possible to conduct basic text analysis with VOSONDash. Text corpus can be pre-processed using Filters to:\nremove common English words that are not relevant to the analysis, such as “and,” “the,” and “but” using Remove Standard Stopwords,\ncreate own stopword list in User-Defined Stopwords,\nreduce words to their stems with Apply word stemming,\nremove URLs, numbers, or punctuation, based on user’s specifications.\nWord lenght, if need to specify number of characters.\nAdvanced options provide HTML Decode and iconv UTF8, specially useful for social media as text often contains encoded characters.\nFor Twitter networks, two other options become available: Remove Twitter hashtags and Remove Twitter Usernames.\nThere are three methods available to visualise text:\nWord frequency bar charts, where further parameters can be applied such as to define the number of results displayed, and frequency to define Minimum frequency, for the text to appear.\nWord clouds where users can adjust Minimum frequency (how many times a word needs to have been used in order for it to feature in the visualisation); Maximum words to control for the number of words appearing in the graph; percentage of vertical words can be set for legibility; and random colours can be assigned to the visualisation. Comparison clouds are only available for datasets with categorical data, like this example where colour represents the node attribute type (Bios, Globals or Toxics).\nThe Sentiment analysis function uses the Syuzhet package and classifies words based on the NRC Emotion Lexicon, which is a list of English words and their associations with eight basic emotions (anger, fear, anticipation, trust, surprise, sadness, joy, and disgust) and two sentiments (negative and positive).\nFigure 3: Text analysisWe hope this guide is useful and easy to follow.\n\n\n\nAckland, R., and M. O’Neil. 2011. “Online Collective Identity: TheCase of the Environmental Movement.” Social Networks 33: 177–90. https://doi.org/https://doi.org/10.1016/j.socnet.2011.03.001.\n\n\nBorquez, F., B. Gertzel, X. Cai, and R. Ackland. 2020. VOSON Dashboard Userguide. Canberra, Australia: VOSON Lab, Australian National University. https://vosonlab.github.io/VOSONDashDocs/.\n\n\n\n\n",
    "preview": "posts/2021-08-06-analysing-online-networks-with-vosondash/Actor_net.png",
    "last_modified": "2022-08-04T14:14:49+10:00",
    "input_file": {},
    "preview_width": 1282,
    "preview_height": 670
  },
  {
    "path": "posts/2021-06-03-us-presidential-debates-2020-twitter-collection/",
    "title": "#DebateNight 2020: Hashtag Twitter Collection of the US Presidential Debates",
    "description": "Methodology for the bulk collection of tweets containing key hashtags for the US Presidential Debates and generation of Networks for Analysis.",
    "author": [
      {
        "name": "Bryan Gertzel",
        "url": {}
      }
    ],
    "date": "2021-06-03",
    "categories": [
      "rstats",
      "twitter",
      "hashtags",
      "networks",
      "election",
      "debate"
    ],
    "contents": "\r\n\r\nContents\r\nCollection Strategy\r\nHashtags\r\nTimezones\r\n\r\nStreaming Collection\r\nSearch Collection\r\nFirst Presidential Debate Preliminary Results\r\nData Summary\r\nData Tweet Activity\r\n\r\nNetwork Analysis\r\nMerge Collected Data\r\nCreate Networks\r\nReply-network Giant Component\r\n\r\n\r\nThe VOSON Lab undertook a number of Twitter collections on selected hashtags during the 2020 US Presidential debates and townhalls. The Twitter streaming API endpoint was used for a sampled real-time collection of tweets during the debates, and the recent search API was used post-debate to collect all tweets containing hashtags that occurred over the debate telecast periods. The two approaches differ in that the streaming API endpoint allows access to a “roughly 1% random sample of publicly available Tweets in real-time” using provided hashtags or terms as a filter to monitor events. The search API endpoint uses terms in a search query to match all tweets containing hashtags for a recent historical period in time. The streaming collections, a filtered sample, produced much smaller datasets and was useful for a timeley review of the Twitter activity during the debates as well as for identifying tweets to act as bounds for search collections. The retrospective search collections were much larger and produced more comprehensive datasets.\r\nThe R packages rtweet and vosonSML, the latter of which wraps the functionality of rtweet for its Twitter capability, were used in a semi-automated way to collect data with both streaming and search API’s. These packages use the standard Twitter API v1.1 endpoints.\r\nTwo US Presidential debates took place between Donald Trump and Joe Biden on September 29th and October 22nd, with one debate scheduled for October 15th cancelled due to COVID-19 concerns. One VP debate between Kamala Harris and Mike Pence took place on October 7th. This article will focus on the datasets collected for the first debate, widely reported as “pure chaos” and “90 minutes of insult and interjection” by commentators, to demonstrate our collection methodology and some simple network analysis.\r\nFigure 1: Next day headlines, tweet from https://twitter.com/oliverdarcy. Embedded tweet sourced from Twitter.Collection Strategy\r\nThe US Presidential debates were all scheduled to run for 1.5 hours between 9pm and 10.30pm Eastern Daylight Time (EDT). To capture Twitter discussion surrounding the debate a four hour streaming window was chosen, with the collection starting at 8.30pm and concluding at 12.30am EDT (0.5 hours before and 2 hours after the debate). Streaming collection was performed by multiple team members, each with slightly staggered start times. This is because streaming collections were divided into 15 minute segments, and offsetting allowed tweets to be collected during connections, disconnections and segment changeover intervals. The tweets could then be merged and any duplicates removed in post-processing.\r\nBecause of the very likely large volume of tweets collected using the search API endpoint the collection window was reduced to 2 hours, starting 15 minutes before and concluding 15 minutes after the debates. Twitter searches are historical and page backwards in time, results are returned from most recent tweet as the first observation to the earliest tweet matching search criteria as last observation collected. There are also limits to how many tweets can be collected as defined by the API rate-limit. Using a bearer authentication token to perform searches allows for 45,000 tweets to be retrieved every 15 minutes.\r\nAs tweet status ID’s are sequential numbers, with new tweets incrementing the identifier, they can be used to set bounds for searches. Simply, the first tweet in the collected data from a search (earliest tweet in data) can be used as a starting point for subsequent searches as we move backwards in time. This means that given two tweets, one at the beginning and the other at the end of an event - the debate, we can systematically and programatically collect all tweets in-between working backwards from the end of event tweet. To identify which tweets to use as bounds we performed timestamp search of collected streaming tweets. We identified the highest status ID tweet matching our end time for the collection window 10.45pm EDT (search start) and the lowest ID matching our debate start time 8.45pm EDT (search end).\r\n\r\nUpdate: Twitter tweet ID’s are large unique 64bit unsigned integers that are not incremented or assigned sequentially, but instead generated from a timestamp, a worker number, and a sequence number (Refer Developer Platform: Twitter ID’s). It is problematic to use and perform operations on these ID’s as integers and they should instead be managed as unique identifier strings. Tweets are retrieved from the API ordered, and it is more reliable to use the first and last observation and/or tweet timestamps to delimit searches. Unlike the Twitter API v1.1 used for this article, the newer API v2 also returns tweet timestamps that record the tweet creation time to the second rather than to the minute.\r\n\r\nIn practice, 45,000 tweets were collected with a pause for the 15 minute rate-limit to reset, then a further 45,000 were collected with a pause, and so on until the tweet that we identified as marking the beginning of the debate window was collected. Because this process could take many hours, it was important to perform the search collections within 7 days of the debates.\r\nHashtags\r\nA set of hashtags were selected in order to capture tweets related to the presidential debates. The following 12 were used for streaming and search collections for all debates, with 4 addditonal hashtags for the vice presidential debate and 3 for the townhalls.\r\nTable 1: Debate hashtags\r\n\r\nHashtags\r\nHashtags for all events\r\n#PresidentialDebate, #PresidentialDebates, #Election2020, #Debates2020, #Debates, #DebateNight, #Biden, #Biden2020, #BidenHarris2020, #Trump, #Trump2020, #TrumpPence2020\r\nAdditional vice presidential debate\r\n#VPDebate, #VPDebate2020, #VPDebates, #VPDebates2020\r\nAdditional televised townhalls\r\n#BidenTownHall, #TrumpTownHall, #TownHall\r\nTimezones\r\nThe first US Presidential debate took place in Cleveland, Ohio at 9pm Eastern Daylight Time (EDT) on the 29th September, 2020. Tweet timestamps are all in Universal Coordinated Time (UTC), meaning times in the data need to be offset by -4 to find the debate time in EDT. As collection took place in Canberra, Australia or Australian Eastern Standard Time (AEST), the local system timestamps produced by scripts for logging are in AEST. The first debate time and timezone conversions can be seen in the table below.\r\nTable 2: First debate timezone reference\r\nTimezone\r\nStart time\r\nEnd time\r\nEDT\r\n2020-09-29 21:00\r\n2020-09-29 22:30\r\nUTC\r\n2020-09-30 01:00\r\n2020-09-30 02:30\r\nAEST\r\n2020-09-30 11:00\r\n2020-09-30 12:30\r\nStreaming Collection\r\nFor the streaming collection a directory was created to for easier post-processing. Streaming data was collected and written to file in JSON format using timestamp formatted file names. The streaming collection period was set to 4 hours and divided into segments or files.\r\n\r\n\r\nShow code\r\n\r\nwd <- getwd()\r\n\r\n# check paths and create directories if they do not exist\r\ndata_path <- paste0(wd, \"/data\")\r\nif (!dir.exists(data_path)) { dir.create(data_path, showWarnings = FALSE) }\r\n\r\nstreams_path <- paste0(data_path, \"/pres-debate-streams\")\r\nif (!dir.exists(streams_path)) { dir.create(streams_path, showWarnings = FALSE) }\r\n\r\n# helper functions to write to log file and to create date time based file names\r\nlog <- function(msg, fn) { cat(msg, file = fn, append = TRUE, sep = \"\\n\") }\r\nfname <- function(path, ts) { paste0(path, \"/stream-\", gsub(\"[^[:digit:]_]\", \"\", ts)) }\r\n\r\n# set stream filter hashtags - comma seperated\r\nstream_filter <- paste0(\"#PresidentialDebate,#PresidentialDebates,#Election2020,\",\r\n                       \"#Debates2020,#Debates,#DebateNight,\",\r\n                       \"#Biden,#Biden2020,#BidenHarris2020,\",\r\n                       \"#Trump,#Trump2020,#TrumpPence2020\")\r\n\r\n# set the time period to collect tweets in seconds\r\nstream_period <- 4 * 60 * 60 # 4 hours or 14400 seconds\r\n\r\n# break up streaming collection into segments\r\nnum_segs <- 16 # each segment is 15 minutes\r\nseg_period <- ceiling(stream_period / num_segs)\r\n\r\n\r\nThe streaming collection is performed by the rtweet function stream_tweets which in our operation uses the query or filter parameter q, a timeout period which is the length of time to collect streaming tweets, and an output JSON file_name. The collection is wrapped in a loop which is for the number of 15 minute segments in the collection period. Each iteration sets up a new timestamped data file and log file.\r\n\r\n\r\nShow code\r\n\r\nlibrary(rtweet)\r\n\r\n# load rtweet auth token\r\ntoken <- readRDS(\"~/.rtweet_oauth1a\")\r\n\r\n# collect streaming tweets with a new file every 15 minutes\r\nfor (i in 1:num_segs) {\r\n\r\n  # create log file and JSON data file\r\n  timestamp <- Sys.time()\r\n  log_file <- paste0(fname(streams_path, timestamp), \".txt\")\r\n  json_file <- paste0(fname(streams_path, timestamp), \".json\")\r\n  \r\n  log(paste0(\"timestamp: \", timestamp, \"\\ntimeout: \",\r\n             seg_period, \" secs\\nfilter: \", stream_filter),\r\n    log_file)\r\n  \r\n  # collect streaming tweets and write to JSON file\r\n  tryCatch({\r\n    rtweet::stream_tweets(\r\n      token = token,\r\n      q = stream_filter,\r\n      timeout = seg_period,\r\n      file_name = json_file,\r\n      parse = FALSE\r\n    )\r\n  }, error = function(e) {\r\n    cat(paste0(e, \"\\n\"))\r\n    log(paste0(\"error: \", e), log_file)\r\n  })\r\n  \r\n  log(paste0(\"completed: \", Sys.time()), log_file)\r\n}\r\n\r\n\r\nLog entries for each 15 minute iteration confirm the collection period and each file matches a JSON data file (note timestamps are in local time which was AEST).\r\n# data/pres-debate-streams/stream-20200930102859.txt \r\n\r\ntimestamp: 2020-09-30 10:28:59\r\ntimeout: 900 secs\r\nquery: #PresidentialDebate,#PresidentialDebates,#Election2020,\r\n#Debates2020,#Debates,#DebateNight,#Biden,#Biden2020,#BidenHarris2020,\r\n#Trump,#Trump2020,#TrumpPence2020\r\ncompleted: 2020-09-30 10:43:59\r\nFor the first streaming collection iteration an 180MB JSON file was written with 111,244 lines. Each line contains the JSON for a single tweet, meaning the same number of tweets were collected.\r\n\r\n> first_json_file <- \"./data/pres-debate-streams/stream-20200930102859.json\"\r\n> file.size(first_json_file)\r\n[1] 188575977 # 180MB\r\n\r\n> length(readLines(first_json_file))\r\n[1] 111244\r\n\r\n16 JSON data files were written, corresponding to the number of 15 minute segments specified. These were then individually processed and converted to dataframes, which were then merged into a single complete streaming collection dataframe for the first debate.\r\nSearch Collection\r\nAs with the streaming collection directories were created to store collected data and search parameters set. The search query containing hashtags uses the Twitter OR search operator unlike the streaming filter which was comma seperated. A maximum number of tweets for each collection iteration as well as a maximum number of iterations are set. The number of tweets is required for the search request and is set to the maximum rate-limit value for a bearer token. A maximum number of iterations is set as a precaution to prevent infinite collection should a problem arise. The two tweets found from the streaming collection and used as search bounds are also set. The search will start at the latest id and continue until the earliest id is found, or the maximum iterations has been reached.\r\n\r\n\r\nShow code\r\n\r\nwd <- getwd()\r\n\r\n# check paths and create directories if they do not exist\r\ndata_path <- paste0(wd, \"/data\")\r\nif (!dir.exists(data_path)) { dir.create(data_path, showWarnings = FALSE) }\r\n\r\nsearches_path <- paste0(data_path, \"/pres-debate-searches\")\r\nif (!dir.exists(searches_path)) { dir.create(searches_path, showWarnings = FALSE) }\r\n\r\n# set search query hashtags - separated with OR search operator\r\nq <- paste0(\"#PresidentialDebate OR #PresidentialDebates \",\r\n            \"OR #Election2020 OR \",\r\n            \"#Debates2020 OR #Debates OR #DebateNight OR \",\r\n            \"#Biden OR #Biden2020 OR #BidenHarris2020 OR \",\r\n            \"#Trump OR #Trump2020 OR #TrumpPence2020\")\r\n\r\ntype <- \"recent\"\r\nnum_tweets <- 45000\r\nmax_iter <- 40\r\n\r\n# pres debate 1 search\r\nlatest_id <- \"1311121700394807296\"    # start tweet\r\nearliest_id <- \"1311104723978579968\"  # end tweet\r\n\r\n\r\nThe search collection is performed by the vosonSML function Collect. The process is more involved than the streaming collection in that the reset time for the rate-limit is calculated each collection iteration and the script sleeps for that period of time before continuing. Tracking of search progress is also logged to the console in this approach but was redirected to a log file.\r\n\r\n\r\nShow code\r\n\r\nlibrary(vosonSML)\r\n\r\nauth <- readRDS(\"~/.vsml_oauth2\")\r\n\r\ncat(\"large twitter search\\n\")\r\ncat(paste0(\"type: \", type, \"\\n\"))\r\ncat(paste0(\"tweets per iter: \", num_tweets, \"\\n\"))\r\ncat(paste0(\"max iter: \", max_iter, \" (\", (max_iter * num_tweets), \" tweets)\\n\\n\"))\r\n\r\ni <- 1\r\nwhile (i <= max_iter) {\r\n  cat(paste0(\"iteration \", i, \" of \", max_iter, \"\\n\"))\r\n  cat(paste0(\"time: \", Sys.time(), \"\\n\"))\r\n  cat(paste0(\"set max_id: \", latest_id, \"\\n\"))\r\n  req_time <- as.numeric(Sys.time())\r\n  reset_time <- req_time + (15 * 60) + 10 # add 10 sec buffer\r\n  \r\n  code_wd <- getwd()\r\n  setwd(searches_path)\r\n  \r\n  data <- tryCatch({\r\n    auth %>%\r\n      Collect(searchTerm = q,\r\n              searchType = type,\r\n              numTweets = num_tweets,\r\n              max_id = latest_id,\r\n              verbose = TRUE,\r\n              includeRetweets = TRUE,\r\n              retryOnRateLimit = TRUE,\r\n              writeToFile = TRUE)\r\n  }, error = function(e) {\r\n    cat(paste0(e, \"\\n\"))\r\n    NULL\r\n  })\r\n  \r\n  setwd(code_wd)\r\n  \r\n  if (!is.null(data) && nrow(data)) {\r\n    data_first_obvs_id <- data$status_id[1]\r\n    data_last_obvs_id <- data$status_id[nrow(data)]\r\n    cat(paste0(\"data nrows = \", nrow(data), \"\\n\",\r\n               \"first row status id = \", data_first_obvs_id, \"\\n\",\r\n               \"last row status id = \", data_last_obvs_id, \"\\n\"))\r\n    \r\n    # set latest id to lowest status id in data for NEXT iteration\r\n    # this is typically the last observation\r\n    latest_id <- data_last_obvs_id\r\n    \r\n    # if our target id is passed then stop\r\n    if (earliest_id >= latest_id) {\r\n      cat(\"earliest id reached\\n\")\r\n      break\r\n    }\r\n    now_time <- as.numeric(Sys.time())\r\n    if (i < max_iter) {\r\n      sleep_time <- reset_time - now_time\r\n      if (sleep_time > 0) {\r\n        cat(\"sleeping \", sleep_time, \" secs\\n\")\r\n        Sys.sleep(sleep_time)  \r\n      }      \r\n    }\r\n  } else {\r\n    cat(\"no data\\n\")\r\n    break\r\n  }\r\n  i <- i + 1\r\n}\r\n\r\ncat(paste0(\"completed: \", Sys.time(), \"\\n\"))\r\n\r\n\r\nThe first search collection iteration collected a full 45,000 tweets and wrote an R dataframe object to an RDS file. The vosonSML output also indicates the minimum and maximum tweet status ID in the data and their timestamp (UTC) to assist with tracking the collection progress, it shows that the first 45,000 tweets were all created within an approximate 2.25 min period. It took 5.5 mins for the first collection to complete, and it slept for over 9.5 mins while the rate-limit reset before iteration 2. Timestamps other than the tweet creation time are in local time AEST.\r\nlarge twitter search\r\ntype: recent\r\ntweets per iter: 45000\r\nmax iter: 40 (1800000 tweets)\r\n\r\niteration 1 of 40\r\ntime: 2020-10-03 08:50:31\r\nset max_id: 1311134926167834628\r\nCollecting tweets for search query...\r\nSearch term: #PresidentialDebate OR #PresidentialDebates OR\r\n#Election2020 OR #Debates2020 OR #Debates OR #DebateNight OR\r\n#Biden OR #Biden2020 OR #BidenHarris2020 OR #Trump OR #Trump2020\r\nOR #TrumpPence2020\r\nRequested 45000 tweets of 45000 in this search rate limit.\r\nRate limit reset: 2020-10-03 09:05:32\r\nDownloading [=========================================] 100%\r\n\r\ntweet  | status_id           | created             | screen_name   \r\n-------------------------------------------------------------------\r\nMin ID | 1311134367985565697 | 2020-09-30 02:42:47 | @pxxxxx_xx   \r\nMax ID | 1311134926167834628 | 2020-09-30 02:45:00 | @ixxxxxxxxxxx\r\nCollected 45000 tweets.\r\nRDS file written: ./data/pres-debate-searches/2020-10-03_085550-TwitterData.rds\r\nDone.\r\nElapsed time: 0 hrs 5 mins 22 secs (321.79)\r\ndata nrows = 45000\r\nfirst row status id = 1311134926167834628\r\nlast row status id = 1311134367985565697\r\nsleeping  588.2078  secs\r\n54 RDS data files containing Twitter collection dataframes were written and the search took approximately 17.5 hours. These files were then merged into a single complete search collection dataframe for the first debate.\r\nFirst Presidential Debate Preliminary Results\r\nData Summary\r\nData was collected by multiple team members, presented are the un-merged results from a single members streaming and search collections for the first presidential debate.\r\nTable 2: Collection summary\r\nTwitter API endpoint\r\nStart time (EDT)\r\nEnd time (EDT)\r\nPeriod (hours)\r\nObservations (unique tweets)\r\nStreaming\r\n2020-09-29 20:30\r\n2020-09-30 00:30\r\n4.00\r\n449,102\r\nSearch\r\n2020-09-29 20:45\r\n2020-09-29 22:45\r\n2.00\r\n2,387,587\r\nData Tweet Activity\r\nTime series plots for the streaming and search collections were created to indicate tweet activity over time. Observations are grouped by tweet type and into 5 minute bins. Perhaps unsurprisingly, retweet activity appears to became more prevalent as the first debate progressed. At around 10.20pm in the search collection just over 100,000 retweets were collected.\r\nFigure 2: Streaming collection time series plotFigure 3: Search collection time series plotNetwork Analysis\r\nUsing vosonSML the Twitter data for both streaming and search collections is able to be converted into networks in the same way. The following code will demonstrate the merging of search collection data, and creation of an activity and actor network for the first debate.\r\nMerge Collected Data\r\n\r\n\r\nlibrary(dplyr)\r\n\r\n# combine the search data files and remove duplicate tweets\r\n\r\n# get all of the files to combine\r\nfiles <- list.files(path = \"./data/pres-debate-searches/\",\r\n                    pattern = \".+TwitterData\\\\.rds$\", full.names = TRUE)\r\n\r\n# merge dataframes\r\ncomb_data <- bind_rows(lapply(files, function(x) { readRDS(x) }))\r\ndata <- comb_data %>% arrange(desc(status_id))\r\n\r\n# find and remove any duplicates\r\ndupes <- data %>% filter(status_id %in% data$status_id[duplicated(data$status_id)])\r\nif (nrow(dupes)) { data <- data %>% distinct(status_id, .keep_all = TRUE) }\r\n\r\nsaveRDS(data, \"./data/pres_debate1_search.rds\") # save combined data\r\n\r\n\r\nCreate Networks\r\nBecause the combined search data for the first debate is quite large, it can be useful to reduce it to a much smaller window of time for demonstration and network visualization purposes. The following code will extract 15 minutes of tweet data from between 21:30 - 21:45 EDT.\r\n\r\ndata <- readRDS(\"./data/pres_debate1_search.rds\") # load the previously saved data\r\n\r\n# filter out tweets with creation timestamps outside of the 15 min window\r\ndata <- data %>% filter(created_at >= as.POSIXct(\"2020-09-30 01:30:00\", tz = \"UTC\") &\r\n                        created_at <= as.POSIXct(\"2020-09-30 01:45:00\", tz = \"UTC\"))\r\n\r\n> nrow(data)\r\n[1] 349937\r\n> min(data$created_at)\r\n[1] \"2020-09-30 01:30:00 UTC\"\r\n> max(data$created_at)\r\n[1] \"2020-09-30 01:45:00 UTC\"\r\n\r\nThe data is now comprised of 349,937 unique tweets that all were created during our specified window. We can now create our networks using vosonSML.\r\n\r\nlibrary(vosonSML)\r\n\r\n# use the vosonsml create function to create networks\r\n\r\n# activity network\r\n> net_activity <- data %>% Create(\"activity2\")\r\nGenerating twitter activity network...\r\n-------------------------\r\ncollected tweets | 349937\r\ntweet            | 125626\r\nretweet          | 212842\r\nreply            | 7316\r\nquote            | 4210\r\nnodes            | 366029\r\nedges            | 349994\r\n-------------------------\r\nDone.\r\n\r\n# actor network\r\n> net_actor <- data %>% Create(\"actor2\")\r\nGenerating twitter actor network...\r\n-------------------------\r\ncollected tweets | 349937\r\ntweet mention    | 15021\r\ntweet            | 125626\r\nretweet          | 212842\r\nreply mention    | 2202 \r\nreply            | 7316 \r\nquote mention    | 734  \r\nquote            | 4210 \r\nnodes            | 202333\r\nedges            | 367951\r\n-------------------------\r\nDone.\r\n\r\nThe activity network has 366,029 nodes or unique tweets, and the actor network has 202,333 nodes or unique actors for our 15 minute window.\r\nReply-network Giant Component\r\nIf we’re interested in exploring some of the Twitter interactions taking place between users during the 21:30 - 21:45 window of the first debate, the network can be further distilled by looking at the actor network and including only reply edges. This will reveal a number of reply-conversations, but we can select for the giant component to find the largest one. The igraph library can be used to perform a number of common network operations such as removing self-loops, isolates and finding the giant component.\r\n\r\nlibrary(igraph)\r\n\r\n# convert vosonsml actor network to igraph object\r\n> g_actor <- net_actor %>% Graph()\r\nCreating igraph network graph...Done.\r\n\r\n# remove edges that are not replies\r\n# remove self-loops and isolates\r\ng_actor_reply <- g_actor %>% delete_edges(E(g_actor)[E(g_actor)$edge_type != \"reply\"])\r\ng_actor_reply <- g_actor_reply %>% simplify(remove.multiple = FALSE)\r\ng_actor_reply <- g_actor_reply %>%\r\n  delete_vertices(V(g_actor_reply)[which(degree(g_actor_reply) == 0)])\r\n\r\n# find the giant component\r\ncomps <- clusters(g_actor_reply, mode = \"weak\")\r\nlargest_cluster_id <- which.max(comps$csize)\r\nnode_ids <- V(g_actor_reply)[comps$membership == largest_cluster_id]\r\ng_actor_reply_gc <- induced_subgraph(g_actor_reply, node_ids)\r\n\r\nThe giant component in the reply-network has 1743 nodes and 1982 edges. We can use igraphs degree function to further explore who the most prominent actors are in the network by in-degree.\r\n\r\n> V(g_actor_reply_gc)$screen_name[order(degree(g_actor_reply_gc, mode = \"in\"), decreasing = TRUE)]\r\n[1] \"realDonaldTrump\"    \"JoeBiden\"           \"GOPLeader\"         \r\n[4] \"KamalaHarris\"       \"ProjectLincoln\"     \"Alyssa_Milano\"\r\n\r\nTo visualise the conversation occurring during this 15 minutes of the debate, we first removed the Twitter accounts for the two candidates (\"realDonaldTrump\", \"JoeBiden\", \"GOPLeader\"), and then constructed a new giant component from the reply network, which now has 1345 nodes and 1484 edges.\r\nThe visualization of this network (using Gephi) with node size proportional to in-degree is below.\r\nFigure 4: Reply-network giant component with candidates removed. Figure created by Robert Ackland using Gephi.This graph provides some insight into the largest reply network at our chosen point in time, revealing the Twitter actors receiving the most reply attention and their associations.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-06-03-us-presidential-debates-2020-twitter-collection/debate_preview.png",
    "last_modified": "2023-02-11T08:32:38+11:00",
    "input_file": "us-presidential-debates-2020-twitter-collection.knit.md",
    "preview_width": 1024,
    "preview_height": 1024
  },
  {
    "path": "posts/2021-03-23-twitter-conversation-networks/",
    "title": "Twitter Conversation Networks",
    "description": "Getting started with the voson.tcn package.",
    "author": [
      {
        "name": "Bryan Gertzel",
        "url": {}
      }
    ],
    "date": "2021-03-23",
    "categories": [
      "rstats",
      "twitter",
      "conversations",
      "voson.tcn",
      "networks"
    ],
    "contents": "\n\nContents\nTwitter Developer\nAccess\nInstallation\nAuthentication\nCollection\nNetwork Creation\nActivity Network\nActor Network\n\nPlot Graphs\nActivity Network\nActor Network\n\n\n The VOSON Lab has recently published to GitHub a new\nopen source R package called voson.tcn.\nThe package uses the Early-Access Twitter API v2, to collect tweets\nbelonging to specified threaded conversations and generate networks. The\nTwitter API v2 provides a new tweet identifier: the conversation\nID, that is common to all tweets that are part of a conversation,\nand can be searched for using the API search endpoints. Identifiers and\nassociated metadata for referenced tweets can also be collected in the\nsame search for conversation tweets, allowing us to construct twitter\nnetworks with tweet and user metadata whilst minimising API\nrequests.\nTwitter Developer Access\nThe voson.tcn package requires developer app\nauthentication keys or tokens to access the Twitter API v2. These can be\neither the Access token & secret of an app or its\nBearer token.\nTo obtain these credentials and use the early-access API you will\nneed to have or apply for a Twitter\nDeveloper Account, as well as have activated the new Developer\nPortal. Once approved you will then need to create a development project,\nwhich is the new management container for apps, and either create a new\napp or\nassociate one of your existing apps with it.\nThere are currently two project types available that correspond to\nTwitter’s developer product\ntracks, a standard and academic type.\nAcademic projects are only available to researchers who have\ncompleted and have had their application\nfor the academic research track approved for non-commercial research\npurposes. Standard projects are for more general use, including\nhobby and educational purposes. The project type features differ in\ntheir API access and caps; standard projects can only access\nthe 7-day recent search endpoint whereas an academic project\ncan access the full-archive\nsearch endpoint for historical tweets. There are also rate-limits\nand monthly tweet caps for API v2 search endpoints. At the time of\nwriting, the caps are 500k and 10 million tweets that can be retrieved\nper month for the standard and academic track projects\nrespectively.\nPlease note that there are also terms\nof use and restricted use cases that should be considered before\napplying for access or using the Twitter API.\nInstallation\nThe voson.tcn R package is in development and currently\nonly available on GitHub. It can be\ninstalled as follows:\n\n\n# use the remotes package to install the latest dev version of voson.tcn from github\nlibrary(remotes)\ninstall_github(\"vosonlab/voson.tcn\")\n\n# Downloading GitHub repo vosonlab/voson.tcn@HEAD\n# √  checking for file\n# -  preparing 'voson.tcn':\n# √  checking DESCRIPTION meta-information ... \n# -  checking for LF line-endings in source and make files and shell scripts\n# -  checking for empty or unneeded directories\n# -  building 'voson.tcn_0.1.6.9000.tar.gz'\n#    \n# * installing *source* package 'voson.tcn' ...\n# ...\n# * DONE (voson.tcn)\n# Making 'packages.html' ... done\n\n\n\nAuthentication\nThe voson.tcn package only supports app based\nauthentication using OAuth2.0 tokens which are also known\nas bearer\ntokens. We will likely support user based tokens in the future,\nhowever at this stage they do not offer any advantages as they have\nlower rate-limits and we are not using any private metadata of which\nthey permit access (such as user-visible only metrics).\nThe token can be created using either your apps\naccess token & secret (also known as consumer keys) or\nits bearer token. It is recommended that this token is\nsaved for future use; there is no need to perform this step more than\nonce as the token will not change unless it is invalidated or you\nregenerate keys on the developer portal.\n\n\nlibrary(voson.tcn)\n\n# retrieves a bearer token from the API using the apps consumer keys\ntoken <- tcn_token(consumer_key = \"xxxxxxxx\",\n                   consumer_secret = \"xxxxxxxx\")\n\n# alternatively if you have a bearer token already you can assign it directly\ntoken <- list(bearer = \"xxxxxxxxxxxx\")\n\n# if you save the token to file this step only needs to be done once\nsaveRDS(token, \"~/.tcn_token\")\n\n\n\nCollection\nCollecting conversation tweets requires the tweet ID or URL of a\ntweet that belongs to each threaded conversation that you are interested\nin. These are passed to the voson.tcn collect function\ntcn_threads as a vector or list. Conversation IDs will be\ntracked by this function to avoid duplication and, if tweet IDs are\nfound to belong to a conversation that has already been collected on,\nthen that conversation will be skipped.\nIn the following example, we are collecting the tweets for a threaded\nconversation belonging to a public lockdown announcement following a\nCOVID-19 outbreak in Brisbane, Queensland, Australia, that took place on\nMarch, 29, 2021. The tweet URL or ID (number following the status in the\nURL) can be passed directly to the collection function.\nFigure 1: Public announcement tweet\nregarding a COVID-19 lockdown of Brisbane, from the Queensland Premier.\nEmbedded tweet sourced from Twitter.\n\n# read token from file\ntoken <- readRDS(\"~/.tcn_token\")\n\n# collect the conversation thread tweets for supplied ids           \ntweets <- tcn_threads(\"https://twitter.com/AnnastaciaMP/status/1376311897624956929\", token)\n\n\n\nWhen completed, a list of named dataframes will be returned, with\ntweets containing all of the tweets and their metadata, and\nusers containing all of the referenced users in the tweets\nand their metadata. In our example, 286 tweets were collected with 180\nassociated users public metadata.\nNote that the collection of a threaded tweet conversation is a\nsnapshot of the state of the conversation at a point in time. Metrics\nand networks produced from our data will not completely match subsequent\ncollections of the same conversation, as it will have likely\ncumulatively expanded over time.\n\n\n# collected tweets\nprint(tweets$tweets, n = 3)\n# # A tibble: 286 x 14\n#   in_reply_to_user~ conversation_id  source  author_id  tweet_id  ref_tweet_type\n#   <chr>             <chr>            <chr>   <chr>      <chr>     <chr>         \n# 1 15999~            137631189762495~ Twitte~ 134852208~ 13763373~ replied_to    \n# 2 1142316897985163~ 137631189762495~ Twitte~ 126908387~ 13763373~ replied_to    \n# 3 25683~            137631189762495~ Twitte~ 137503906~ 13763371~ replied_to    \n# # ... with 283 more rows, and 8 more variables: ref_tweet_id <chr>, text <chr>,\n# #   created_at <chr>, includes <chr>, public_metrics.retweet_count <int>,\n# #   public_metrics.reply_count <int>, public_metrics.like_count <int>,\n# #   public_metrics.quote_count <int>\n\n# users metadata\nprint(tweets$users, n = 3)\n# # A tibble: 180 x 12\n#   profile.username profile.created_~ profile.profile_~ user_id profile.descript~\n#   <chr>            <chr>             <chr>             <chr>   <chr>            \n# 1 MSMW~            2013-03-30T06:48~ https://pbs.twim~ 131592~ \"Fact checking i~\n# 2 bpro~            2012-12-04T02:07~ https://pbs.twim~ 987844~ \"Only way to get~\n# 3 scre~            2009-10-22T22:56~ https://pbs.twim~ 844463~ \"I'm a  creative~\n# # ... with 177 more rows, and 7 more variables: profile.name <chr>,\n# #   profile.verified <lgl>, profile.location <chr>,\n# #   profile.public_metrics.followers_count <int>,\n# #   profile.public_metrics.following_count <int>,\n# #   profile.public_metrics.tweet_count <int>,\n# #   profile.public_metrics.listed_count <int>\n\n\n\nIf interested in text analysis, the tweet text can be found in the\ntext column of the tweets dataframe and user\nprofile descriptions in profile.description of the\nusers dataframe.\nPublic metrics for tweets and users are\nfound in dataframe columns prefixed, with public_metrics\nand profile.public_metrics respectively.\n\n\nlibrary(dplyr)\n\nnames(select(tweets$tweets, starts_with(\"public_metrics\")))\n# [1] \"public_metrics.retweet_count\" \"public_metrics.reply_count\"\n# [3] \"public_metrics.like_count\" \"public_metrics.quote_count\"\n\nnames(select(tweets$users, starts_with(\"profile.public_metrics\")))\n# [1] \"profile.public_metrics.followers_count\"\n# [2] \"profile.public_metrics.following_count\"\n# [3] \"profile.public_metrics.tweet_count\"\n# [4] \"profile.public_metrics.listed_count\"\n\n\n\nNetwork Creation\nThere are two types of networks that can be generated using\nvoson.tcn: activity and actor\nnetwork. These differ by the type of node and resulting structure of the\nnetworks.\nActivity Network\nAn activity network is a representation of the\nconversation as seen on Twitter: nodes are tweets and the edges are how\nthey are related. Tweets (or nodes) are identified by their unique\nidentifier Tweet ID (formerly Status ID). In a\nTwitter threaded conversation, there are only two types of connections\nor edges between tweets and these are replied_to and\nquoted.\nReplies are made when a user chooses the reply option and\npublishes a tweet response to the tweet they are replying to. Quotes are\na little different in that the user has included a link to or\nquoted another tweet in the body of their tweet. In Twitter\nconversation networks, it is common to quote a tweet as part of\na reply tweet, generating in the activity network a\nreplied_to and quoted edge from the same\nnode.\n\n\n# generate an activity network\nactivity_net <- tcn_network(tweets, \"activity\")\n\n# number of nodes\nnrow(activity_net$nodes)\n# [1] 279\n\n# number of edges\nprint(activity_net$edges, n = 3)\n# # A tibble: 281 x 3\n#   from                to                  type      \n#   <chr>               <chr>               <chr>     \n# 1 1376337359126495232 1376328523518898176 replied_to\n# 2 1376337350163267584 1376325216658317315 replied_to\n# 3 1376337128016113665 1376311897624956929 replied_to\n# # ... with 278 more rows\n\nunique(activity_net$edges$type)\n# [1] \"replied_to\" \"quoted\"\n\n\n\nActor Network\nAn actor network represents the interactions between\nTwitter users in the conversation: nodes are the users and edges are\ntheir connections. As in the activity network, edges are\neither a reply or a quote but edges represent\nthe classification of a tweet connecting users rather than the activity.\nUsers (or nodes) are identified by their unique Twitter\nUser ID. In the actor network, interactions\nbetween users are more apparent and can be measured by the frequency\n(and direction) of edges between them.\n\n\n# generate an actor network\nactor_net <- tcn_network(tweets, \"actor\")\n\n# number of nodes or actors\nnrow(actor_net$nodes)\n# [1] 180\n\nprint(actor_net$edges, n = 3)\n# # A tibble: 286 x 6\n#   from      to        type  tweet_id     created_at    text                     \n#   <chr>     <chr>     <chr> <chr>        <chr>         <chr>                    \n# 1 13485220~ 15999128~ reply 13763373591~ 2021-03-29T0~ \"@Ther~ @Scott~\n# 2 12690838~ 11423168~ reply 13763373501~ 2021-03-29T0~ \"@Luke~ @Annas~\n# 3 13750390~ 25683344~ reply 13763371280~ 2021-03-29T0~ \"@AnnastaciaMP You do un~\n# # ... with 283 more rows\n\nunique(actor_net$edges$type)\n# [1] \"reply\" \"quote\" \"tweet\"\n\n\n\nNote that in the actor network there is an additional\nedge type: tweet, which is assigned to a self-loop edge\ncreated for the thread’s initial tweet. This is a technique used to\nretain the initial tweet’s metadata as edge attributes comparable to\nother edges in the network.\nThe initial conversation tweet would not usually be included in the\nedge list, as the initial conversation tweet is not directed at another\nuser, and hence no edge to attach metadata.For example, this allows the\ntext of the initial tweet to be included in any actor network tweet text\nanalysis. It would not usually be included in the edge list as the\ninitial conversation tweet is not directed at another user, and hence no\nedge to attach metadata is naturally found in this type of network.\nPlot Graphs\nActivity Network\nVisualisation of the activity network produced with\nigraph.\n\n\nlibrary(igraph)\nlibrary(RColorBrewer)\n\ng <- graph_from_data_frame(activity_net$edges, vertices = activity_net$nodes)\n\n\n\n\n\nShow code\n\n# change likes to log scale\nlike_count <- V(g)$public_metrics.like_count\nlike_count[is.na(like_count)] <- 0\nln_like_count <- log(like_count)\nln_like_count[!is.finite(ln_like_count)] <- 0\n\n# set node size based on likes, min size 4\nsize <- ln_like_count * 2\nV(g)$size <- ifelse(size > 0, size + 8, 4)\n\n# set node label if number of likes >= 2\nV(g)$label <- ifelse(like_count >= 2, like_count, NA)\nV(g)$label.color <- \"black\"\n\n# set node colors based on number of retweets low to high is yellow to green\n# set tweets with no retweets to grey\nrt_count <- V(g)$public_metrics.retweet_count\nrt_count[is.na(rt_count)] <- 0\ncols <- colorRampPalette(c(\"yellow1\", \"green3\"))\ncols <- cols(max(rt_count) + 1)\nV(g)$color <- cols[rt_count + 1]\nV(g)$color[which(rt_count < 1)] <- \"lightgrey\"\n\n# set edge color to orange if tweet quoted another tweet\nE(g)$color <- ifelse(E(g)$type == \"quoted\", \"orange\", \"grey\")\n\n\n\n\n\n# plot the graph using fruchterman reingold layout\nset.seed(200)\ntkplot(g,\n       canvas.width = 1024, canvas.height = 1024,\n       layout = layout_with_fr(g),\n       edge.arrow.size = 0.5,\n       edge.width = 2)\n\n\n\nFigure 2: Conversation activity network -\nNode size and label represent number of tweet likes, color scale is\nindicating low to high number of retweets (yellow to green). Orange\ncoloured edges are quoting linked tweet.voson.tcn collects tweets that are all linked to each\nother via the conversation ID. This means that in a network\ngenerated from this data, such as the activity network, all of the nodes\n(tweets) should be connected in a single component per\nconversation ID. If multiple conversation IDs\nwere collected on then, it is also possible to have one component\nbecause of quote edges. These edges joining conversations occur\nwhen a tweet in one conversation has quoted a tweet in another that you\nhave collected.\nIn the example activity network above, there are two components even\nthough only one conversation ID was collected on. Multiple\ncomponents are usually due to a missing conversation tweet not able to\nbe retrieved from the API and producing a broken reply chain. This can\noften occur if, for example, a tweet has been deleted, or the tweet or\nuser flagged or suspended in some way restricting public\navailability.\nActor Network\nVisualisation of the actor network produced with\nigraph.\n\n\nlibrary(dplyr)\nlibrary(magrittr)\nlibrary(stringr)\n\nregex_ic <- function(x) regex(x, ignore_case = TRUE)\n\n# best effort set the node colour attribute based on presence of city, state,\n# or country in the actors profile location field\n# value assigned from first match\nnodes <- actor_net$nodes %>%\n  mutate(color = case_when(\n    str_detect(profile.location, regex_ic(\"brisbane|bris\")) ~ \"orange\",\n    str_detect(profile.location, regex_ic(\"queensland|qld\")) ~ \"gold\",\n    str_detect(profile.location, regex_ic(\"australia|oz\")) ~ \"yellow\",\n    TRUE ~ \"lightgrey\"))\n\ng2 <- graph_from_data_frame(actor_net$edges, vertices = nodes)\n\n\n\n\n\nShow code\n\n# the following code de-clutters the actor network by removing some nodes\n# that are not part of conversation chains and are stand-alone replies to\n# the initial thread tweet\n\n# get the author of the initial thread tweet using the conversation id\nconversation_ids <- c(\"1376311897624956929\")\nthread_authors <- activity_net$nodes %>%\n  filter(tweet_id %in% conversation_ids) %>% select(user_id)\n\n# remove actors replying to the initial tweet that have a degree of 1\nthread_spokes <- unlist(\n  incident_edges(g2, V(g2)[which(V(g2)$name %in% thread_authors$user_id)],\n                 \"in\"))\nspokes_tail_nodes <- V(g2)[tail_of(g2, thread_spokes)]$name\ng2 <- delete_vertices(g2, degree(g2) == 1 & V(g2)$name %in% spokes_tail_nodes)\n\n# convert the graph to undirected\n# simplify the graph and collapse edges into an edge weight value\nE(g2)$weight <- 1\ng2 <- as.undirected(simplify(g2, edge.attr.comb = list(weight = \"sum\")))\ng2 <- delete_vertices(g2, degree(g2) == 0)\n\n# use edge weight for graph edge width\nE(g2)$width <- ifelse(E(g2)$weight > 1, E(g2)$weight + 1, 1)\n\n# use the actors followers count for node size \nfollowers_count <- log(V(g2)$profile.public_metrics.followers_count)\nfollowers_count[!is.finite(followers_count)] <- 0\nsize <- followers_count * 3\nV(g2)$size <- ifelse(size < 6, 6, size)\nV(g2)$label <- ifelse(followers_count > 0,\n                      V(g2)$profile.public_metrics.followers_count, NA)\n\n\n\n\n\n# plot the graph using automatically chosen layout\nset.seed(201)\ntkplot(g2,\n       canvas.width = 1024, canvas.height = 1024,\n       layout = layout_nicely(g2),\n       vertex.label.cex = 0.8,\n       vertex.label.color = \"black\")\n\n\n\nFigure 3: Conversation actor network -\nNode size and label represent users follower counts. Node color\nindicates user self-reported location. Edge width represents number of\ncollapsed edges.\n\n\n",
    "preview": "posts/2021-03-23-twitter-conversation-networks/activity_network.png",
    "last_modified": "2022-08-04T14:14:49+10:00",
    "input_file": {},
    "preview_width": 1024,
    "preview_height": 1025
  },
  {
    "path": "posts/2021-03-15-hyperlink-networks-with-vosonsml/",
    "title": "Hyperlink Networks with vosonSML",
    "description": "An introduction to creating hyperlink networks with vosonSML.",
    "author": [
      {
        "name": "Bryan Gertzel",
        "url": {}
      }
    ],
    "date": "2021-03-15",
    "categories": [
      "rstats",
      "hyperlinks",
      "vosonSML",
      "networks"
    ],
    "contents": "\n\nContents\nIntroduction\nInstallation\n\nHyperlink Collection\nSetting Up\nPerforming the\nCollection\n\nNetwork Creation\nNetworks\nPlot a Graph\n\n\nThe VOSON software for\nhyperlink collection and analysis was an early research output of the\nVOSON Lab (Ackland 2010).\nIt addressed a need for tools that could help study online social\nnetworks, even before the rise of social media, and assisted researchers\ngain insights into important phenomena such as networks around issue\nspheres and online social movements [see (Ackland and O’Neil 2011) and (Ackland 2013)]. After many years and many\niterations since its inception in 2004, the VOSON Lab is happy to\nreintroduce the canonical VOSON hyperlink collection software as part of\nour R open-source toolkit for social media collection:\nvosonSML.1\nThis simple guide will demonstrate how to use the new features of the\nvosonSML package to perform a hyperlink collection and\ngenerate networks for analysis.\nIntroduction\nThe vosonSML hyperlink collection and network creation\nworks similarly to the 3-step process we use with other social media\nsources: the Authenticate, Collect and\nCreate verb functions. The Authenticate\nfunction is first called with the parameter “web” to identify and set up\nthe context for subsequent operations, but it does not require any\nfurther credentials in this implementation. vosonSML uses\nstandard web crawling and text-based page scraping techniques to\ndiscover hyperlinks and, as such, there is no need to access any\nrestricted data API’s as we commonly do with social media.\nInstallation\nThe new hyperlink collection and network features are currently\navailable in the development version of vosonSML on GitHub, and\nare to soon be released on CRAN. The development version can be\ninstalled as follows:\n\n\n# use the remotes package to install the latest dev version of vosonSML from github\nlibrary(remotes)\ninstall_github(\"vosonlab/vosonsml\")\n\n# Downloading GitHub repo vosonlab/vosonsml@HEAD\n# √  checking for file\n# -  preparing 'vosonSML':\n# √  checking DESCRIPTION meta-information ... \n# -  checking for LF line-endings in source and make files and shell scripts\n# -  checking for empty or unneeded directories\n# -  building 'vosonSML_0.30.00.9000.tar.gz'\n#    \n# * installing *source* package 'vosonSML' ...\n# ...\n# * DONE (vosonSML)\n# Making 'packages.html' ... done\n\n\n\nHyperlink Collection\nSetting Up\nThe web sites or pages to collect hyperlinks from are specified and\ninput to the Collect function in a dataframe. As there are\npage specific options that can be used, this format helps us to organise\nand set the request parameters. The URL’s set in the dataframe for the\npage column are called ‘seed pages’ and are the starting\npoints for web crawling. Although not explicitly indicated in the URL’s,\nthe seed pages are actually the landing pages or “index” pages of the\nweb sites and a page name can be specified if known or desired.\n\n\n# set sites as seed pages and set each for external crawl with a max depth\npages <- data.frame(page = c(\"http://vosonlab.net\",\n                             \"https://www.oii.ox.ac.uk\",\n                             \"https://sonic.northwestern.edu\"),\n                    type = c(\"ext\", \"ext\", \"ext\"),\n                    max_depth = c(2, 2, 2))\n\n\n\nThe example above shows seed pages with some additional per-seed\nparameters that are used to control the web crawling. The\ntype parameter can be set to a value of either\nint, ext or all, which correspond\nto following only internal, external or following all hyperlinks found\non a seeded web page and subsequent pages discovered from that\nparticular seed. How a hyperlink is classified is determined by the seed\ndomain name, for example, if the seed page is\nhttps://vosonlab.net a type of ext will follow\nhyperlinks from that page that do not have a domain name of\n“vosonlab.net”. A type of int will follow only hyperlinks\nthat match a domain of “vosonlab.net”, and a type of all\nwill follow all hyperlinks found irrespective of their domain. The final\nparameter max_depth refers to how many levels of pages to\nfollow from the seed page. In the diagram below, the green dots are\npages scraped by the web crawler and the blue dots links are the\nhyperlinks collected from them for a max depth of 1,2 and 3.\nFigure 1: Scope of hyperlinks collected\nusing the max_depth parameterAs can be seen, a max depth of 1 directs the crawler to scrape and\ncollect hyperlinks from only seed pages, a max depth of 2 to follow\nhyperlinks found on the seed pages and collect hyperlinks from those\npages as well, and so on radiating outwards. The number of pages and\nhyperlinks can rise very rapidly so it is best to keep this number as\nlow as possible. If a greater reach in collection sites is desired, this\ncould perhaps more efficiently be achieved by revising and adding more\nseed pages in the first instance. In the example code the\ntype has been set to “ext” (external) for all three seed\nsites, so as to limit “mapping” of the internal seed web sites and focus\non their outward facing connections. Depth of crawl was set to 2.\nIt should be noted that all hyperlinks found are collected from\nscraped pages and used to generate networks. The type and\nmax_depth parameters only apply to the web crawling and\nscraping activity.\nPerforming the Collection\nThe hyperlink data can now be collected using the\nCollect function with the pages parameter.\nThis produces a dataframe that contains the hyperlink URL’s found, pages\nthey were found on and other metadata that can be used to help construct\nnetworks.\n\n\nlibrary(magrittr)\nlibrary(dplyr)\nlibrary(vosonSML)\n\n# set up as a web collection and collect the hyperlink data using the\n# previously defined seed pages\nhyperlinks <- Authenticate(\"web\") %>% Collect(pages)\n\n# Collecting web page hyperlinks...\n# *** initial call to get urls - http://vosonlab.net\n# * new domain: http://vosonlab.net \n# + http://vosonlab.net (10 secs)\n# *** end initial call\n# *** set depth: 2\n# *** loop call to get urls - nrow: 6 depth: 2 max_depth: 2\n# * new domain: http://rsss.anu.edu.au \n# + http://rsss.anu.edu.au (0.96 secs)\n# ...\n\n# dataframe structure\nglimpse(hyperlinks)\n# Rows: 1,163\n# Columns: 9\n# $ url       <chr> \"http://rsss.anu.edu.au\", \"http://rsss.cass.anu.edu.au\", \"ht~\n# $ n         <int> 1, 1, 4, 1, 1, 2, 2, 2, 2, 2, 2, 1, 1, 1, 4, 1, 1, 1, 1, 1, ~\n# $ page_err  <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, ~\n# $ page      <chr> \"http://vosonlab.net\", \"http://vosonlab.net\", \"http://vosonl~\n# $ depth     <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ~\n# $ max_depth <dbl> 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ~\n# $ parse     <df[,6]> <data.frame[26 x 6]>\n# $ seed      <chr> \"http://vosonlab.net\", \"http://vosonlab.net\", \"http://vos~\n# $ type      <chr> \"ext\", \"ext\", \"ext\", \"ext\", \"ext\", \"ext\", \"ext\", \"ext\", \"ext~\n\n# number of pages scraped for hyperlinks\nnrow(hyperlinks %>% distinct(page))\n# [1] 38\n\n# number of hyperlinks collected\nnrow(hyperlinks)\n# [1] 1163\n\n\n\nA total of 1,163 hyperlinks were collected from 38 pages followed\nfrom our 3 seed pages. Using this data, it is now possible to generate\nhyperlink networks.\nNetwork Creation\nNetworks\nAs with other vosonSML social media, there are two\nstandard types of networks we can create. An activity\nnetwork that produces a more structural representation of the network\nwhere nodes are web pages and edges are the hyperlink references between\nthem, and an actor network that instead groups pages into\nentities based on their domain names.\n\n\n# generate a hyperlink activity network\nactivity_net <- Create(hyperlinks, \"activity\")\n\n# generate a hyperlink actor network\nactor_net <- Create(hyperlinks, \"actor\")\n# Generating web actor network...\n# Done.\n\n\n\nThe output of the network creation is a named list of two dataframes,\none for the nodes and the other for the edges\nor edge list data. The example below shows the actor_net.\nNote that the edges of the actor network are also\naggregated into a weight value and that actors can link to themselves\nforming self-loops.\n\n\nprint(as_tibble(actor_net$nodes))\n# # A tibble: 185 x 2\n#   id                              link_id\n#   <chr>                             <int>\n# 1 accounts.google.com                   1\n# 2 alumni.kellogg.northwestern.edu       2\n# 3 anu.edu.au                            3\n# # ... with 182 more rows\n\nprint(as_tibble(actor_net$edges))\n# # A tibble: 226 x 3\n#   from            to              weight\n#   <chr>           <chr>            <int>\n# 1 rsss.anu.edu.au anu.edu.au           2\n# 2 rsss.anu.edu.au rsss.anu.edu.au     36\n# 3 rsss.anu.edu.au soundcloud.com       1\n# # ... with 223 more rows\n\n\n\nPlot a Graph\nNow that the network has been generated, we can create a graph and\nplot it. The Graph function creates an igraph\nformat object that can be directly plotted or adjusted for presentation\nusing igraph plotting parameters.\n\n\nlibrary(igraph)\nlibrary(stringr)\n\nactor_net <- Create(hyperlinks, \"actor\")\n\n# identify the seed pages and set a node attribute\nseed_pages <- pages %>%\n  mutate(page = str_remove(page, \"^http[s]?://\"), seed = TRUE)\nactor_net$nodes <- actor_net$nodes %>%\n  left_join(seed_pages, by = c(\"id\" = \"page\"))\n\n# create an igraph from the network\ng <- actor_net %>% Graph()\n\n# set node colours\nV(g)$color <- ifelse(degree(g, mode = \"in\") > 1, \"yellow\", \"grey\")\nV(g)$color[which(V(g)$seed == TRUE)] <- \"dodgerblue3\"\n\n# set label colours\nV(g)$label.color <- \"black\"\nV(g)$label.color[which(V(g)$seed == TRUE)] <- \"dodgerblue4\"\n\n# set labels for seed sites and nodes with an in-degree > 1\nV(g)$label <- ifelse((degree(g, mode = \"in\") > 1 | V(g)$seed), V(g)$name, NA)\n\n# simplify and plot the graph\nset.seed(200)\ntkplot(simplify(g),\n       canvas.width = 1024, canvas.height = 1024,\n       layout = layout_with_dh(g),\n       vertex.size = 3 + (degree(g, mode = \"in\")*2),\n       vertex.label.cex = 1 + log(degree(g, mode = \"in\")),\n       edge.arrow.size = 0.4,\n       edge.width = 1 + log(E(g)$weight))\n\n\n\nFigure 2: Hyperlink network of\nactorsWe now have a simple graph of the actor hyperlink network. Our seed\nactors are indicated by blue nodes and sites with an in-degree greater\nthan one indicated in yellow. Node size and label size reflect most\nlinked to nodes or highest in-degree. Perhaps unsurprisingly, social\nmedia sites and the institutions at which the seed pages are located\nfeature most prominently in the network, and the graph plot provides us\na view of the actors online presence and connections.\nThere is much more network visualisation and analysis that could be\nperformed on the vosonSML hyperlink networks and we will be\nworking to add more features such as text analysis and network\nrefinements in our near future releases. In the meantime, we hope you\nhave found this practical introduction to our new tool useful and look\nforward to your feedback!\n\n\n\nAckland, R. 2010. “WWW Hyperlink Networks.” Edited by D. L.\nHansen and B. Shneiderman and M. A. Smith. Morgan-Kaufmann.\n\n\n———. 2013. Web Social Science: Concepts, Data and Tools for Social\nScientists in the Digital Age. SAGE Publications.\n\n\nAckland, R., and M. O’Neil. 2011. “Online Collective Identity: The\nCase of the Environmental Movement.” Social Networks 33\n(3): 177–90. https://doi.org/10.1016/j.socnet.2011.03.001.\n\n\nReferences compiled by Francisca\nBorquez↩︎\n",
    "preview": "posts/2021-03-15-hyperlink-networks-with-vosonsml/hyperlink_network.png",
    "last_modified": "2022-08-04T14:14:49+10:00",
    "input_file": {},
    "preview_width": 1024,
    "preview_height": 1025
  },
  {
    "path": "posts/2021-02-11-twitter-vosonsml-from-rtweet/",
    "title": "Creating Twitter Networks with vosonSML using rtweet Data",
    "description": "Simple guide to collecting data with rtweet and generating networks with vosonSML.",
    "author": [
      {
        "name": "Bryan Gertzel",
        "url": {}
      }
    ],
    "date": "2021-02-11",
    "categories": [
      "rstats",
      "twitter",
      "vosonSML",
      "rtweet",
      "networks"
    ],
    "contents": "\n\nContents\nIntroduction\nAPI Authentication\n\nTwitter Data\nCollection with rtweet\nSearch Collection\nSave the Data\n\nCreating Networks with\nvosonSML\nRead the Data\nPrepare the Data\nCreate the Network\n\n\nIntroduction\nSocial media platforms are a rich resource for Social Network data.\nTwitter is a highly popular public platform for social commentary that,\nlike most social media supporting third-party applications, allow\nsoftware to access and retrieve it’s data via Application Programming\nInterfaces or API’s.\nBecause of its popularity with individuals and communities around the\nworld, the ready availability of its data, and low barrier for entry,\nTwitter has become of great interest as a data source for online\nempirical research.\nThere have been many pieces of software developed across\nprogramming languages and environments to access the Twitter\nAPI. Within the R ecosystem the most comprehensive and well\nsupported of Twitter packages is rtweet\ndeveloped by Michael Kearney and part of the rOpenSci initiative. The\nrtweet package provides R functions to both authenticate\nand collect timelines, tweets and other metadata using Twitter’s v1.1\nstandard and premium API’s.\nThe VOSON Lab\ndevelops and maintains the open source R packages vosonSML\nand VOSONDash. These were created to integrate online data\ncollection, network generation and analysis into a consistent and easy\nto use work flow across many popular web and social media platforms. For\nTwitter, the vosonSML package provides an interface to\nrtweet’s collection features through which tweets can be searched for\nand retrieved, and then uses this data to produce networks. There may be\ncases however, such as in the collection of streaming data or analysis\nof previously collected twitter data where you haven’t used vosonSML’s\ncollection function but instead simply wish to produce\nvosonSML generated networks from your rtweet\ndata. Because vosonSML uses rtweet this is\neasily achievable and with minimal R coding.\nAPI Authentication\nAccessing the Twitter API to collect tweets requires authentication\nvia a Twitter app. There are generally two ways this can be achieved,\nyou can apply for a Twitter Developer account and create your own app\n(and access keys) or you can authorize another persons app to access the\nAPI on your behalf (using their keys). The latter still requires your\nown Twitter user account but you do not need to go through the Developer\napplication or app creation process. The vosonSML package\nrequires users to create their own app and use their own keys but the\nrtweet package supports both methods, and you can collect\ntweets after a simple one-time web authorization step of their embedded\nrstats2twitter app.\nTwitter Data Collection\nwith rtweet\nThe following simple example will demonstrate how to use the\nrtweet package to collect some tweet data using built-in\nauthentication via the rtweet app.\nSearch Collection\nA fairly standard tweet collection usually involves using the Twitter\nSearch API endpoint to search for past tweets that meet a\ncertain criteria. This can be done with rtweet and the\nsearch_tweets function with the criteria set by passing\nadditional parameters. In our example we will direct the API to search\nand return 100 tweets (n = 100) containing the hashtag\n#auspol and excluding any retweets\n(include_rts = FALSE). By default only the most recent\ntweets within the last 7 days will be returned by the API.\n\n\nlibrary(rtweet)\n\n# recent tweet search collection\nauspol_tweets <- search_tweets(\"#auspol\", n = 100, include_rts = FALSE)\n\n#> Requesting token on behalf of user...\n#> Waiting for authentication in browser...\n#> Press Esc/Ctrl + C to abort\n#> Authentication complete.\n\n\n\nThe first time rtweet collection functions are run they\nwill open a Twitter web page on your default web browser asking\npermission to authorize rstats2twitter.\n\n\n\nFigure 1: rstats2twitter app authorization\n\n\n\nIf API authentication and search succeeds then the\nsearch_tweets function will return a data frame of tweet\ndata. The data frame will have up to 100 rows, one for each tweet\ncollected and 90 columns for associated tweet metadata:\n\n\nlibrary(tibble)\n\n# print the first 2 rows\nprint(auspol_tweets, n = 2)\n# # A tibble: 100 x 90\n#   user_id  status_id  created_at          screen_name text      source\n#   <chr>    <chr>      <dttm>              <chr>       <chr>     <chr> \n# 1 27007685 136400068~ 2021-02-22 23:54:39 ronth~      \"@janeen~ Twitt~\n# 2 1359301~ 136400067~ 2021-02-22 23:54:37 Injur~      \"When th~ Twitt~\n\n\n\n\n\nShow additional columns\n\n# # ... with 98 more rows, and 84 more variables:\n# #   display_text_width <dbl>, reply_to_status_id <chr>,\n# #   reply_to_user_id <chr>, reply_to_screen_name <chr>,\n# #   is_quote <lgl>, is_retweet <lgl>, favorite_count <int>,\n# #   retweet_count <int>, quote_count <int>, reply_count <int>,\n# #   hashtags <list>, symbols <list>, urls_url <list>,\n# #   urls_t.co <list>, urls_expanded_url <list>, media_url <list>,\n# #   media_t.co <list>, media_expanded_url <list>, media_type <list>,\n# #   ext_media_url <list>, ext_media_t.co <list>,\n# #   ext_media_expanded_url <list>, ext_media_type <chr>,\n# #   mentions_user_id <list>, mentions_screen_name <list>, lang <chr>,\n# #   quoted_status_id <chr>, quoted_text <chr>,\n# #   quoted_created_at <dttm>, quoted_source <chr>,\n# #   quoted_favorite_count <int>, quoted_retweet_count <int>,\n# #   quoted_user_id <chr>, quoted_screen_name <chr>,\n# #   quoted_name <chr>, quoted_followers_count <int>,\n# #   quoted_friends_count <int>, quoted_statuses_count <int>,\n# #   quoted_location <chr>, quoted_description <chr>,\n# #   quoted_verified <lgl>, retweet_status_id <chr>,\n# #   retweet_text <chr>, retweet_created_at <dttm>,\n# #   retweet_source <chr>, retweet_favorite_count <int>,\n# #   retweet_retweet_count <int>, retweet_user_id <chr>,\n# #   retweet_screen_name <chr>, retweet_name <chr>,\n# #   retweet_followers_count <int>, retweet_friends_count <int>,\n# #   retweet_statuses_count <int>, retweet_location <chr>,\n# #   retweet_description <chr>, retweet_verified <lgl>,\n# #   place_url <chr>, place_name <chr>, place_full_name <chr>,\n# #   place_type <chr>, country <chr>, country_code <chr>,\n# #   geo_coords <list>, coords_coords <list>, bbox_coords <list>,\n# #   status_url <chr>, name <chr>, location <chr>, description <chr>,\n# #   url <chr>, protected <lgl>, followers_count <int>,\n# #   friends_count <int>, listed_count <int>, statuses_count <int>,\n# #   favourites_count <int>, account_created_at <dttm>,\n# #   verified <lgl>, profile_url <chr>, profile_expanded_url <chr>,\n# #   account_lang <lgl>, profile_banner_url <chr>,\n# #   profile_background_url <chr>, profile_image_url <chr>\n\n\n\nThis contains all of the data necessary for vosonSML to\nconstruct Twitter networks.\nSave the Data\nThere are a few methods of saving data depending on where and how it\nwill be used. Two common methods are to use a text-based file format\nsuch as a CSV, or\nalternatively if the data will be used within R we can save\nthe dataframe object to a binary compressed\nRDS (R data object) file using saveRDS\ninstead. Conveniently, the rtweet package has a method to\nsave Twitter data to file in CSV format with the write_as_csv\nfunction that takes care of Twitter nested data and conversion issues,\nand saving an RDS file is also very easy as follows.\n\n\n# save data using rtweet write csv\nwrite_as_csv(auspol_tweets, \"auspol_tweets.csv\")\n\n# save data to file as an R data object\nsaveRDS(auspol_tweets, \"auspol_tweets.rds\")\n\n\n\nCreating Networks with\nvosonSML\nRead the Data\nIf the data was saved to file with the rtweet function\nwrite_as_csv it can be read again using\nread_twitter_csv or readRDS if from an\nRDS file.\n\n\nauspol_tweets <- read_twitter_csv(\"auspol_tweets.csv\")\n\nauspol_tweets <- readRDS(\"auspol_tweets.rds\")\n\n\n\nPrepare the Data\nFor vosonSML to recognize the previously collected data\nas a Twitter data source and be able to internally route it to the\nappropriate network functions a minor change needs to be made to the\ndata frame first. This involves adding two attributes\ndatasource and twitter to the class list of\nthe auspol_tweets data frame object as follows:\n\n\n# original class list\nclass(auspol_tweets)\n\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n# add to the class list\nclass(auspol_tweets) <- append(c(\"datasource\", \"twitter\"), class(auspol_tweets))\n\n# modified class list\nclass(auspol_tweets)\n\n\n[1] \"datasource\" \"twitter\"    \"tbl_df\"     \"tbl\"        \"data.frame\"\n\nThe order of classes is important and for the data frame to be\ncompatible with dplyr - a\nvery common data manipulation package in R, and subsequently usable in\nthe tidyverse and\nvosonSML, then the new attributes need to be added to the\nbeginning of the list.\nFor versions of vosonSML more recent than\n0.29.13 this can now all be managed by using the\nImportData function. This method is preferable as it is\neasier, works for both files and data frames, and will support any\nfuture updates to vosonSML without breaking your code.\n\n\nlibrary(vosonSML)\n\n# use the import data function\nauspol_tweets <- ImportData(auspol_tweets, \"twitter\")\n\n\n\nPlease note that modifying data frame attributes or importing data is\nonly required for rtweet data and not a necessary step for\nTwitter data collected using the vosonSML Twitter\nCollect function.\nObject classes in R are a more advanced topic and not required\nknowledge to use vosonSML but if you would like to learn\nmore a good introduction can be found in the Object-oriented programming\nchapter of Advanced R by Hadley Wickham.\nCreate the Network\nThe tweet data can now be used to create the nodes and edges network\ndata, and a graph by using the vosonSML Create\nand Graph functions:\n\n\n# create the network data\nauspol_actor_network <- Create(auspol_tweets, \"actor\")\n\n\nGenerating twitter actor network...\n-------------------------\ncollected tweets | 100\ntweet mention    | 26\ntweet            | 57\nreply mention    | 15\nreply            | 25\nquote mention    | 7 \nquote            | 18\nnodes            | 149\nedges            | 148\n-------------------------\nDone.\n\n\n\n# create an igraph\nauspol_actor_graph <- Graph(auspol_actor_network)\n\n\nCreating igraph network graph...Done.\n\nThat’s all there is to it, and now the resulting igraph\nnetwork can be plotted.\n\n\nlibrary(igraph)\n\n# set plot margins\npar(mar = c(0, 0, 0, 0))\n\n# auspol actor network with fruchterman-reingold layout\nplot(auspol_actor_graph, layout = layout_with_fr(auspol_actor_graph),\n     vertex.label = NA, vertex.size = 6, edge.arrow.size = 0.4)\n\n\n\n\nFigure 2: Actor network graph for collected #auspol tweets\n\n\n\nFor further information about rtweet, its features and\nhow to use it to collect twitter data please refer to the package site and\nintroductory rtweet\nvignette. For creating different types of networks such as the\nactivity, 2-mode and semantic\ntypes with vosonSML see the package documentation and\nintroductory vosonSML\nvignette.\n\n\n\n",
    "preview": "posts/2021-02-11-twitter-vosonsml-from-rtweet/rtweet_logo_preview.png",
    "last_modified": "2022-08-04T14:14:49+10:00",
    "input_file": {},
    "preview_width": 432,
    "preview_height": 499
  },
  {
    "path": "posts/2021-02-05_welcome/",
    "title": "Welcome to the VOSON Lab Code Blog",
    "description": "The code blog is a space to share tools, methods, tips, examples and code. A place to collect data, construct and analyze online networks.",
    "author": [
      {
        "name": "Robert Ackland",
        "url": "http://vosonlab.net/"
      }
    ],
    "date": "2021-02-04",
    "categories": [
      "rstats",
      "SNA",
      "Computational Social Science"
    ],
    "contents": "\nWelcome to the VOSON Lab Code Blog! We have created this space to share methods, tips, examples and code. It’s also a place where we will demonstrate constructing and analyzing networks from various API and other online data sources.\nMost of our posts will cover techniques around the tools we have developed at the Lab: vosonSML, VOSONDash and voson.tcn, which are available on both CRAN and GitHub. But we also plan to use this space to cover other complementary R packages and open-source software, such as fantastic R packages within the tidyverse, RStudio’s shiny for web apps, and visualization tools such as igraph and Gephi.\nVOSON Lab R Packages - Hex stickersVOSON Lab Open Source Tools\nvosonSML is a R package for social media data collection (currently twitter, youtube, and reddit), hyperlink collection and network generation. VOSONDash is a Shiny app that integrates tools for visualizing and manipulating network graphs, performing network and text analysis, as well as an interface for collecting data with vosonSML.\nMore information on these packages, their development and code can be found on our vosonSML, VOSONDash and voson.tcn github pages.\nWe also have some other guides for using the packages. Check the vosonSML Vignette and the VOSON Dash Userguide for some practical examples and feature reference.\nWe hope you find this content useful!\nThe VOSON Lab team.\nVirtual Observatory for the Study of Online Networks VOSON Lab, School of Sociology, The Australian National University.\n\n\n\n",
    "preview": "posts/2021-02-05_welcome/square-cards.png",
    "last_modified": "2023-02-10T13:54:31+11:00",
    "input_file": {},
    "preview_width": 800,
    "preview_height": 640
  }
]
